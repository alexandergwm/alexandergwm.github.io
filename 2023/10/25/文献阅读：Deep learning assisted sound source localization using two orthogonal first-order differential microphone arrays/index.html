<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基本信息论文地址：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arraysa) | The Journal of the Acoustical Society of America | AIP Publishing引用格式：@art">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays">
<meta property="og:url" content="http://example.com/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="基本信息论文地址：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arraysa) | The Journal of the Acoustical Society of America | AIP Publishing引用格式：@art">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/XVMYNX3/image.png">
<meta property="article:published_time" content="2023-10-25T11:05:44.000Z">
<meta property="article:modified_time" content="2024-05-05T12:10:07.528Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DoA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/XVMYNX3/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-05 20:10:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/XVMYNX3/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-25T11:05:44.000Z" title="发表于 2023-10-25 19:05:44">2023-10-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-05T12:10:07.528Z" title="更新于 2024-05-05 20:10:07">2024-05-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>论文地址：<a target="_blank" rel="noopener" href="https://pubs.aip.org/asa/jasa/article-abstract/149/2/1069/600660/Deep-learning-assisted-sound-source-localization?redirectedFrom=fulltext">Deep learning assisted sound source localization using two orthogonal first-order differential microphone arraysa) | The Journal of the Acoustical Society of America | AIP Publishing</a><br>引用格式：<br>@article{10.1121&#x2F;10.0003445,<br>    author &#x3D; {Liu, Nian and Chen, Huawei and Songgong, Kunkun and Li, Yanwen},<br>    title &#x3D; “{Deep learning assisted sound source localization using two orthogonal first-order differential microphone arraysa)}”,<br>    journal &#x3D; {The Journal of the Acoustical Society of America},<br>    volume &#x3D; {149},<br>    number &#x3D; {2},<br>    pages &#x3D; {1069-1084},<br>    year &#x3D; {2021},<br>    month &#x3D; {02},<br>    issn &#x3D; {0001-4966},<br>    doi &#x3D; {10.1121&#x2F;10.0003445},<br>    url &#x3D; {<a target="_blank" rel="noopener" href="https://doi.org/10.1121/10.0003445%7D">https://doi.org/10.1121/10.0003445}</a>,<br>    eprint &#x3D; {<a target="_blank" rel="noopener" href="https://pubs.aip.org/asa/jasa/article-pdf/149/2/1069/16680262/1069/_1/_online.pdf%7D">https://pubs.aip.org/asa/jasa/article-pdf/149/2/1069/16680262/1069\_1\_online.pdf}</a>,<br>}</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使用麦克风阵列在嘈杂和混响的房间中进行声源定位仍然是一项具有挑战性的任务，特别是对于小型阵列。近年来，通过将声音定位问题重新表述为分类问题，深度学习辅助方法取得了有希望的进展。基于深度学习的方法的关键在于在噪声和混响条件下有效提取声音位置特征。广泛采用的功能基于完善的广义互相关相位变换 (GCC-PHAT)，众所周知，它有助于消除房间混响。然而，GCC-PHAT 功能可能不适用于小型阵列。本文提出了一种利用两个正交一阶差分麦克风阵列构建的小型麦克风阵列的深度学习辅助声音定位方法。还提出了一种基于声强估计的改进特征提取方案，通过解耦白化加权构造中的声压和粒子速度分量之间的相关性，以增强时频分箱声强特征的鲁棒性。仿真和真实实验结果表明，所提出的深度学习辅助方法可以实现更高的空间分辨率，并且优于在噪声和环境下使用 GCC-PHAT 和小型阵列的声强特征的最先进的方法。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​	使用多个麦克风信号进行声源定位是麦克风阵列处理领域的一个活跃研究课题，并已获得广泛的实际应用，例如电话会议的自动摄像头跟踪[^1]人机交互[^2]和助听器[^3] 以及其他[^4–6 ]。 众所周知的声源定位经典方法包括 (1) 基于到达时间差 (TDOA) 的方法，[^7,8] (2) 基于转向响应功率 (SRP) [^9,10] 和（3）基于高分辨率光谱估计[^11,12]。 然而，这些经典方法在封闭环境中使用时可能面临一些挑战。挑战之一是由于房间边界和物体的多次声音反射而产生的房间混响。这些声音反射导致麦克风观测信号中源信号的多次衰减和延迟复制，因此可能会引入严重的信号失真 [^13]。众所周知，麦克风阵列的空间分辨率通常取决于其孔径大小，即，孔径越大，空间分辨率越高[^13]。 空间分辨率越高，我们通常也可以实现更多的降噪。不幸的是，在某些应用中可能需要限制麦克风阵列的孔径尺寸。因此，麦克风阵列的有限孔径尺寸对经典声音定位方法提出了另一个挑战。</p>
<p>​	为了解决房间混响带来的挑战，基于机器学习的方法已应用于使用麦克风阵列的声源定位[^14-27] 。 传统的基于麦克风阵列的方法，例如前面提到的基于TDOA和SRP的方法，通常仅依赖于嵌入在接收到的传感器信号中的空间信息来估计声源位置。相比之下，基于机器学习的方法还可以在训练过程中考虑与声源定位相关的先验信息，例如声学环境，从而更好地解决高房间混响情况下的声源定位问题。然而，许多传统的机器学习方法通常适用于可用训练数据有限的情况，因为当训练样本数量很大时，它们的计算成本很高[^28]。因此，基于深度学习的声源定位方法，深度学习是一种机器学习形式，使计算机能够从经验中学习[^28]，近年来尤其引起了人们的广泛兴趣。</p>
<p>​	众所周知，广义互相关相位变换（GCC-PHAT）在一定程度上对房间混响具有鲁棒性，因此它已与基于机器学习的声源定位方法广泛结合[^16-23]。  例如，肖等人[^16] 将声音定位任务表述为一个分类问题，以估计噪声和混响环境中的声音位置， 其中多层感知器 (MLP) 被用作网络架构，GCC-PHAT 特征作为 MLP 的输入。随后，卷积神经网络（CNN）在声源定位中得到了广泛的应用，并被证明能够在分类任务中产生最先进的性能，因为它们能够识别声源中的大量局部特征。通过小型局部感受野的共享权重进行输入[^28]。 其中，Yue等人[^17]提出使用GCC-PHAT特征作为CNN的输入来估计三维声音位置。在参考文献[^18]中,李等人。提出了一种结合 CNN 和长短期记忆 (LSTM) 来解决在线声音定位的方法，其中对所有传感器对上基于 GCC-PHAT 的特征矩阵进行求和。该方法对于麦克风阵列的拓扑结构具有鲁棒性。我们想指出的是，尽管据报道使用 GCC-PHAT 特征的基于深度学习的声音定位方法能够在混响环境下实现良好的性能，但不幸的是，正如我们将在本文后面展示的那样，GCC-PHAT 特性实际上可能不适用于小尺寸麦克风阵列。</p>
<p>​	在本文中，我们感兴趣的是使用小型麦克风阵列在嘈杂和混响环境中进行声源定位。为此，我们提出了一种 CNN 辅助声音定位方法，该方法使用由两个正交一阶差分麦克风阵列（DMA）构成的小型麦克风阵列。这项工作可以看作是我们之前工作的改进[^25]， 其中提出了一种基于声音强度（SI）估计的特征提取方法，并采用最小二乘支持向量机（LSSVM）作为机器学习模型。与之前的工作相比，当前工作的主要贡献简要总结如下：</p>
<ul>
<li><p>先前工作的一个问题是，SI 特征在所有时频 (T-F) 箱上进行平均，以减少数据维度，以便于 LSSVM 模型的后处理。然而，这会导致 T-F 域上有关源位置的有用局部信息完全丢失。相比之下，在本文中，我们在这项工作中利用 CNN 进行声源定位，它可以处理高维数据，非常适合捕获局部信息，因此可以保留整个 T-F 域上的所有 T-F bin-wise SI 特征。</p>
</li>
<li><p>正如本文分析的，先前工作的另一个问题是，由于白化加权下的声压和粒子之间的相关性，SI 特征提取中用于解决房间混响的基于相位变化（PHAT）的白化加权对加性噪声敏感。为了解决这个问题，本文通过解耦白化加权结构中声压和粒子速度分量之间的相关性，提出了一种改进的SI特征白化加权。</p>
</li>
<li><p>众所周知，论文中 SI 特征提取中使用的 DMA 对传感器失配非常敏感，例如麦克风增益和相位误差。然而，传感器失配对使用 SI 特征的声音定位方法的影响尚不清楚。本文研究了传感器失配对各种基于 SI 特征的声音定位方法的影响。结果表明，所提出的声音定位方法对于传感器失配具有鲁棒性，而现有的对应方法可能无法工作。</p>
</li>
</ul>
<p>​	在各种声学条件下进行了广泛的模拟以及真实世界的实验，一致表明所提出的深度学习辅助方法可以实现更高的空间分辨率，并且优于使用 GCC-PHAT 的最先进的方法或 SI 功能，用于在嘈杂和混响环境中使用小型阵列进行声音定位。</p>
<p>​	本文的其余部分安排如下。第二节简要介绍了信号模型和 SI 估计的基础。第三节介绍了所提出的声源定位系统的框架，其中特征提取和网络结构将在第二节中详细讨论。分别为 III A 和 III B。模拟和真实实验结果在章节 IV 和 V中显示，分别将我们提出的方法与其基线对应方法进行比较。最后，第二节。第六章总结了本文。</p>
<h2 id="信号模型"><a href="#信号模型" class="headerlink" title="信号模型"></a>信号模型</h2><p>​	考虑由两个正交的一阶 DMA 组成的四元件麦克风阵列，如图 1 所示。全向麦克风 $M_1$ 和 $M_3$ 沿 $x$ 轴形成一阶 DMA，另一对全向麦克风$ M_2$ 和 $M_4$ 形成沿 $y$ 轴的一阶 DMA。两个DMA的大小记为$d$，选择两个DMA的中心作为坐标原点。假设混响室中有一个远场声源以到达方向（DOA） $\theta \in [-180\degree,180\degree)$撞击麦克风阵列，其中 DOA 是相对于正$ x$ 轴定义的。那么第$i$个麦克风接收到的信号可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023211729875.png" alt="image-20231023211729875" style="zoom:67%;" />

<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023211744940.png" alt="image-20231023211744940" style="zoom: 67%;" />

<p>其中 $s(t)$、$h_i(t)$ 和 $n_i(t)$ 分别表示源信号、从声源到第 $i$ 个麦克风的房间脉冲响应 (RIR) 以及加性噪声，并$*$指的是卷积算子。</p>
<p>​	坐标原点处的声压可以通过对所有麦克风接收到的信号进行平均来估计[^29]， 也就是：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023211925996.png" alt="image-20231023211925996" style="zoom: 67%;" />

<p>​	根据动量守恒（参见参考文献 30 中的方程（3.12）），可以得出粒子速度在$ r$ 方向上的分量与声压的关系为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023212103329.png" alt="image-20231023212103329" style="zoom:67%;" />

<p>其中 $v_r(t)$ 是 $r$ 方向上的粒子速度，$p(t)$ 是声压，$\rho$ 是空气密度。在实践中，方程（3）中的压力梯度 可以通过有限差分来近似[^29]， 因此方程 (3) 变为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023212210617.png" alt="image-20231023212210617" style="zoom:67%;" />

<p>其中$ p_{r1}$ 和 $p_{r2}$ 是沿 $r$ 方向在两个相距较近的点处测量的声压，$\triangle r$ 是两点之间的距离。</p>
<p>​	通过将短时傅立叶变换（STFT）应用到方程 (4), 两个一阶 DMA 测量的坐标原点处的两个正交速度分量可分别在 T-F 域中表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023212340754.png" alt="image-20231023212340754" style="zoom:67%;" />

<p>其中$j &#x3D; \sqrt{-1}$ 是虚部， 有：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023212550176.png" alt="image-20231023212550176" style="zoom:67%;" />

<p>其中$P_i(w,t), H_i(w,t), S(w,t)$ 与$N_i(w,t)$ 分别为$p_i(t), h_t(t), s(t)$ 和$n_i(t)$ 的STFT表示。</p>
<p>​	而在方程(2) 中的声压在原点处$p_0(t)$的STFT表示为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023213431074.png" alt="image-20231023213431074" style="zoom:67%;" />

<p>​	在方程(5),(6),(8)中，瞬时复数声强SI的$x$和$y$ 分量可以表达为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023213559960.png" alt="image-20231023213559960" style="zoom:67%;" />

<p>其中上标星号代表复共轭。</p>
<h2 id="特征提取和网络架构"><a href="#特征提取和网络架构" class="headerlink" title="特征提取和网络架构"></a>特征提取和网络架构</h2><p>​	在这项工作中，使用 CNN 将声源定位表述为分类问题。所提出的声源定位系统的框图如图2所示。它包括两个主要组件：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023213759746.png" alt="image-20231023213759746" style="zoom:80%;" />

<ul>
<li><p>特征提取：为了解决小尺寸阵列的声源定位问题，我们提出了一种基于SI估计的特征提取方法，其方向与声源的DOA有关。为了提高针对房间混响和加性噪声的鲁棒性，我们提出的功能采用了抗噪声白化加权方案，并且还通过通过分解原始两个正交一阶 DMA 所制定的四个子阵列，将 SI 估计中的冗余纳入其中。然后，从所有 T-F bin 中提取的 SI 特征形成具有固定大小的特征矩阵，将其作为 CNN 的输入。特征提取的流程图如图2（b）所示，而子阵列配置的详细信息如图3所示。</p>
</li>
<li><p>网络架构：我们采用 CNN 作为机器学习模型。这项工作中使用的 CNN 架构如图 2(c) 所示。 CNN 旨在使用大量标记训练数据来学习从麦克风阵列信号中提取的 SI 特征到声源 DOA 的映射关系。因此，声源定位问题转化为端到端的多分类问题。</p>
</li>
</ul>
<p>​	下面，我们对上述两个组件进行更详细的介绍。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023214235796.png" alt="image-20231023214235796" style="zoom:67%;" />

<h3 id="提出的特征提取"><a href="#提出的特征提取" class="headerlink" title="提出的特征提取"></a>提出的特征提取</h3><h4 id="基于满尺度阵列的特征提取"><a href="#基于满尺度阵列的特征提取" class="headerlink" title="基于满尺度阵列的特征提取"></a>基于满尺度阵列的特征提取</h4><p>​	根据SI理论[^31],只有复数SI的实部，即有源强度向量，包含声源的位置信息。因此，有效 SI 的 $x$ 和 $y$ 分量，即分别对应于方程(9)和(10)的实部。 可以用作源位置特征。因此，提取的SI特征可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023215007619.png" alt="image-20231023215007619" style="zoom:67%;" />

<p>其中 $Re{}$ 代表实部。</p>
<p>​	众所周知，SI 特征对房间混响很敏感，因此会导致混响环境中声源定位的性能不佳。为了提高针对房间混响的鲁棒性，提出了基于一阶 DMA[^25]的现有特征提取方案来应用白化加权, 也就是 PHAT 加权到 SI特征上。然而，这是以增加对附加噪声的敏感性为代价的。 这里，我们给一个分析来证明此观点。 根据文献25，现有的白化权重定义为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023215249471.png" alt="image-20231023215249471" style="zoom:67%;" />

<p>其中$W_x(w,t)$和$ W_y(w,t)$ 表示方程的权重分别为（11）和（12）。回想一下Sec II， 总的声压信号$P_0(w,t)$ 是通过对四个麦克风信号进行平均估计得到的，而两个正交的粒子速度向量$V_x(w,t)$ 和$V_y(w,t)$ 是分别通过麦克风信号$p_1(t)$ 和$p_3(t)$ 以及麦克风$p_2(t)$ 和$p_4(t)$ 的差异估计得到的。 从方程(13) 和(14)， 我们可以看到在现有的特征提取中的白化权重是根据声压信号与粒子速度分量之间的相关性形成的。最后，也就是说， 在snr条件下，白化权重$W_x(w,t)$ 会严重受到$p_1(t)$ 和$p_3(t)$ 中的加性噪声的影响，而$W_y(w,t)$ 会严重受到$p_2(t)$ 和$p_4(t)$ 中的加性噪声的影响。</p>
<p>​	为了克服加性噪声的影响，我们必须在白化加权的构造中解耦声压和粒子速度分量之间的相关性。为此，我们建议使用以下白化权重，</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023220522135.png" alt="image-20231023220522135" style="zoom:67%;" />

<p>其中$\beta &gt; 0$ 表示在训练阶段需要调整的权衡参数。与现有的白化加权不同，声压和粒子速度分量之间的相关性现在已在方程(15) 中解耦。 值得注意的是，通过一阶DMA估计的粒子速度分量的功率谱通常远小于估计的声压的功率谱。例如，如下面的模拟结果所示，他们之间的差异可能高达6个数量级。因此，对于我们手头的问题，$\beta$ 的大小不应该设置很小。否则，两个正交粒子速度分量在加权中几乎可以被忽略。</p>
<p>​	为了更好地理解加性噪声对现有白化权重的影响，即方程 （13）和（14），以及建议的白化加权方程 (15)，现在我们进行一些解析分析。为了简化符号，这里我们暂时省略参数$(w,t)$ 。首先，我们分析加性噪声对现有白化的影响。通过联立方程(5),(6)以及(8), 方程(13), (14) 可以表示为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023221730152.png" alt="image-20231023221730152" style="zoom:67%;" />

<p>其中 $\triangle_1$ 和 $\triangle_2$ 是由于存在加性噪声而产生的干扰项，由下式给出:</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222152964.png" alt="image-20231023222152964" style="zoom:67%;" />

<p>这里的$O_1(N_i)$ 和$O_2(N_i)$ 表示低阶余数项。 省略低阶余数项，加性噪声对现有白化的影响近似表征为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222318541.png" alt="image-20231023222318541" style="zoom:67%;" />

<p>这意味着加性噪声对现有白化的影响与SNR的四次方成反比。</p>
<p>​	接下来，我们分析加性噪声对所提出的白化的影响。由方程式（5）、（6）和（8），方程 (15) 可以重新表述为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222504442.png" alt="image-20231023222504442" style="zoom:67%;" />

<p>其中扰动项 $\triangle w_1$； $\triangle w_2$；和 $\triangle w_3$ 由于加性噪声由下式给出</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222607998.png" alt="image-20231023222607998" style="zoom:67%;" />

<p>其中的$O_{w_1}(N_i) , O_{w_2}(N_i) $ 和$O_{w_3}(N_i) $ 都是低阶余数项。 因此忽略低阶余数项，加性噪声对所提出的白化的影响近似表征为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222758219.png" alt="image-20231023222758219" style="zoom:67%;" />

<p>这意味着加性噪声对所提出的白化加权的影响与 SNR 的二次方成反比。因此，我们可以预期，与现有的与SNR的四次方成反比的白化相比，在低SNR的条件下，加性噪声对所提出的白化的影响将不那么严重。</p>
<p>通过使用等式(15) 修正后的带有白化权重的SI特征可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231023222844039.png" alt="image-20231023222844039" style="zoom:67%;" />

<p>现有使用一阶 DMA 进行特征提取的另一个问题[^25] ，是 SI 特征在所有 T-F bin 上进一步平均。虽然 SI 特征的平均可以显着降低数据维度以方便支持向量机 (SVM) 的后处理，但它会导致源位置的有用局部信息完全丢失。在这项工作中，因为我们利用CNN进行声源定位，它可以处理高维数据并且非常适合捕获局部信息，因此我们保留了整个T-F域上的所有T-F bin-wise SI特征作为CNN的特征。</p>
<h4 id="通过合并子阵列丰富-SI-特征提取"><a href="#通过合并子阵列丰富-SI-特征提取" class="headerlink" title="通过合并子阵列丰富 SI 特征提取"></a>通过合并子阵列丰富 SI 特征提取</h4><p>​	上述基于SI估计的源位置特征提取利用了所有四个麦克风接收到的信号。实际上，通过使用共享麦克风形成两个正交的一阶 DMA，仅三个麦克风就足以估计 SI。因此，我们可以进一步将原始四元素正交一阶DMA分解为四个独立的三元素子阵列，通过纳入SI估计中的冗余来丰富SI特征。</p>
<p>​	图 3 显示了分解后的四个子阵列，每个子阵列分别形成两个正交的一阶 DMA，并分别共享麦克风 M1、M2、M3 和 M4。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155206990.png" alt="image-20231025155206990" style="zoom:67%;" />

<p>​	对于图3(a)所示的共享麦克风M1的子阵列，其中$M_2-M_1$沿$3\pi&#x2F;4$方向形成一阶DMA，$M_4-M_1$沿$-3\pi &#x2F; 4$方向形成一阶DMA，类似于式(1)。由(5)和(6)式可知，麦克风$M_1$处沿$-3\pi&#x2F;4$方向的质点速度和沿$3\pi&#x2F;4$方向的质点速度可分别近似表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025154519733.png" alt="image-20231025154519733" style="zoom:67%;" />

<p>​	相应地，瞬时复数SI在麦克风$M_1$ 处沿着方向$-3\pi&#x2F;4$ 和$3\pi&#x2F;4$ 可以为：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025154647410.png" alt="image-20231025154647410" style="zoom:67%;" />

<p>那么这里的麦克风$M_1$ 处的声压有： $P_{M_1}(w,t) &#x3D; [P_1(w,t) + P_2(w,t) + P_4(w,t)]&#x2F;3$。</p>
<p>​	类似于等式中的程序。 (29) 和 (30) 中，基于子阵列 M2-M1-M4 的 SI 特征具有由式（29）和（30）给出的建议白化权重。 (15) 现在可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155054261.png" alt="image-20231025155054261" style="zoom:67%;" />

<p>这里有：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155107972.png" alt="image-20231025155107972" style="zoom:67%;" />

<p>现在我们考虑图3（b）所示的子阵列$M_3-M_2-M_1$，其中麦克风$M_3$、$M_2$和$M_1$与共享麦克风$M_2$形成两个正交的一阶DMA，即沿$-\pi &#x2F;4$方向的$M_1-M_2$ 4和$M_3-M_2$沿方向$-3\pi&#x2F;4$。按照与上面类似的过程，沿方向 $-3\pi&#x2F;4$ 和 $-\pi&#x2F;4$ 的白化 SI 特征可以构造为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155218277.png" alt="image-20231025155218277" style="zoom:67%;" />

<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155649457.png" alt="image-20231025155649457" style="zoom:67%;" />

<p>其中：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155727084.png" alt="image-20231025155727084" style="zoom:67%;" />

<p>里面有：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025155756198.png" alt="image-20231025155756198" style="zoom:67%;" />

<p>接下来，对于图3（c）所示的子阵列$M_4-M_3-M_2$，其中麦克风$M_4$、$M_3$和$M_2$与共享麦克风$M_3$构造两个正交的一阶DMA，即$M_2-M_3$沿方向$\pi&#x2F; 4$和$M_4-M_3$沿$-\pi&#x2F;4$方向，所提出的沿$\pi&#x2F;4$和$-\pi&#x2F;4$方向的白化SI特征可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025160534724.png" alt="image-20231025160534724" style="zoom:67%;" />

<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025160740310.png" alt="image-20231025160740310" style="zoom:67%;" />

<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025161233409.png" alt="image-20231025161233409" style="zoom:67%;" />

<p>最后，对于图3（d）所示的子阵列$M_1-M_4-M_3$，其中麦克风$M_1$、$M_4$和$M_3$与共享麦克风$M_4$制定两个正交的一阶DMA，即$M_1-M_4$沿方向$\pi&#x2F; 4$和$M_3-M_4$沿方向$3\pi&#x2F;4$，所提出的具有白化权重的SI特征沿方向$\pi&#x2F;4$和$3\pi&#x2F;4$分别给出：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025161528514.png" alt="image-20231025161528514" style="zoom:67%;" />

<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025161510984.png" alt="image-20231025161510984" style="zoom:67%;" />

<h4 id="提议的-SI-特征总结"><a href="#提议的-SI-特征总结" class="headerlink" title="提议的 SI 特征总结"></a>提议的 SI 特征总结</h4><p>​	将用于估计 SI 特征的 STFT 的频率箱和帧的数量表示为 $M$ 和$ N$，当使用快速傅里叶变换（FFT）执行STFT时，$M$通常设置为偶数。然后，总而言之，我们提出的基于两个正交一阶 DMA 及其子阵列对应项的 SI 特征可以表示为三维矩阵：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025161916195.png" alt="image-20231025161916195" style="zoom:67%;" />

<p>其中$I_x^{W}(w_m,t_n)$ 由方程(29) 给出，并且$w_i(i &#x3D; 1,…,M&#x2F;2-1)$ 表示了STFT域的正频率频帧。 请注意，零频率处的 SI 特征等于 0，因此不包含源位置信息。因此，零频率处的SI特征被丢弃。此外，考虑到傅里叶变换的共轭对称性，只需利用正频率频率仓上的 SI 特征就足够了，如（56）所示。因此，零频率处的SI特征被丢弃。此外，考虑到傅里叶变换的共轭对称性，只需利用正频率频率仓上的 SI 特征就足够了，如（56）所示。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025163033179.png" alt="image-20231025163033179" style="zoom:67%;" />

<h3 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h3><p>​	众所周知，语音频谱图在时域和频域中具有局部相关性，而 CNN 能够通过局部连接有效地对这些相关性进行建模。此外，CNN 还可以捕获平移不变性，例如由于说话风格或说话者变化而导致的频移。[^32] 因此，CNN 已广泛应用于语音增强和语音源定位。[^17,20,32] 在这项工作中，我们还使用 CNN 来执行语音源定位。</p>
<p>​	如图2（c）所示，CNN由一个输入层、两个卷积层、两个全连接层和一个输出层组成。每个卷积层使用64个大小为3×3的卷积核来学习局部T-F区域之间的局部相关性。然后在每个卷积层之后使用批量归一化（BN）层，以提高网络的稳定性并加速网络的收敛。卷积层和全连接层的激活函数是修正线性单元（ReLU）。[^33]在卷积层和全连接层之间以及每个全连接层之后，使用速率为0.5的dropout过程[^34]来避免过拟合。卷积核的大小和数量以及全连接层中的节点数量如图2（c）所示。</p>
<p>![image-20231025163448002](&#x2F;images&#x2F;文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays&#x2F;image-20231025163448002.png)</p>
<p>​	所提出的 SI 特征首先被放入卷积层，并根据以下公式确定相应的输出：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025163712705.png" alt="image-20231025163712705" style="zoom:67%;" />

<p>其中$\textbf{W}<em>{c1}$和$\textbf{W}</em>{c2}$分别指第一和第二卷积层对应的卷积核的权重；$\textbf{ b}<em>{c1}和$$\textbf{b}</em>{c2}$分别代表第一和第二卷积层对应的加性偏置； $f$ 表示 ReLU 激活函数，定义为 $f(x) &#x3D; max(0, x)$。</p>
<p>​	全连接层将卷积层提取的所有特征结合起来，将输入的二维特征矩阵缩减为一维特征向量，方便输出层进行分类处理。在网络的最后一层，使用softmax激活函数[^35]进行分类，声源DOA候选的后验概率可以表示为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025164147853.png" alt="image-20231025164147853" style="zoom:67%;" />

<p>其中$o_k$是第$k$类对应的输出层的输出值。最终的源 DOA 通过最大化后验概率来估计，即</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025164222334.png" alt="image-20231025164222334" style="zoom:67%;" />

<p>其中 $\hat{\theta}$ 表示估计的源 DOA。</p>
<p>​	在CNN训练中，使用交叉熵函数[^36]作为损失函数，其由下式给出</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025164331539.png" alt="image-20231025164331539" style="zoom:67%;" />

<p>其中 $z_k$ 表示对应于第 $k$ 个类别的真实标签。我们采用 Adam[^37] 作为优化器。初始学习率设置为 $10^{- 3}$ ，最大 epoch 数选择为 100。在验证集上测量的 10个 epoch 的耐心提前停止也用于防止过度拟合。</p>
<h2 id="仿真验证"><a href="#仿真验证" class="headerlink" title="仿真验证"></a>仿真验证</h2><p>​	在本节中，我们将所提出的声源定位方法与不同声学环境、阵列尺寸和麦克风缺陷下的一些基线方法进行比较。本节首先描述基线方法、评估指标和模拟设置，然后介绍模拟结果。</p>
<h3 id="Baseline-方法"><a href="#Baseline-方法" class="headerlink" title="Baseline 方法"></a>Baseline 方法</h3><p>基线方法包括 SI-PHAT-Normal- Redund-LSSVM[^25] 使用 LSSVM 作为分类器，其他三种使用 CNN 作为分类器，即 (1) 具有遵循参考文献 [^25] 的 SI 特征 (SI-PHAT-NormalRedund-CNN)，(2) 具有仅基于全尺寸阵列 (SI-CNN) 的基本 SI 特征，(3) 具有基于 GCC 的声源定位中常用的特征-PHAT (GCC-PHAT-CNN).[^16–19] 为了便于表示，我们将这些基线方法简要总结如下：</p>
<ul>
<li><p>SI-PHAT-Normal-Redund-LSSVM：该方法在参考文献中提出[^25]. 这里我们遵循其中的符号。该方法通过对所有T-F bin进行平均来构造SI特征，以降低数据维度，以利于LSSVM的处理。</p>
</li>
<li><p>SI-PHAT-Normal-Redund-CNN：有趣的是，可以看到直接按照参考文献提取的 SI 特征的性能[^25] 当使用 CNN 而不是 LSSVM 作为分类器时。为了使特征适应CNN处理，这里我们在特征提取过程中放弃了平均操作，以保留整个T-F域上的SI信息。由此产生的方法被称为 SI-PHAT-Normal-Redund-CNN，遵循参考文献中的符号。此外，为了使该方法获得更好的学习效果，去除了CNN中每个卷积层之后的BN层，并在第一个卷积层中添加了一个速率为0.5的dropout层。</p>
</li>
<li><p>SI-CNN：我们还将我们提出的特征与基本 SI 特征进行了比较，基本 SI 特征仅基于全尺寸阵列提取，而不采用白化加权，所得方法表示为 SI-CNN。</p>
</li>
<li><p>GCC-PHAT-CNN：除了上述基于SI的特征外，我们还将基于GCC-PHAT[^16-19]的声源定位中常用的特征进行比较，使用GCC-PHAT特征的基线方法称为GCC-PHAT -CNN。</p>
</li>
</ul>
<h3 id="验证指标"><a href="#验证指标" class="headerlink" title="验证指标"></a>验证指标</h3><p>​	为了便于算法评估，我们使用定位精度作为性能指标，定义为</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025165300629.png" alt="image-20231025165300629" style="zoom:67%;" />

<p>其中 $N_s$ 表示正在评估的源位置的数量，$N_c$ 表示正确识别的源位置的数量。在此，如果对于$\theta_0$ 的分辨率，估计 DOA 与实际 DOA 的偏差在 $\theta_o$ 以内，则认为源被正确定位.</p>
<h3 id="仿真设置"><a href="#仿真设置" class="headerlink" title="仿真设置"></a>仿真设置</h3><p>​		如图4所示，在这里，我们考虑在尺寸为$9.64 \times7:04 \times2:95 m3$ 的矩形房间中进行声源定位任务。由两个正交的一阶DMA构成的麦克风阵列的中心位于（4.82， 3.52，1.5）m，阵列尺寸 $d&#x3D; 0.04 m$。声源与麦克风阵列中心的距离设置为1.5 m，声源与阵列处于同一高度。从声源到麦克风的 RIR 是使用软件 RIR GENERATOR 生成的[^38],  该软件基于众所周知的图像模型。[^39] 声音反射的顺序设置为软件 RIR GENERATOR 定义的默认值，该值对应于给定所需 RIR 长度的软件自动计算的最大反射阶数[^38]，即我们的模拟中的 3200。添加在麦克风上的加性噪声是高斯白噪声，其互不相关，也与声源信号不相关。 STFT 在 1024 个样本的每一帧上执行，连续帧之间有一个半重叠的汉宁窗。对于所有基于SI的特征，CNN的输入大小为$14\times 511$。此外，CNN中的通道C设置为（2； 6； 10； 10）；分别对应于（SI-CNN、GCC-PHAT-CNN、SI-PHATNormal-Redund-CNN 和所提出的方法）。对于LSSVM模型，选择径向基函数（RBF）作为核函数，并使用十倍交叉验证来调整正则化参数和平方核参数。有关 LSSVM 训练的更多详细信息，请参阅参考文献[^ 25].</p>
<p>​		在模拟中，我们考虑了源位置空间分辨率为 $30\degree$ 和 $10\degree$ 的两种情况。当空间分辨率设置为$30\degree$ 时，候选源位置总数为$K &#x3D;12$ ，均匀分布在$-170\degree$ 到$160\degree$ 之间。分辨率为 $10\degree$ ，候选源位置总数为 $K &#x3D; 36$ ，均匀分布在 $-180\degree$ 到 $170\degree$ 之间。语音信号选自TIMIT数据库[^40]作为声源信号，持续时间为500 ms，采样频率为16 kHz。我们随机抽取300个TIMIT数据库训练集中的句子合成训练数据，另外100个句子作为验证数据。训练集中总共有 6000 个数据点，验证集中有 1000 个数据点。在测试阶段，对于每个声源位置，我们从TIMIT数据库的测试数据中随机选择10个句子来生成测试集。</p>
<h3 id="仿真结果"><a href="#仿真结果" class="headerlink" title="仿真结果"></a>仿真结果</h3><h4 id="加性噪声的影响"><a href="#加性噪声的影响" class="headerlink" title="加性噪声的影响"></a>加性噪声的影响</h4><p>​	首先，我们将所提出的方法与不同加性噪声水平下的基线对应方法进行比较。这里，信噪比从0到30 dB变化，步长为5 dB，混响时间设置为$T_{60} &#x3D; 300 ms$。在图中。在图5（a）和5（b）中，我们展示了当空间分辨率分别设置为$30\degree$和$10\degree$时，各种方法的定位精度作为SNR的函数。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025192152648.png" alt="image-20231025192152648" style="zoom:67%;" />

<p>​	从结果中我们可以看出，当空间分辨率为$30\degree$时，所有使用SI特征的基于CNN和LSSVM的方法在SNR不低于10 dB时都具有相似的识别性能，可实现的定位精度更高超过95%。然而，对于空间分辨率为$10\degree$的更具挑战性的情况，如图5（b）所示，SI-PHAT-Normal-RedundLSSVM的性能随着SNR的降低而显着下降。这表明SI-PHAT-Normal-Redund-LSSVM无法在具有较高空间分辨率要求的低SNR条件下工作。相比之下，SI-PHAT-Norm-Redund-CNN 比 SI-PHAT-Normal-RedundLSSVM 显示出更好的鲁棒性，因为使用了所有 T-F bin 上的 SI 特征，因此捕获了有关源位置的更详细信息，并且 CNN比带有 RBF 核的 LSSVM 具有更强大的学习能力[^25] 关于 SVM 方法，我们想指出的是，可以使用其他核代替 RBF 核来提高声音定位性能。然而，这超出了这项工作的范围。然而，由于 PHAT 加权在存在强加性噪声的情况下受到影响，如第  III A 1节中所述。SI-PHAT-Norm-RedundCNN 在低信噪比下的定位精度方面比所提出的方法差很多。当信噪比为  0 dB、空间分辨率为 $10\degree$ 时，提出方法的定位精度为 84.72%，而 SI-PHATNorm-Redund-CNN 的定位精度仅为 51.11%。这表明，通过解耦声压和粒子速度分量之间的相关性所提出的白化加权确实比现有的基于 PHAT 的对应物在低 SNR 环境下表现更好。</p>
<p>​	对于使用流行的 GCC-PHAT 特征的 GCC-PHAT-CNN，我们从模拟结果可以看出，与所提出的方法相比，它对加性噪声也更敏感，并且性能相当差，尤其是在空间分辨率设置较高的情况下。对于信噪比 为0 dB、空间分辨率为 $10\degree$ 的情况，GCC-PHAT-CNN 的定位精度低至 46.67%。除了在信噪比较低时 PHAT 加权对加性噪声高度敏感的原因之外，另一个原因是所使用的麦克风阵列尺寸较小，这限制了基于 GCC-PHAT 特征的方法的性能，同样的，下面在有关阵列尺寸影响的数值分析中清楚地显示了这一点。</p>
<p>​	回想一下，在所提出的 SI 特征提取中，我们建议通过解耦声压和粒子速度分量之间的相关性来使用改进的白化加权，以克服现有基于 PHAT 的对应问题。在改进的白化加权中，我们引入了变化的权衡参数$\beta$来补偿声压和粒子速度分量的功率谱之间的巨大差异。在这里，我们提供一个数值示例来展示 $\beta$的效果。在图6中，不同$\beta$值下所提出方法的定位精度表示为SNR的函数，其中源位置的空间分辨率设置为$10\degree$。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025193428898.png" alt="image-20231025193428898" style="zoom:67%;" />

<p>​	从图 6 可以看出，最佳定位性能在 $\beta&#x3D;10^6$ 左右实现。$\beta$ 偏离 $10^6$ 将导致对加性噪声的敏感性增加。这是因为使用一阶 DMA 估计的粒子速度分量的功率谱远小于估计的声压的功率谱，并且在本模拟示例中它们的差异高达 6 个数量级。因此，如果选择的 $\beta$太小，则在白化加权中将忽略正交粒子速度分量的贡献。相反，b 不应设置得远大于 $\beta&#x3D; 10^6$；否则，在白化加权中也将忽略声压的贡献。</p>
<h4 id="房间混响的影响"><a href="#房间混响的影响" class="headerlink" title="房间混响的影响"></a>房间混响的影响</h4><p>​	在第二个示例中，我们比较了房间混响对各种源定位方法的影响。图7(a)和7(b)分别显示了空间分辨率为$30\degree$和$10\degree$时各种方法的定位精度。这里，混响时间T60从200到800ms变化，步长为50ms，SNR设置为10dB。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025194047911.png" alt="image-20231025194047911" style="zoom:67%;" />

<p>​	从模拟结果中，我们可以看到，虽然一些基线方法，即SI-PHAT-Norm-RedundLSSVM和GCC-PHAT-CNN，在空间分辨率为$30\degree$时可能会产生与所提出的方法相当的性能，但当分辨率提高到 $10\degree$ ，在各种混响环境下，所有基线方法都明显劣于所提出的方法。一般来说，基于 LSSVM 的方法 SIPHAT-Norm-Redund-LSSVM 的性能比基于 CNN 的方法差，特别是在高空间分辨率的条件下。这进一步意味着 SI 特征提取中源位置的局部信息（在 SI-PHAT-Norm-Redund-LSSVM 中丢失）有助于提高空间分辨率。请注意，与使用 SI 特征的基于 CNN 的对应物相比，SI-CNN 不会进行白化加权来对抗房间混响，也没有在 SI 估计中纳入冗余来丰富特征提取。因此，在使用 SI 特征的基于 CNN 的方法中，SI-CNN 在混响环境中表现出最差的性能。对于使用传统GCC-PHAT特征的GCC-PHAT-CNN，我们从图7（a）可以看出，当空间分辨率较低时，它可以达到与所提出的方法相当的令人满意的性能。然而，当空间分辨率增加时，它会随着混响时间的增加而显着下降，并且变得比使用 SI 特征的对应物差得多，如图 7（b）所示。</p>
<h4 id="阵列尺寸的影响"><a href="#阵列尺寸的影响" class="headerlink" title="阵列尺寸的影响"></a>阵列尺寸的影响</h4><p>​	现在我们来讨论阵列尺寸对不同方法的影响，方法8展示了不同方法随着麦克风阵列尺寸$d$ 变化的定位准确率的函数，这里选取了源定位的空间分辨率为$10\degree$， 这列，阵列的尺寸$d$ 从$1$ 到40$cm$ 变化，这里的$T_{60} &#x3D; 300ms $ 及$SNR&#x3D;10dB$</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025194534198.png" alt="image-20231025194534198" style="zoom:67%;" />

<p>​	从图8的模拟结果中，我们可以看到，当使用SI信息作为源定位的特征时，无论使用哪种分类器，阵列大小对所有基于SI的方法都表现出相似的效果。也就是说，当数组大小太小或太大时，定位性能会恶化。一个原因是 SI 特征基于使用一阶 DMA 进行粒子速度近似的声压信号的有限差分，即根据方程 （4）。因此，当阵列尺寸增大时，近似误差将相应增大，从而导致 SI 估计较差。另一方面，如果阵列尺寸设置得太小，DMA 的噪声敏感性将会较差，尤其是在低频下。[^41] 根据大量仿真结果，该方法的最佳阵列尺寸范围为 2–5.5 cm。对于GCC-PHAT-CNN，从结果可以看出，它只适用于数组规模较大的情况。对于小阵列尺寸，如我们在 $d &#x3D;4$ cm 的模拟中所使用的，其定位精度可能比基于 SI 的对应产品差很多。</p>
<h4 id="麦克风不匹配的影响"><a href="#麦克风不匹配的影响" class="headerlink" title="麦克风不匹配的影响"></a>麦克风不匹配的影响</h4><p>​	众所周知，小尺寸麦克风阵列通常对麦克风失配敏感，例如麦克风增益和相位误差。在此示例中，我们比较了存在麦克风增益和相位误差的情况下各种方法的性能。</p>
<p>​	假设麦克风增益和相位误差未知且分别由下式确定：</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025195149081.png" alt="image-20231025195149081" style="zoom:67%;" />

<p>其中$\epsilon$表示麦克风增益误差，$\phi$表示麦克风相位误差，$\lambda$表示用于控制误差范围的比例参数。在仿真中，每个麦克风的麦克风增益和相位误差是在式（62）给出的误差范围内随机选择的。</p>
<p>​	图 9 显示了各种方法的定位精度，$\lambda$ 从 0 变化到 1，步长为 0.2，即随着麦克风失配的增加，其中混响时间 T60 &#x3D; 300 ms，SNR &#x3D; 10 dB，源位置的空间分辨率设置为 $10\degree$ 。从图9中我们可以看到，与基于CNN的方法相比，基于LSSVM的方法SI-PHAT-NormRedund-LSSVM随着麦克风失配的增加而显着退化。这可能是因为LSSVM本质上属于浅层神经网络，仅包含一层非线性特征变换，因此其泛化能力有限，无法有效地从失配误差破坏的特征中学习到声源位置信息，导致容错能力较差。相比之下，CNN是具有更多隐藏层的深层架构，可以在训练过程中自动提取最有效的特征，以提高神经网络的容错能力。因此，四种基于 CNN 的方法的定位性能基本上对麦克风缺陷不敏感。此外，由于我们的特征提取的有效性，所提出的方法的定位精度始终保持在 95% 以上，并且在存在麦克风失配的情况下在所有基于 CNN 的方法中表现出优越的性能。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025195329679.png" alt="image-20231025195329679" style="zoom:67%;" />

<h3 id="真实世界实验结果"><a href="#真实世界实验结果" class="headerlink" title="真实世界实验结果"></a>真实世界实验结果</h3><p>​	在本节中，在室内环境中进行了一些真实世界的实验，以评估所研究方法的性能。实验设置如图10（a）所示。房间的大小是 $9.64 \times 7.04 \times 2.95 m^3$  。测得房间的混响时间约为 300 毫秒。我们使用与上述模拟中类似的麦克风阵列，尺寸为4厘米，由四个$\frac{1}{2}$英寸麦克风组成，形成两个正交的一阶DMA，如图10（b）所示。麦克风阵列放置在房间中心周围，距离地板高度为 1.5 m。我们使用扬声器作为声源，距离麦克风阵列中心1.5 m。采用 NI 的 USB-4432 数据采集设备，分辨率为 24 位，采样频率为 16 kHz。在实验中，我们考虑了两种不同空间分辨率的情况，即$30\degree$和$20\degree$。</p>
<img src="/images/文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays/image-20231025195624277.png" alt="image-20231025195624277" style="zoom:67%;" />

<p>​	与参考文献[^25]不同 ，它使用真实世界的实验数据来训练神经网络，这里我们使用与上述模拟示例相同的模拟数据来训练 CNN 和 LSSVM（请注意，在模拟中，我们使用了一个与现实世界实验中大小相同的房间）。通过这样做，我们可以检验各种方法处理训练和测试条件不匹配的能力，并且不需要重复费力的实验来收集大量数据进行神经网络训练。真实实验中CNN和LSSVM的参数设置与上述模拟中的参数设置相同。在源定位阶段，在每个位置随机选择20个不同的语音信号，这导致总共240个和360个测试数据点，分别对应于30和20的空间分辨率。</p>
<p>​	表一和表二分别显示了空间分辨率为$30\degree$ 和$20\degree$ 时各种方法的定位精度。从结果可以看出，总体来说，现实世界的实验结果与第 4 节中给出的模拟结果非常一致。尽管一些基线方法在低空间分辨率的情况下可以达到与所提出的方法相当的性能，但当空间分辨率变高时，所有这些方法都会急剧恶化。相比之下，所提出的方法对空间分辨率变化不太敏感，并且在实际混响环境中显示出所有研究方法中最好的性能。</p>
<p>![image-20231025200044500](&#x2F;images&#x2F;文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays&#x2F;image-20231025200044500.png)</p>
<p>![image-20231025200051123](&#x2F;images&#x2F;文献阅读：Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays&#x2F;image-20231025200051123.png)</p>
<p>对于SI-PHAT-Norm-Redund-LSSVM，性能不如其对应的SI-PHAT-Norm-RedundCNN，特别是对于高空间分辨率为20的情况。与仿真结果类似，这表明SI-PHAT-NormRedund-CNN中采用的SI特征提取中的源位置局部信息有助于提高实际环境中的定位性能。在使用 SI 特征的基于 CNN 的方法中，SI-CNN 的表现最差，这进一步证实了 SI 估计中的白化加权和冗余的使用对于构建鲁棒的源位置特征是必不可少的。此外，所提出的方法表现出比 SI-PHATNorm-Redund-CNN 更好的性能，对于空间分辨率为 20 的更具挑战性的情况，平均定位精度提高了近 10%。正如模拟中所揭示的，这再次意味着所提出的白化加权方案比现有的 PHAT 加权方案更稳健。对于使用流行的GCC-PHAT特征的基于CNN的方法，我们还可以看到GCCPHAT特征提取不适用于小尺寸阵列，因为它在高空间分辨率的条件下可能导致性能相当差。对于空间分辨率为 20 的真实实验，GCC-PHAT-CNN 的平均定位精度大于低于我们提出方法的$20%$。</p>
<h3 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h3><p>​	在本文中，我们针对由两个正交一阶DMA组成的小型麦克风阵列提出了一种CNN辅助的室内声源定位算法。特别是，基于我们之前提出的 SI 特征，提出了一种改进的声音位置特征提取方案，该方案对加性噪声、房间混响和麦克风失配具有鲁棒性。与现有的 SI 特征不同，现有的 SI 特征完全丢失了 T-F 域上源位置的有用本地信息，而所提出的 SI 特征保留了所有 T-F bin-wise SI 特征。此外，还分析了现有SI特征对加性噪声的敏感性。结果表明，由于白化加权中声压和粒子速度分量之间的相关性，现有的 SI 特征对加性噪声敏感。然后通过解耦白化加权构造中的这种相关关系，提出一种改进的SI特征提取方案。</p>
<p>​	对于声音定位文献中广泛采用的 GCC-PHAT 特征，我们对阵列尺寸影响的分析表明，GCC-PHAT 特征不适合小型麦克风阵列，例如论文中直径仅为4厘米的阵列。相比之下，所提出的 SI 特征对于小尺寸阵列表现更好，特别是在声音位置的高空间分辨率的条件下。还分析了麦克风失配的影响，这是小尺寸阵列设计的主要问题。有趣的是，所提出的方法对变化的传感器缺陷不太敏感，因此在实际应用中很有前景。仿真和现实世界的实验结果一致证明了所提出的声音定位方法比现有的同类方法具有优越的性能。</p>
<p>​	最后，我们想指出一些未来的工作。在本研究中，我们研究了当训练条件与其目标应用场景相似时所提出的定位模型的性能。然而，我们可能希望定位模型能够在房间的大小与训练过程中的房间不同。为此，通过包含更多不同房间大小的训练数据来研究所提出的本地化模型的通用性是很有意义的。此外，目前的工作涉及单个语音源的定位问题。未来的其他工作将是扩展所提出的方法以解决多个语音源的本地化问题。这可以通过使用所谓的语音信号的 W 不相交正交性属性来实现[^42]。 即每个 T-F bin 最多只有一个语音源处于活动状态，这已广泛应用于语音源分离和多语音源本土化。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/">http://example.com/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/DoA/">DoA</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/XVMYNX3/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/" title="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR"><img class="cover" src="https://i.ibb.co/vHV5BKD/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR</div></div></a></div><div class="next-post pull-right"><a href="/2023/09/29/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFundamentals%20of%20Spherical%20Array%20Processing/" title="文献阅读:Fundamentals of Spherical Array Processing"><img class="cover" src="https://i.ibb.co/pRR1S1b/Eigenmike.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读:Fundamentals of Spherical Array Processing</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/" title="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR"><img class="cover" src="https://i.ibb.co/vHV5BKD/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-27</div><div class="title">文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR</div></div></a></div><div><a href="/2024/04/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20SFANC-FxNLMS%20Algorithm%20for%20Active%20Noise%20Control%20based%20on%20Deep%20Learning/" title="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning"><img class="cover" src="https://i.ibb.co/5GmwJBS/image.png	" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-23</div><div class="title">文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning</div></div></a></div><div><a href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"><img class="cover" src="https://i.ibb.co/rk6B9g0/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control</div></div></a></div><div><a href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"><img class="cover" src="https://i.ibb.co/xSTFTsh/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control</div></div></a></div><div><a href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"><img class="cover" src="https://i.ibb.co/LSLjfkw/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-05</div><div class="title">文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL</div></div></a></div><div><a href="/2023/08/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABlind%20Localization%20of%20Early%20Room%20Reflections%20Using%20Phase%20Aligned%20Spatial%20Correlation/" title="文献阅读:Blind Localization of Early Room Reflections Using Phase Aligned Spatial Correlation"><img class="cover" src="https://i.ibb.co/L56nmjT/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-25</div><div class="title">文献阅读:Blind Localization of Early Room Reflections Using Phase Aligned Spatial Correlation</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">基本信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">信号模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">5.</span> <span class="toc-text">特征提取和网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">5.1.</span> <span class="toc-text">提出的特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%BB%A1%E5%B0%BA%E5%BA%A6%E9%98%B5%E5%88%97%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">5.1.1.</span> <span class="toc-text">基于满尺度阵列的特征提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%90%88%E5%B9%B6%E5%AD%90%E9%98%B5%E5%88%97%E4%B8%B0%E5%AF%8C-SI-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">5.1.2.</span> <span class="toc-text">通过合并子阵列丰富 SI 特征提取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8F%90%E8%AE%AE%E7%9A%84-SI-%E7%89%B9%E5%BE%81%E6%80%BB%E7%BB%93"><span class="toc-number">5.1.3.</span> <span class="toc-text">提议的 SI 特征总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-number">5.2.</span> <span class="toc-text">网络架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BF%E7%9C%9F%E9%AA%8C%E8%AF%81"><span class="toc-number">6.</span> <span class="toc-text">仿真验证</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Baseline-%E6%96%B9%E6%B3%95"><span class="toc-number">6.1.</span> <span class="toc-text">Baseline 方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%8C%87%E6%A0%87"><span class="toc-number">6.2.</span> <span class="toc-text">验证指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BF%E7%9C%9F%E8%AE%BE%E7%BD%AE"><span class="toc-number">6.3.</span> <span class="toc-text">仿真设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BF%E7%9C%9F%E7%BB%93%E6%9E%9C"><span class="toc-number">6.4.</span> <span class="toc-text">仿真结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E5%99%AA%E5%A3%B0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.4.1.</span> <span class="toc-text">加性噪声的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%BF%E9%97%B4%E6%B7%B7%E5%93%8D%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.4.2.</span> <span class="toc-text">房间混响的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%98%B5%E5%88%97%E5%B0%BA%E5%AF%B8%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.4.3.</span> <span class="toc-text">阵列尺寸的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%BA%A6%E5%85%8B%E9%A3%8E%E4%B8%8D%E5%8C%B9%E9%85%8D%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">6.4.4.</span> <span class="toc-text">麦克风不匹配的影响</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">6.5.</span> <span class="toc-text">真实世界实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-number">6.6.</span> <span class="toc-text">结论和未来工作</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/07/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AGFANC-Kalman-%20Generative%20Fixed-Filter%20Active%20%20Noise%20Control%20With%20CNN-Kalman%20Filtering/" title="文献阅读: GFANC-Kalman: Generative Fixed-Filter Active  Noise Control With CNN-Kalman Filtering"><img src="https://i.ibb.co/VQ54Jw6/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: GFANC-Kalman: Generative Fixed-Filter Active  Noise Control With CNN-Kalman Filtering"/></a><div class="content"><a class="title" href="/2024/07/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AGFANC-Kalman-%20Generative%20Fixed-Filter%20Active%20%20Noise%20Control%20With%20CNN-Kalman%20Filtering/" title="文献阅读: GFANC-Kalman: Generative Fixed-Filter Active  Noise Control With CNN-Kalman Filtering">文献阅读: GFANC-Kalman: Generative Fixed-Filter Active  Noise Control With CNN-Kalman Filtering</a><time datetime="2024-07-03T13:30:55.710Z" title="发表于 2024-07-03 21:30:55">2024-07-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMaximization%20of%20acoustic%20energy%20difference%20between%20two%20spaces/" title="文献阅读: Maximization of acoustic energy difference between two spaces"><img src="https://i.ibb.co/Sd650ms/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Maximization of acoustic energy difference between two spaces"/></a><div class="content"><a class="title" href="/2024/06/15/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMaximization%20of%20acoustic%20energy%20difference%20between%20two%20spaces/" title="文献阅读: Maximization of acoustic energy difference between two spaces">文献阅读: Maximization of acoustic energy difference between two spaces</a><time datetime="2024-06-15T06:24:10.886Z" title="发表于 2024-06-15 14:24:10">2024-06-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"><img src="https://i.ibb.co/xSTFTsh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"/></a><div class="content"><a class="title" href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control">文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control</a><time datetime="2024-05-06T12:48:52.939Z" title="发表于 2024-05-06 20:48:52">2024-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"><img src="https://i.ibb.co/rk6B9g0/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"/></a><div class="content"><a class="title" href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control">文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control</a><time datetime="2024-05-05T16:00:00.000Z" title="发表于 2024-05-06 00:00:00">2024-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"><img src="https://i.ibb.co/LSLjfkw/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"/></a><div class="content"><a class="title" href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL">文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL</a><time datetime="2024-05-05T11:30:38.183Z" title="发表于 2024-05-05 19:30:38">2024-05-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>