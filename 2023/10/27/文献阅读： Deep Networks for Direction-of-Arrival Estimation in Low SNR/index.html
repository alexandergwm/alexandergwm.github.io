<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基本信息引用格式：@article{papageorgiouDeepNetworksDirectionofArrival2021,    title &#x3D; {Deep Networks for Direction-of-Arrival Estimation in Low {SNR}},    volume &#x3D; {69},    issn &#x3D; {1941-0476},">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR">
<meta property="og:url" content="http://example.com/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="基本信息引用格式：@article{papageorgiouDeepNetworksDirectionofArrival2021,    title &#x3D; {Deep Networks for Direction-of-Arrival Estimation in Low {SNR}},    volume &#x3D; {69},    issn &#x3D; {1941-0476},">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/vHV5BKD/image.png">
<meta property="article:published_time" content="2023-10-26T23:33:28.000Z">
<meta property="article:modified_time" content="2024-05-05T12:09:37.904Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DoA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/vHV5BKD/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-05-05 20:09:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/vHV5BKD/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-26T23:33:28.000Z" title="发表于 2023-10-27 07:33:28">2023-10-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-05-05T12:09:37.904Z" title="更新于 2024-05-05 20:09:37">2024-05-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/">文献阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>引用格式：<br>@article{papageorgiouDeepNetworksDirectionofArrival2021,<br>    title &#x3D; {Deep Networks for Direction-of-Arrival Estimation in Low {SNR}},<br>    volume &#x3D; {69},<br>    issn &#x3D; {1941-0476},<br>    url &#x3D; {<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9457195%7D">https://ieeexplore.ieee.org/abstract/document/9457195}</a>,<br>    doi &#x3D; {10.1109&#x2F;TSP.2021.3089927},<br>    pages &#x3D; {3714–3729},<br>    author &#x3D; {Papageorgiou, Georgios K. and Sellathurai, Mathini and Eldar, Yonina C.},<br>    urldate &#x3D; {2023-10-07},<br>    date &#x3D; {2021},<br>    note &#x3D; {Conference Name: {IEEE} Transactions on Signal Processing},<br>    keywords &#x3D; {&#x2F;unread},<br>    file &#x3D; {全文:D:\Zetero\storage\F5T72877\Papageorgiou 等 - 2021 - Deep Networks for Direction-of-Arrival Estimation in Low SNR.pdf:application&#x2F;pdf;IEEE Xplore Abstract Record:D:\Zetero\storage\B7GDIEM4\9457195.html:text&#x2F;html},<br>}</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​	在这项工作中，我们考虑使用深度学习 (DL) 在存在极端噪声的情况下进行到达方向 (DoA) 估计。特别是，我们引入了一个卷积神经网络（CNN），它使用样本协方差矩阵估计来预测角度方向。该网络是根据低信噪比 (SNR) 状态下真实阵列流形矩阵的多通道数据进行训练的。通过采用网格方法，我们将问题建模为多标签分类任务，并训练 CNN 来预测所有 SNR 上的 DoA。所提出的架构展示了在存在噪声的情况下增强的鲁棒性以及对相对少量快照的弹性。此外，它能够解析网格分辨率内的角度。实验结果表明，与最先进的方法相比，低信噪比条件下的性能显着提升，并且在相关和不相关源的情况下都不需要任何参数调整。最后，我们放宽了源数量先验已知的假设，并提出了一种训练方法，其中 CNN 学习推断源数量并以高置信度预测 DoA。在从无线阵列传感器到声学麦克风或声纳等多个领域出现的挑战性场景中，所提出的解决方案增强的鲁棒性是非常理想的。</p>
<p>索引术语 — 到达方向 (DoA) 估计、卷积神经网络 CNN、深度学习 DL、多标签分类、阵列信号处理。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>​	数十年来，由于从雷达和无线通信到声纳和声学等众多应用，到达方向 (DoA) 估计一直处于研究活动的前沿 [1]，其中定位是最重要的之一那些。通过使用指定几何配置（例如线性、矩形和圆形）的多个传感器可以估计角度方向。有效利用多个传感器的观测结果可以对多个源进行 DoA 估计，具体取决于阵列传感器的数量。 DoA估计有两个主要类别：超定情况，其中源数量小于阵列传感器的数量，以及欠定情况，其中源数量等于或大于传感器数量[2]， [3]。在这项工作中，我们研究第一类。此外，<strong>我们专注于低 SNR 下的 DoA 估计，这带来了一些挑战，并且在许多现实情况中至关重要</strong>。</p>
<p>​	为 DoA 估计引入的首批方法之一是多重信号分类 (MUSIC) [4]，随后不久又出现了其他几种变体。 MUSIC估计器属于基于子空间的技术，它试图分离信号和噪声子空间；角度估计是根据指定网格上的所谓音乐伪频谱得出的，其中选择了伪频谱的相应峰值。通过旋转不变技术（ESPRIT）[5]及其变体估计信号参数是另一个成功的例子[6]。单一 ESPRIT [6]、[7] 结合了前向-后向平均，是一个值得注意的变体，与 ESPRIT 相比，它可以提高性能，特别是在源信号相关的情况下。根多重信号分类 (R-MUSIC) 的发展是 DoA 估计改进的重要一步，它根据高阶多项式的解来估计角度方向 [8]。上述方法是基于协方差的技术，需要足够数量的数据快照来准确估计 DoA，特别是在低 SNR 的情况下。此外，他们经常假设源的数量是已知的，但在许多实际应用中情况并非如此。</p>
<p>​	在过去的十年中，压缩感知（CS）方法也被用来解决 DoA 估计问题 [9]、[10]。这些方法利用空间域（角度）中信号源的稀疏特性。 CS技术通常分为三个主要类别：a）on-grid，b）off-grid 和c）grid-less 方法[11]，[12]。grid-less 方法以非常高的计算复杂性为代价获得了更好的性能[13]。off-grid 和on-grid 方法提供了更平衡的解决方案，计算需求更少，但由于grid不匹配问题而造成的性能损失可以忽略不计[14]。 DoA 估计是在稀疏最小化任务求解后获得的，为此确定了两种主要方法：i）基于 0 伪范数的贪婪方法和 ii）基于 1-范数的凸松弛。值得注意的是称为 $l_{2,1}-SVD $的方法，该方法首先对接收的信号数据执行降维技术然后以显着减少的计算负担解决降维中的 $l_{2,1}-svd$范数最小化任务。它在[15]中被引入，后来也在[16]中被使用。所有这些方法共同的主要缺点之一是需要调整一个或多个参数（取决于快照的数量、SNR 或两者）来保证良好的性能；此外，DoA 估计通常对这些参数的调整极其敏感。</p>
<p>​	最近的 DoA 估计方法是使用深度学习 (DL) [17]、[18]。与基于优化的方法相比，基于深度学习的方法具有多个优势：a）训练网络后不需要优化，解决方案是简单运算（乘法和加法）的结果； b) 与基于优化的技术相比，它们不需要任何特定的参数调整，后者的解决方案强烈依赖于这些参数的调整，并且 c) 它们表现出对数据缺陷的弹性，例如，使用更少的快照，在低信噪比。 [19] 采用具有全连接 (FC) 层的深度神经网络 (DNN)，使用信号协方差矩阵对两个目标进行 DoA 分类。然而，报告的结果表明在高 SNR 下 DoA 估计结果较差。[20] 中的作者提出了一种用于大规模 MIMO 系统中信道估计的 DNN；然而，他们关注的是高 SNR 机制和信道估计性能。 [21]中采用了具有一系列并行多层分类器（即多层感知器（MLP））的多层自动编码器，重点关注对阵列缺陷的鲁棒性。 MLP 架构通过使用多任务自动编码器来解决仅两个源的 DoA 估计问题，该多任务自动编码器充当一组空间滤波器，后接一系列用于空间谱估计的并行多层 DNN。网络在每个单独的 SNR下进行训练（<a target="_blank" rel="noopener" href="https://github.com/LiuzmNUDT/DNN-DOA%EF%BC%89%E3%80%82[22]">https://github.com/LiuzmNUDT/DNN-DOA）。[22]</a> 中提出了一种也在低 SNR 下进行训练的深度卷积神经网络 (CNN)。然而，由于采用一维 (1D) 滤波器（卷积），该方法在 DoA 估计方面并未表现出显着的性能提升。 [23]、[24] 中提出了一种用于语音处理背景下宽带 DoA 估计的 CNN。与这些工作相反，我们研究了窄带 DoA 估计的情况。 [25]、[26] 中的作者采用 DNN 进行基于距离的船舶定位。他们的方法基于距离估计，而我们专注于信号方向的估计。 [27] 中发表了一种基于深度学习的伪谱估计方法，其中作者还提出了一种估计角度的扩展。然而，所展示的结果是高信噪比的，并且假设源的数量是已知的。在声学背景下，[28] 中的作者提出了一种使用单快照样本协方差矩阵 (SCM) 进行波束形成的 DNN，后来在 [29]、[30] 中进行了扩展，以包含更多的快照和源。这种方法不能在低信噪比场景中采用，因为低信噪比场景中快照和传感器的数量需要相当高。</p>
<p>据我们所知，没有开发出特定的基于深度学习的技术来在低信噪比条件下进行稳健的 DoA 估计。 [21]、[22]等方法的另一个缺点是它们是针对特定数量的快照进行训练的，这导致不同数量的快照和不同的快照数量和信噪比存在显着偏差。</p>
<p>​	这项工作的范围是填补低信噪比下使用 DL 的 DoA 估计文献中的空白。由于在低信噪比下 SCM 估计与真实流形矩阵存在较大偏差，DoA 估计变得非常具有挑战性，并且大多数方法无法证明所需的鲁棒性。在这项工作中，我们通过以下方式为这个方向做出贡献：a）利用具有 2D 卷积层的深度网络，该网络以其出色的特征提取特性而闻名； b) 使用多通道数据，即复值协方差矩阵条目的实部、虚部和相位（类似于它们在图像处理中的使用方式）； c) 采用 dropout 层作为正则化手段，从而提高估计器的泛化性并避免过度拟合。我们的贡献总结如下：</p>
<ul>
<li>我们引入了一种在多通道数据上训练的深度 CNN，这些数据是由真实协方差矩阵的复值数据显式形成的。第一和第二通道是通过分别从协方差矩阵的复值条目中获取实部和虚部而形成的。第三个通道包括来自协方差矩阵的复值条目的相位（在[−π, π]中）。所提出的 CNN 采用 2D 卷积层，并经过训练可以直接预测多个源的角度方向。使用 SCM 估计对任意数量的快照进行测试。多通道数据的使用以及 2D 卷积层的采用可以从输入数据中提取特征，从而在低 SNR 下实现更稳健的 DoA 估计。对于所需的角度（空间）区域采用离散化方法（网格上），并将方向估计任务建模为多标签分类任务。</li>
<li>提出了训练所提出的网络的有效方法。特别是，我们在一系列低 SNR 下训练 CNN，并证明它也可以在高 SNR 下成功预测 DoA。</li>
<li>我们引入了针对不同数量来源的培训方法。随后，所提出的 CNN 从接收到的数据推断源数量，同时预测 DoA。</li>
<li>所提出的解决方案的性能通过大量的模拟实验进行评估，并与具有off-grid角度的各种实验设置中的最先进方法进行比较。此外，还提供了与 Cramér-Rao 下限 (CRLB) 的比较作为基准。</li>
</ul>
<p>​	结果表明，所提出的 CNN：a）在低 SNR 情况下的 DoA 估计方面优于其竞争对手； b) 即使对于一小部分快照，估计也具有弹性，无论源的角度间隔如何； c) 展示了在 SNR 不匹配的情况下增强的鲁棒性，并且 d) 能够以非常小的误差和高置信度推断源和 DoA 的数量。</p>
<p>​	本文的其余部分组织如下：在第二节中，我们介绍了信号模型。在第三节中，我们介绍了用于 DoA 预测的 CNN，在第四节中我们讨论了所采用的训练方法。第五节介绍了模拟结果，在第六节中，我们总结并强调了我们的结论。</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231007150707443.png" alt="image-20231007150707443" style="zoom:67%;" />



<h2 id="信号模型"><a href="#信号模型" class="headerlink" title="信号模型"></a>信号模型</h2><p>​	窄带模式下 N 元件传感器阵列（存在 K 个远场源）的标准模型为：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231007150738810.png" alt="image-20231007150738810" style="zoom:67%;" />

<p>这里 $A(θ) &#x3D; [a(θ_1), a(θ_2), . 。 。 , a(θ_K )] $为 N × K 数组流形矩阵，$ θ &#x3D; [θ_1, . 。 。 , θ_K ]^T $是（未知）源方向的向量，T 是收集的快照总数，$s(t) &#x3D; [s_1(t), . 。 。 , s_K (t)]^T$ 和$ e(t)$ 分别表示样本索引 $t$ 处的传输信号和加性噪声向量。模型 (1) 是通用的，因为它不依赖于阵列几何形状；然而，在这项工作中，为了简单起见，我们将考虑均匀线性阵列 (ULA) 配置。2 因此，阵列流形矩阵的列表示为</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231007151008801.png" alt="image-20231007151008801" style="zoom:67%;" />

<p>其中 $d$ 是阵列元件间距离，$λ &#x3D; c&#x2F;f$ 是载波频率 $f$ 处的波长，$c$ 是光&#x2F;声速。在这种情况下，$A(θ)$ 成为范德蒙矩阵。以下假设是窄带 DoA 估计文献中的典型假设：</p>
<p>​	A1) 源 DoA 是不同的。</p>
<p>​	A2）每个源信号遵循[31]中的无条件模型假设（UMA），它假设传输的信号是随机生成的（高斯信号）。此外，源是不相关的，导致对角源协方差矩阵：$\textbf{R}_s 	&#x3D; E[s(t)sH (t)] &#x3D; diag(σ12, …, σ2K )$。</p>
<p>​	A3) 加性噪声值是独立同分布 (i.i.d.) 零均值白色圆对称高斯分布，即$ e(t) ∼ CN (0, σ_e^2\textbf{I}_N ) $并且与源不相关。</p>
<p>​	A4) 每个快照之间没有时间相关性。</p>
<p>我们感兴趣的是根据测量 $y(1)。 。 。 y(T)$, 估计未知的 DoAs $\textbf{θ}$)。考虑 A1–A4，接收信号的协方差矩阵由下式给出：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008123735249.png" alt="image-20231008123735249" style="zoom: 67%;" />

<p>(3) 中$\textbf{R}_y$ 的统计丰富度允许估计最多 K ≤ N − 1 个不同的 DoA。然而，在实践中，（3）中的矩阵是未知的，并用其样本估计值代替</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008125243650.png" alt="image-20231008125243650" style="zoom: 67%;" />

<p>这里的$\textbf{R}_y$ 为无偏估计。</p>
<p>​	我们注意到假设 A2-A4 仅用于在所提出的 CNN 训练过程中生成数据。即使违反这些假设，网络也可以做出预测。然而，随着测试与训练数据的偏差变得更大，估计变得不太准确。违反这些假设也会对 DoA 估计领域的经典估计器产生不同的影响。</p>
<h2 id="用于-DOA-估计的深度卷积神经网络"><a href="#用于-DOA-估计的深度卷积神经网络" class="headerlink" title="用于 DOA 估计的深度卷积神经网络"></a>用于 DOA 估计的深度卷积神经网络</h2><p>​	在本节中，我们将 DoA 估计表述为多标签分类任务。在第 III-A 节中，我们介绍了数据管理和标记方法，而第 III-B 节则致力于描述学习预测 DoA 的 CNN 架构。卷积层从多通道输入数据中执行特征提取，随后，FC 层使用卷积层的输出通过预选网格推断 DoA 估计。</p>
<h3 id="A-数据管理和标签"><a href="#A-数据管理和标签" class="headerlink" title="A. 数据管理和标签"></a>A. 数据管理和标签</h3><p>​	DoA 预测被建模为多标签分类任务。对于 $\phi_{max }∈ ({1°…. 90°})$ 我们考虑 2G + 1 个分辨率为 $ρ$（以度为单位）的离散点，它们定义了一个网格$ G &#x3D; (−Gρ….−ρ, 0°, ρ… Gρ) ⊂ [−90°, 90°]$，使得 $\phi_{max} &#x3D; G_ρ$。在每个SNR级别，从集合G中选择K个角度，并根据（3）计算相应的协方差矩阵。所提出的 CNN 的输入数据 $\textbf{X}$ 是一个实值$ N × N × 3$ 矩阵，其第三维代表不同的“通道”。特别地，第一和第二通道是 $\textbf{R}_y$ 的实部和虚部，即 $\textbf{X}:,:,1 &#x3D; Re({\textbf{R}_y})$ 和 $\textbf{X}:,:,2 &#x3D; Im{\textbf{R}_y}$，而第三通道对应于相位条目，即 $\textbf{X}:,:,3 &#x3D; ∠{\textbf{R}_y}$。因此，CNN 的输入是 D 个数据点的集合，定义为 $X &#x3D; {\textbf{X}(1),…。 。 。 ，\textbf{X}(D)}$。</p>
<p>​	接下来，对于每个示例 $\textbf{X}(i)$，G 中的 $K $个训练角度被转换为具有$ K $个（其余为零）的二进制向量。例如，如果 $\phi_{max} &#x3D; 60°$ 并且所需分辨率为 $ρ &#x3D; 1°$，则网格变为 $G &#x3D; {−60°, . 。 。 , −1°, 0°, 1°, . 。 。 , 60°}$ 与 $|G| &#x3D; 121$ 个网格点；此外，角度对 {−60°, −59°} 对应于 121 × 1 二元向量 $z &#x3D; [1, 1, 0, . 。 。 , 0]^T $，作为所提出的 CNN 的相应标签&#x2F;输出。因此，根据所描述的过程，第i个标签z(i)属于集合$Z &#x3D; {0, 1}^{2G+1}$。因此，第 i 个训练示例由 (X(i), z(i)) 形式的对组成，导致训练数据集$ D &#x3D; {(X(1), z(1)), (X(2 ), z(2)), . 。 。 , (X(D), z(D))}$ 大小为 D。</p>
<p>​	根据众所周知的通用逼近定理 [32]，由多层感知器处理的具有单个隐藏层的前馈网络可以逼近 Rn 的紧凑子集上的连续函数。这个多标签分类任务的目标是引入一个 ML 假设，定义为从输入空间到输出空间的函数 f，即 f : $R^{N×N×3} → Z$。尽管真实的协方差矩阵用于训练网络，对于其测试和评估，使用（4）中的样本协方差，因为前者是未知的。为此，在 CNN 的测试阶段，所有输入示例都可以被视为训练的“看不见的数据”。</p>
<h3 id="拟议的-CNN-架构"><a href="#拟议的-CNN-架构" class="headerlink" title="拟议的 CNN 架构"></a>拟议的 CNN 架构</h3><p>非线性函数$ f$ 由 8 层 CNN 参数化，即</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008131112768.png" alt="image-20231008131112768" style="zoom: 67%;" />

<p>所提出的 CNN 的架构基于图像处理文献 [33]、[34] 中使用的标准卷积结构，由于我们问题的性质，需要进行一些修改。每个函数 {fi(·)}i&#x3D;1,…,4 表示一系列卷积层：$n_C &#x3D; 256 $个滤波器的 2D 卷积层，后面是批量归一化层 [35] 和修正线性单元 ReLU 层，即非线性激活函数 $ReLU(x) &#x3D; max(0, x) $将元素明智地应用于前一层的变量。此外，在 f4(·) 的 ReLU 层之后使用了扁平化层，它将最终卷积层的张量值输出整形为向量。对于大小为$ κ × κ $的内核，我们对 f1(·) 使用 $κ &#x3D; 3$，对其余卷积层使用 $κ &#x3D; 2$。对于 f1(·)，我们使用的步长 $δ $是 $δ &#x3D; 2$，对于其余卷积层（无填充），步长 $δ$ 是 $δ &#x3D; 1$。因此，对于每个 $n_C$ 滤波器，第一层卷积运算的数学表达式为输入数据$ X ∈ R^{N×N×3} $且内核 $K ∈ R^{κ×κ×3}$ 是维度为 M × M 的二维矩阵（输出每个过滤器的层数）由下式给出：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008131537696.png" alt="image-20231008131537696" style="zoom: 67%;" />

<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008131638918.png" alt="image-20231008131638918" style="zoom:67%;" />

<p>​	此后是 FC 层。每个函数 ${fi(·)}<em>{i&#x3D;5,6,7}$ 是一个密集层，分别具有 4096、2048 和 1024 个神经元，后面是 ReLU 层和 Dropout 层。后者以 30% 的概率随机将权重设置为零（不可训练的参数），因此网络被迫学习而不是记住数据。 Dropout 层还充当学习过程的正则化。第 $l$层FC通过一组权重 $\textbf{W}^{l} \in R^{V^{l} \times V^{l-1}}$ 和偏差 $\textbf{b}</em>{FC}^l \in R^{V^l}$将其输入 $c^{l-1} \in R^{V^{l-1}}$映射到输出 $c^l \in R^{V(l)}$ 。因此，第 $l-th$ FC 层的输出（非线性激活和 dropout 之前）由下式给出</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008133755255.png" alt="image-20231008133755255" style="zoom:67%;" />

<p>其中参数集 $v^l &#x3D; {\textbf{W}^l, b^l_{FC}}$ 以及 $V ^{[l −1]} × V ^{[l]} + V ^{[ l]}$ 条目（总计）在神经网络训练期间进行优化。最终（输出）层 f8(·) 由具有 2G + 1 个神经元的密集层组成，后跟 Sigmoid 层，该层将函数 $s(x) &#x3D; e^x&#x2F;(e^x + 1) $按元素应用于值上一层的值并返回 [0, 1] 中的值。选择 sigmoid 函数而不是 softmax 是因为存在 K 个标签，这些标签可以独立接收等于（在训练期间）或接近（在推理期间）1 的值。</p>
<p>​	因此，CNN 的输出是预测标签每个条目的概率，对于输入数据$\textbf{ X}_{(i)}$ 表示为</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008134057899.png" alt="image-20231008134057899" style="zoom:67%;" />

<p>所提出的 CNN 的布局如图 1 所示。</p>
<p>![image-20231008134117156](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008134117156.png)</p>
<p>​	CNN 的训练是在训练数据集 D 上以有监督的方式离线执行的。特别是，由于采用的方法是多标签分类任务，因此我们尝试优化所有可训练参数 θ 的集合，其更新是通过后面进行的-通过最小化重建误差进行传播，即：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008134317927.png" alt="image-20231008134317927" style="zoom:67%;" />

<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008134327667.png" alt="image-20231008134327667" style="zoom:67%;" />

<h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>​	为了训练所提出的 CNN，我们考虑了$ ρ &#x3D; 1°$ 分辨率的网格，并考虑了两种情况：i) 使用 $φ_{max} &#x3D; 60°$（或 G &#x3D; 60），它定义了窄网格集 $G_n &#x3D; {−60°, 。 。 。 , 60°}$ 与 $|G_n| &#x3D; 121 $个网格点，ii) 使用 φmax &#x3D; 90°（或 G &#x3D; 90）定义宽网格集 Gw &#x3D; {−90°, 。 。 。 , 90°} 与 |Gw| &#x3D; 181 个网格点。因此，DoA 二进制变换后 CNN 的输出维度为 121 或 181（取决于 φmax）。在每种情况下，所提出的网络都是单独训练的。我们利用真实协方差矩阵中的数据来：a）减少所需的训练数据数量，b）为 SCM 估计的各种快照启用 DoA 预测。该方法的主要优点之一是，由于问题被建模为多分类任务，因此还可以训练 CNN 来推断源的数量。因此，我们采用两种方法来训练 CNN：在第 IV-A 节中，我们考虑固定数量的源，这更适合先验已知源数量的情况。在第 IV-B 节中，网络针对不同数量的源进行了训练；因此，我们将第二种方法称为混合数量源训练方法，该方法更适合源&#x2F;目标数量未知的情况，例如在军事应用中。根据接下来描述的训练选项，评估所提出的模型的实验部分也分为两部分。在这两种方法中，训练都是根据使用网格上角度（Gn 或 Gw）和（3）中的真实协方差矩阵生成的合成数据离线执行的。</p>
<h3 id="A-固定源数量"><a href="#A-固定源数量" class="headerlink" title="A.固定源数量"></a>A.固定源数量</h3><p>​	在本节中，我们将描述先验已知源数量的情况下的合成数据生成过程。我们考虑 K &#x3D; 2，并从 G 中所有可能的组合生成 DoA 对，即 (2G+1 K ) 角度对。3 在每个 SNR 生成 (2G+1 K ) 角度对后，我们首先计算使用（3）中的真实协方差矩阵和第 III-A 节中描述的标签来计算相应的输入数据 X。对于窄角区域，我们设置 G &#x3D; 60，从而在每个 SNR 值生成 7260 个训练示例，而对于 G &#x3D; 90，我们每个 SNR 值生成 16290 个训练示例。为了训练所提出的 CNN，我们使用了低 SNR 的数据，即 {−20, −15, −10, −5, 0} dB，导致总共 D &#x3D; 5 · 7260 &#x3D; 36300 和 D &#x3D; 5 · 16290 &#x3D; 窄范围和宽范围的训练样本总数分别为 81450 个。我们观察到，在低 SNR 状态下进行训练（最坏情况场景训练方法）也足以在较高 SNR 下进行预测。一旦 CNN 经过训练，就可以在高 SNR 下执行预测，正如我们在第 V-B 节中通过实验证明的那样。虽然在每个单独的 SNR 上进行训练可以稍微提高所提出的 CNN 的性能（我们对此进行了测试，但改进并不显着），但它可能非常有限：a）每个 SNR 需要训练多个模型并存储它们的参数，b）在测试阶段，应该知道运行信噪比（通常情况并非如此）。因此，我们采取了低信噪比范围内的联合训练。</p>
<p>​	CNN 的训练是离线进行的。数据被随机分为训练集 (90%) 和验证集 (10%)。在图 2(a)、(b) 中，我们绘制了固定数量源的每个时期的训练和验证二进制准确度（顶部）和损失（底部），其中 (a) 对应于窄网格，(b ) 对应于宽的。由于验证损失低于训练损失，因此不会发生过度拟合。然而，应该注意的是，验证集仅包含on-grid的角度。然而，测试数据的情况并非如此，测试数据是a）从off-grid角度产生的，b）使用SCM估计的结果（针对选定数量的快照）而不是实际上未知的真实协方差矩阵产生的。为了更新&#x2F;优化 CNN 参数，我们采用 Adam [36]，初始学习率为 0.001，而 $β_1 &#x3D; 0.9，β_2 &#x3D; 0.999$；此外，学习率按一旦验证损失在 10 个 epoch 的耐心下达到稳定水平，那么就以因子为 0.7减少。批量大小设置为 32，网络训练了 200 轮。 CNN 使用 Tensorflow 作为后端在 Keras 中实现；操作系统是 Windows，运行在 3.8 GHz 的 Intel Xeon Gold 5222 处理器上，并配有 NVIDIA TITAN RTX GPU。</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008134936051.png" alt="image-20231008134936051" style="zoom:67%;" />

<h3 id="B-混合来源数量"><a href="#B-混合来源数量" class="headerlink" title="B. 混合来源数量"></a>B. 混合来源数量</h3><p>​	在这里，我们描述了混合源数量的训练方法，其范围从 1 到 $K_{max}$ 且 G &#x3D; 60（窄网格）。在实验（第 V-C 节）中，我们使用 $K_{max} &#x3D; 3$。每个 SNR 和每个 $k &#x3D; 1 。 。 。 、K_{max}$ 的生成如第 IV-A 节中所述。现在的区别在于，我们不再在每个 SNR 下拥有 7260 个训练样本，而是拥有 $Σ^{K_max}<em>{ k&#x3D;1} (2G+1&#x2F; k )$ &#x3D; 295361 个训练样本（所有 k &#x3D; 1, …, $K</em>{max}$ 的总和）。每个训练标签都是一个 121 × 1 的二进制向量，其中 1 的数量从 1 到 $K_{max} &#x3D; 3$（其余条目为零）。使用真实协方差矩阵，以类似的方式使用来自低 SNR {−15, −10, −5, 0} dB^4 的数据进行离线训练。因此，总共使用了 D &#x3D; 4 · 295361 &#x3D; 1181444 个训练样本。 K 现在变化的事实使得学习过程更加困难，如图 2(c) 所示。网络的训练参数与之前相同，只是批量大小增加到了 64。</p>
<p>​	尽管 DoA 估计文献中的标准方法（例如 MUSIC）可以估计最多 K &#x3D; N − 1 个源&#x2F;目标，但这对于使用神经网络来说并不简单。训练数据的大小呈指数级增长（不按比例缩放）。我们应该注意到，这不仅仅是所提出的 CNN 的限制，而是大多数基于 DL 的方法面临的挑战，因为需要执行详尽的训练 [19]、[21]、[22]。此外，在低SNR的特定场景下，即使对于MUSIC、EPSRIT和R-MUSIC等标准方法，$N-1$的估计也不是很准确。</p>
<h2 id="仿真结果"><a href="#仿真结果" class="headerlink" title="仿真结果"></a>仿真结果</h2><p>​	在本节中，我们提供了广泛的模拟结果，其中我们评估了所提出的 CNN 在各种设置下的 DoA 估计中的性能。首先，我们总结了与所提出的 CNN 进行比较的方法，其中假设源数量已知，我们评估了 CNN 的性能。在第二部分中，在针对混合数量的源（最多达到最大数量）对网络进行训练后，我们放松了源数量已知的假设，并提供了 CNN 能够预测源数量和源数量的结果。</p>
<p>​	在训练和测试实验阶段，我们考虑 ULA，其 N &#x3D; 16 个元件在半波长距离 (d &#x3D; λ&#x2F;2) 处等距分布。第一个卷积层的可训练参数数量为 7680 个，而第二、第三和第四卷积层各有 262912 个参数（包括对应于四个批量归一化层的每一个的 512 个可训练参数）。第一FC层有16781312个可训练参数；第二个是8390656；第三个有 2098176，第四个有 124025（G &#x3D; 60）和 185525（G &#x3D; 90）（取决于窄或宽网格的选择）。总共，所提出的 CNN 有大约 2820 万个参数需要训练。对于所有实验，SNR 的定义如[37]中所示：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008140801390.png" alt="image-20231008140801390" style="zoom:67%;" />

<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008140827798.png" alt="image-20231008140827798" style="zoom:67%;" />

<h3 id="与其他方法比较"><a href="#与其他方法比较" class="headerlink" title="与其他方法比较"></a>与其他方法比较</h3><p>下面，我们总结了与所提出的 CNN 进行比较时使用的方法：</p>
<p>1）[4]中的 MUSIC。 </p>
<p>2）[8]中的R-MUSIC。 </p>
<p>3）ESPRIT[5]，[38]。 </p>
<p>4）[6]、[7]、[38]中的酉 ESPRIT 表示为 UnESPRIT。 </p>
<p>5）[15]、[16]中的$l_{2,1}-SVD$。 </p>
<p>6）[21]中的MLP。</p>
<p>​	方法1-4）对应DoA估计文献中的经典方法；方法 5) 是基于 CS 的方法的良好代表，而方法 6) 用于与最近的基于 DL 的方法进行比较（另请参阅第 I 节以了解更多详细信息）。对于on-grid方法（MUSIC 和 $l_{2,1}-SVD$），采用的网格分辨率选择与 CNN 相同（$ρ &#x3D; 1°$）。对于 ESPRIT 和 UnESPRIT，都使用了最大子阵列重叠以及具有最大可能权重的行加权技术，在我们的例子中等于 N&#x2F;2 &#x3D; 8；此外，在两种实现中都使用了总最小二乘法（TLS）标准[38]。此外，我们还计算了两个 CRLB：$CRLB_{uncr}$，它表示 [39] 中引入的严格不相关源的界限；CRLB 表示基于 [31] 中针对相关源情况引入的无条件模型。我们应该注意到，后一个界限对于不相关源的情况也有效；然而，前者更严格。</p>
<h3 id="已知源数量"><a href="#已知源数量" class="headerlink" title="已知源数量"></a>已知源数量</h3><p>​	首先，我们考虑第 IV-A 节中介绍的训练策略。对于每个测试示例，CNN 的输出由 (13) 给出。假设我们知道源的数量 K，我们可以在 (13) 中选取 K 个最大概率；网格对应的索引是估计的DoA。</p>
<p>​	用于评估 CNN 的指标是经验 RMSE，其定义为：</p>
<img src="/images/文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本/image-20231008141144609.png" alt="image-20231008141144609" style="zoom:67%;" />

<p>​	1) 到达方向估计和误差：在第一组实验中，我们考虑低 SNR 状态下的三种 DoA 场景。首先，两个角距离为 Δθ &#x3D; 4.7°、SNR &#x3D; −10 dB 的信号撞击到阵列上，而第一个信号的方向从 −60° 到 55° 变化，步长为 1°（窄网格） 。两个源之间的角距离不包含在训练集中，因此，第二信号的角方向偏离用于网络训练的角方向。我们认为收集了 T &#x3D; 2000 个快照用于 SCM 估计。所提出的 CNN 预测的 DoA 如图 3（a）所示（实线对应于实际的 DoA），而图 3（h）中则绘制了相应的误差。此外，在图 3(b)、(d) 和 (e) 中，我们分别绘制了 MUSIC、2,1-SVD 和 MLP DoA 估计。 R-MUSIC估计的方向如图3（c）所示，其误差如图3（i）所示。最后，ESPRIT 和 UnESPRIT 的 DOA 估计如图 3(f)、(g) 所示，相应的误差如图 3(j)、(k) 所示。</p>
<p>![image-20231008141432914](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008141432914.png)</p>
<p>​	我们观察到，CNN 的误差位于区间 [−0.3°, 0.7°] 内。 MUSIC 在角度区域的边缘有一些非常大的误差； $l_{2,1}-SVD $具有较高方差的误差，并且 MLP 无法一致地估计每个样本索引处的两个 DoA（黑色圆圈）。 R-MUSIC 和 ESPRIT 表现出整体良好的性能，但它们在音程边缘附近显得较弱（{−60°, 60°} 附近变化较大），误差高达 ±2°。由于 UnESPRIT 的前向-后向平均特性，这一弱点得到了缓解。其误差位于区间[−1°, 1°]内。另一个有趣的事实是图 3(h) 中 CNN 误差的分布差异，与图 3(i)、(j) 和（k）。总体而言，CNN 的 DoA 估计中的经验 RMSE 为 0.30°； 5.01°  MUSIC； R-MUSIC 为 0.35°； 1° 对于 $l_{2,1}-SVD$； ESPRIT 为 0.35°，UnESPRIT 为 0.27°。应该指出的是，MLP 无法解析 116 个示例中的 18 个 DoA（图 3（e）中圈出的 DoA 估计）；因此，无法计算估计误差。本实验中 $l_{2,1}-SVD $方法使用的阈值为 $η &#x3D; 550$。</p>
<p>​	接下来，我们考虑 SNR &#x3D; 0 dB 时角距离为 Δθ &#x3D; 2.11° 的两个信号。第一个信号的方向从 -59.5° 变化到 57.5°，增量为 1°（窄网格）。在第二种情况下，两个信号都从 CNN 训练过程中看不见的方向撞击到阵列上。使用 T &#x3D; 200 个快照根据 SCM 估计来估计方向。 CNN 的 DoA 估计如图 4(a) 所示，相应的误差如图 4(h) 所示。此外，在图 4(b) 和 4(c) 中，我们分别绘制了 MUSIC 和 R-MUSIC DoA 估计。在图 4(d) 中，描述了$l_{ 2,1}-SVD$ DoA 估计，其相应的误差如图 4(i) 所示。 MLP估计的DoA如图4(e)所示。 ESPRIT 和 UnESPRIT DOA 估计如图 4(f)、(g) 所示，它们相应的误差分别如图 4(j)、(k) 所示。</p>
<p>![image-20231008142044136](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008142044136.png)</p>
<p>​	尽管角度之间的角度间隔很小，但我们观察到 CNN 的所有误差都位于区间 [−0.5°, 0.61°] 内。另一方面，MUSIC 有几个较大的误差，而 R-MUSIC 仅在靠近角度区域边界的地方有几个较大的误差（圈出），即 {−60°, 60°}。 $l_{2,1}-SVD$ 的性能似乎更稳健，并且与所提出的 CNN 相似。然而，我们应该注意到，η 阈值（噪声界限）的调整是$l_{ 2,1}-SVD$ 方法的主要缺点之一，因为在许多实际应用中操作 SNR 水平未知。$^5 $MLP 表现出较差的性能，无法解析当前设置中 118 个测试示例中的 103 个中的两个紧密间隔的角度（大误差的出现是次要的）。尽管 ESPRIT 和 UnESPRIT 与 MUSIC 和 R-MUSIC 相比有所改进，但它们在音程边缘附近仍然存在相当大的误差。对于 CNN，DoA 估计中的 RMSE 为 0.50°； 20.31° MUSIC； 11.17° R-MUSIC； $l_{2,1}-SVD$ 为 0.54°； ESPRIT 为 1.23°，UnESPRIT 为 0.83°。因此，我们得出的结论是，与竞争对手相比，所提出的 CNN 在低和中等 SNR 范围内的两个源的 DoA 估计中展示了强大且自动化（参数独立）的解决方案。本实验中$l_{2,1}—SVD$方法使用的阈值为η &#x3D; 60。</p>
<p>​	在第三个实验中，我们考虑两个DoA在更宽范围内的情况，即在[−90°，90°]内。我们遵循与第一个实验类似的设置。第一信号的方向θ1从-90°变化到85°，而θ2 &#x3D; θ1 + 4.7°； SNR 为 −10 dB，T &#x3D; 2000。由于在这种情况下网格$ G_w$ 有 181 个点（而不是 121），因此提出的 CNN 以类似的方式再次进行训练。 MLP 也是如此。最后，$l_{2,1}-SVD$ 的阈值设置为 $η &#x3D; 550$。图 5 显示了 $θ_1, θ_2 ∈ [−90°, 90°]$ 情况下的 DoA 估计。我们观察到所提出的 CNN 的性能非常出色，尤其是在估计具有挑战性的角区域（尽头）边缘附近。所有经典的 DoA 估计器，即 MUSIC、R-MUSIC、ESPRIT 和 UnESPRIT，都无法在边缘附近提供准确的估计。 MLP 未能解析 176 个测试示例中的 85 个角度，并且在边缘附近也存在较大误差。尽管 $l_{2,1}-SVD$ 在一定程度上表现出鲁棒性，但它仍然在边缘附近产生一些较大的误差。 CNN 估计的 RMSE 为 0.96°； 44.69° MUSIC； 32.22° R-MUSIC； 2,1-SVD 为 11.73°； ESPRIT 为 30.12°，UnESPRIT 为 26.49°。 CNN 的绝对最大（最差）误差为 5.7°，而其竞争对手在边缘附近的误差高达 160°（绝对最大）。</p>
<p>![image-20231008142839945](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008142839945.png)</p>
<ol start="2">
<li>RMSE 与 SNR：在本实验中，我们评估了所提出的 CNN 在 θ1 &#x3D; 10.11° 和 θ2 &#x3D; 13.3° 方向上两个源的 DoA 估计中的性能。在每个 SNR 级别，RMSE 是通过 $D_{te} &#x3D; 1000 $次蒙特卡罗 (MC) 运行计算得出的，而样本协方差是使用 T &#x3D; 1000 个快照来估计的。 RMSE 结果绘制在图6；此外，[39]中的$CRLB_{uncr}$是根据SNR计算的。在低 SNR 情况下，CNN 在 RMSE 方面表现出非常好的性能，接近稳健的 $l_{2,1}-SVD$。然而，与后一种方法相比，CNN 不需要调整任何类型的参数，这在实际应用中是一个主要优点，其中 SNR 水平要么未知，要么可能略有不同。结果还表明，在高 SNR 状态下，所有网格方法的 RMSE 下限，而只有无网格估计器 ESPRIT、R-MUSIC 和 UnESPRIT 达到 CRLB。这种性能适用于所有基于网格的方法，并且除非使用更精细的网格，否则无法提高。应该指出的是，尽管没有在这种情况下接受过训练，CNN 仍然能够在高 SNR 下预测足够的角度估计。我们也不应该绘制低于 -5 dB SNR 的 MLP RMSE 结果，因为无法（一致）解析 DoA（在一千个测试示例中至少有一个是单个 DoA 估计）。本实验中 $l_{2,1}-SVD$ 方法使用的阈值对应的 SNR 值是 η &#x3D; {1260, 700, 400, 230, 140, 100, 70, 70, 60, 60, 60}。</li>
</ol>
<p>![image-20231008143250418](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008143250418.png)</p>
<ol start="3">
<li>RMSE 与快照数量 T ：在此设置中，我们尝试估计两个源在 −10 dB SNR 下的 DoA，同时快照数量 T 从 100 到 10000 变化。对于 T 的每个值，RMSE 为平均超过 1000 次 MC 运行。第一个源的方向是-13.18°，第二个源的方向是-9.58°（off-grid）。在图 7 中，描述了每种方法的估计 RMSE（两个轴均为对数刻度）。据观察，对于（相对）少量的快照（最多 T &#x3D; 500），与所有经典 DoA 估计器（MUSIC、R-MUSIC、ESPRIT、UnESPRIT）相比，CNN 表现出稳健的行为。随着快照数量的增加，网格的有限分辨率限制了基于网格的估计器的性能。值得注意的是，CNN 的性能与$l_ {2,1}-SVD$ 方法类似，但具有不依赖于控制估计的任何参数的微调的额外优点。 MLP 无法解决此设置中的角度（因此不包括在内）。本实验中$l_{2,1}-SVD $方法使用的阈值为 T 的相应值 η &#x3D; {130, 180, 270, 410, 570, 910, 1280}。</li>
</ol>
<p>![image-20231008143549947](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008143549947.png)</p>
<ol start="4">
<li><p>RMSE 与角度间隔 Δθ：图 8 包括不同角度间隔 Δθ ≥ 1° 的两个源的 DoA 估计中的 RMSE 结果。对于每个 Δθ 值，RMSE 是 1000 次 MC 运行的平均值。第一个源的 DoA 为 θ1 &#x3D; −13.8°（off-grid），第二个方向为 θ2 &#x3D; θ1 + Δθ，而 SNR&#x3D; −10 dB 且 T &#x3D; 500。我们观察到，对于紧密分离的角度 (1° ≤ Δθ &lt; 4°）CNN 和 $l_{2,1}-SVD$ 能够解析角度，而其他方法则无法提供准确的 DoA 估计。此外，CNN 和$l_{ 2,1}-SVD$ 的性能对于所有角度间隔几乎是恒定的。我们还可以观察到，对于小源分离，$l_{2,1}-SVD$ 和 CNN 都击败了 CRLB。一种可能的解释是估计者并非无偏。 MLP 只能解析角度间隔 Δθ &gt; 6° 的 DoA（因此仅提供 10° 和 14° 的结果）。本实验中 2,1-SVD 使用的阈值为 η &#x3D; 290。</p>
<p>![image-20231008143856029](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008143856029.png)</p>
</li>
</ol>
<p>5）对SNR不匹配的鲁棒性：在之前的所有模拟中，在每个SNR下，我们设置$σ_1^2 &#x3D; σ_2^2 &#x3D; 1$并根据方程计算$σ_e^2$。 （12）。然而，在实践中并不总是能保证完全了解准确的 SNR 水平。在此设置中，我们评估了所提出的方法针对 SNR 不匹配的鲁棒性。考虑两种情况。</p>
<p>​	第一个实验紧密遵循第 V-B1 节中第二个实验在 0 dB SNR 下的设置（相同的角度方向和快照数量 T &#x3D; 200）。噪声方差为 $σ_e^2 &#x3D; 1$；然而，我们考虑的不是单位功率，而是对源功率的小扰动，$σ_1^2 &#x3D; 0.7 和 σ_2^2 &#x3D; 1.25$。因此，实际 SNR 现在为 -1.549 dB，而不是 0 dB。所有方法的 DoA 估计如图 9 所示。值得注意的是图 9(a) 中所提出的 CNN 和图 9(d) 中的 $l_{2,1}-SVD $的性能。对于后一种方法，阈值 η &#x3D; 60 经过优化，可在 0 dB SNR 下使用。我们观察到$l_ {2,1}-SVD$ 的误差分散性更大，而 CNN 则表现出增强的鲁棒性。正如我们在图 9(b)、(c)、(f)、(g)、(j) 和 (k) 中观察到的，经典估计器，即 MUSIC、R-MUSIC、ESPRIT 和 UnESPRIT 具有很大的误差。 MLP 无法解析 118 个角度对中的 71 个。估计的 RMSE 为：CNN 为 0.46°； 21.46°  MUSIC； R-MUSIC 为 15.79°； 2,1-SVD 为 0.70°； ESPRIT 为 1.49°，UnESPRIT 为 0.91°。</p>
<p>![image-20231008144347885](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008144347885.png)</p>
<p>​	在第二个实验中，我们考虑相隔 4° 的两个角度方向。第一个源的方向范围从 -59.43° 到 55.57°，递增步长为 1°，而快照数量为 T &#x3D; 1000。我们考虑$ σ_e^2 &#x3D; 10$，并对源功率产生一个小扰动，其中 $σ_1^2 &#x3D; 0.7$ 且 $σ_2^2 &#x3D; 1.25$，这导致实际 SNR&#x3D; −11.549 dB，而不是 −10 dB。图 10 显示了所有方法的 DoA 估计。在图 10(a) 和 (h) 中，我们得到了所提出的 CNN 的 DoA 估计和误差，RMSE&#x3D;0.76°。在图 10(b) 和 (c) 中，MUSIC 和 R-MUSIC DoA 估计的 RMSE 分别等于 20.26° 和 15.85°。 $l_{2,1}-SVD$ 的 DoA 估计和误差如图 10(d) 和 (i) 所示，RMSE&#x3D;1.42°（阈值设置为 η &#x3D; 400）。 MLP DoA估计如图10(e)所示；然而，116 个角度对中的 41 个没有得到解析（因此，没有 RMSE）。最后，ESPRIT和UnESPRIT的DoA如图10(f)、(g)所示，它们的误差如图10(j)、(k)所示。它们的 RMSE 分别为 1.49° 和 1.08°。从这两个实验中，我们有证据表明，所提出的 CNN 对 SNR 不匹配的恢复能力优于所有竞争对手。</p>
<p>![image-20231008144943443](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008144943443.png)</p>
<p>(6）相关信号：到目前为止，我们假设源信号是不相关的。在下面的实验中，我们评估了所提出的 CNN 在相关信号情况下的性能。特别是，我们考虑 θ1 &#x3D; 10° 和 θ2 &#x3D; 13.8° 方向上的两个源，其中 Rs &#x3D; [ 1 ρ ρ 1 ] 在 −10 dB SNR 下，而 T &#x3D; 1000 个快照用于 SCM 估计。我们让相关系数 ρ 从 0（不相关）到 1（完全相关信号）变化，并计算图 11 中每个 ρ 值的 RMSE（Dte &#x3D; 1000 次 MC 运行的平均结果）。值得注意的是 CNN（建议）、UnESPRIT 和$l_{2,1}-SVD$（阈值 η &#x3D; 400）的性能，它们受相关系数增加的影响较小。然而，我们应该强调，在低信噪比下，所采用的信号模型（无条件）对 ρ 的增加具有显着的不敏感性，正如[31]中提到的，这可以通过 CRLB（常数）的行为来验证。</p>
<h3 id="未知源的数量"><a href="#未知源的数量" class="headerlink" title="未知源的数量"></a>未知源的数量</h3><p>​	在本节的第二部分中，根据第 IV-B 节训练网络后，我们在不使用有关传输源数量的信息的情况下测试 CNN。因此，我们放宽了源数量先验已知的假设，只考虑它们的最大数量 Kmax &#x3D; 3 是已知的（在训练中使用）。在模型训练后 ，我们可以使用置信度 p ̄ 并选择网格点 i &#x3D; 1,…。 。 。 , 2G + 1 ，概率为 pˆi ≥ p ̄ 。 （13）。但是，不能保证源 DoA 的数量现在等于 K；事实上，预测方向集的基数可能从零到 K^max（其中 K^max Kmax）变化。因此，RMSE 指标不再是适合评估CNN的性能。为此，我们采用豪斯多夫距离，对于两个集合 A 和 B 定义为：</p>
<p>![image-20231008145405107](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008145405107.png)</p>
<p>是有向差，6 d(α, B) &#x3D; inf{d(α, β)|β ∈ B} 且 d(α, β) &#x3D; |α − β|。为了对测试集进行评估，我们使用了其平均值和最大值，分别表示为 μ(dH) 和 max(dH)。简而言之，豪斯多夫距离衡量度量空间的两个子集彼此之间的距离，在 A 和 B 不具有相等基数的情况下尤其重要。例如，如果 A &#x3D; {−30°, 20°, 23°} 且 B &#x3D; {−30.2°, 20.15°, 22.83°}，则它们的 RMSE &#x3D; 0.18°，而 dH(A, B) &#x3D; 0.2°（这是对应于第一个角度的最大误差）。此外，如果 A &#x3D; {−30°, 21°}，则豪斯多夫距离为 dH(A, B) &#x3D; 1.83°，而如果 A &#x3D; {−30°, 51°}，则豪斯多夫距离变为 dH(A, B) &#x3D; 30.85°。后一个例子表明，大的偏差会受到度量的严重惩罚。最后，如果两个集合之一为空，则豪斯多夫距离将变为无穷大。</p>
<p>​	首先，我们考虑 K &#x3D; 1 至 K &#x3D; 3 的固定off-grid角度。第一个信号的方向为 7.8°；第二个信号的DoA为-2.6°，第三个信号的方向为2.6°（角度间隔为Δθ &#x3D; 5.2°）。我们根据豪斯多夫距离（a）μ（dH），b）max（dH）以及c）对源数量进行分类的能力来评估训练后的CNN的性能。对于每个 K &#x3D; 1、2、3，我们分别在 −10 和 0 dB SNR 下使用 T &#x3D; 3000 和 T &#x3D; 1000 个快照生成 10000 个测试示例。表 I 报告了 −10 和 0 dB SNR 下的 DoA 估计结果。在第二列中，我们提供选定的置信水平 p ̄；在第三列中，我们列出了平均豪斯多夫距离，表示为 μ(dH)；表的第四列列出了表示为 max(dH) 的最大豪斯多夫距离；最后，在最后一列中，我们包含了（用于比较）估计的 RMSE，假设每种情况下发射源 K 的数量已知（仅用于比较）。在 −10 dB SNR 时，对于 K &#x3D; 1, 2，平均和最大豪斯多夫距离不能为评估，因为在某些情况下没有识别出 DoA，导致预测集为空（豪斯多夫距离等于无穷大）。然而，正如我们接下来将看到的，这些情况非常罕见。此外，在图 12 中，我们评估了网络通过混淆矩阵对传输源数量进行分类的能力。图 12(a) 是 −10 dB SNR 时的分类结果（概率 %），而图 12(b) 是 0 dB 时的结果，置信水平列于表 I 中。主对角线对应于类型 I 或假阳性 (FP) 错误，而主对角线左侧的条目对应于类型 II 或假阴性 (FN) 错误。我们观察到，在大多数示例中，对于每个 K &#x3D; 1, 2, 3，所提出的 CNN 已正确识别传输源的数量。然而，我们观察到错误随着源数量的增加而增加，这使得这是预料之中的，因为问题已被考虑</p>
<p>![image-20231008145721289](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008145721289.png)</p>
<p>![image-20231008145913940](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008145913940.png)</p>
<p>​	在最后的实验中，我们让光源的方向在感兴趣的角度区域内变化。具体来说，对于 K &#x3D; 1，我们考虑 120 个测试示例，方向从 -59.8° 到 59.2°，递增步长为 1°。当 K &#x3D; 2 时，第一个信号的方向范围为 -59.8° 到 49.2°，第二个信号的方向范围为 -49.8° 到 59.2°（110 个示例，步长为 1°）。对于 K &#x3D; 3，第一个 DoA 范围为 -59.8° 至 39.2°，第二个 DoA 范围为 -49.8° 至 49.2°，第三个 DoA 范围为 -39.8° 至 59.2°（100 个示例，步长为 1°）。测试数据生成于分别来自 T &#x3D; 3000 和 T &#x3D; 1000 快照的 SNR&#x3D; −10 和 SNR&#x3D; 0 dB。在不知道 K 的情况下的 DoA 估计结果如图 13（a-c）所示（-10 dB SNR 时）和图 13（d-f）（0 dB SNR 时）。在图 13(a) 中，我们观察到 K &#x3D; 1 的 DoA 估计在整个值范围内都是正确的。在图 13(b) 中，我们出现了 1 次 FN（II 类错误），而在图 13(c) 中，我们出现了 3 次 FN。所有 K &#x3D; 1, 2, 3 的置信水平设置为 p ̄ &#x3D; 0.8。尽管信噪比较低且传输源的数量未知，但所提出的 CNN 能够很好地估计（未知）方向。在 0 dB SNR 下相同设置的结果如图 2 和 3 所示。 13（d-f）。特别是，在图 13(d) 中，K &#x3D; 1，我们有一个 FP 和另一个 FN 误差。在图 13(e) 中，K &#x3D; 2，存在两个 FN 误差，而在图 13(f) (K &#x3D; 3) 中，K 的估计中存在单个 FN 误差。此外，在表 II 中，我们计算了 DoA 估计与实际方向的 Hausdorff 距离。对于正确识别的 DoA，由于网格不匹配，它们被估计有量化误差。当然，所提出的具有未知数量源的方法也可以应用于高 SNR 区域（我们可以用更少数量的快照获得足够的估计），而不必仅限于低 SNR 区域。然而，我们无法克服网格失配误差，因此与 0 dB SNR 时的结果相比，预计结果不会有太大改善。</p>
<p>![image-20231008150119067](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008150119067.png)</p>
<p>![image-20231008150126370](&#x2F;images&#x2F;文献阅读：A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES - 副本&#x2F;image-20231008150126370.png)</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>​	在本文中，我们引入了一种带有 2D 滤波器的深度卷积神经网络 (CNN)，用于低 SNR 下的 DoA 预测。特别是，我们通过考虑网格方法将角度估计建模为多标签分类任务。采用 2D 卷积层可以从多通道输入数据中提取特征并将信息传输到完全连接的层，从而在低 SNR 下实现稳健的 DoA 估计。提出了两种不同的训练策略：a）针对固定的源，b）针对不同数量的源。前者适用于先验已知源数量的情况，这是相关文献中的典型假设。将所提出的解决方案与最先进的方法进行比较，并且还提供了 Cramér-Rao 下界作为基准。所提出的 CNN 的性能根据各种设置下的离网角度的 RMSE 进行评估：i）固定和变化的方向，ii）在广泛的 SNR 范围内，iii）针对各种快照，iv）各种角度间隔源和 v) 在 SNR 不匹配的情况下。结果表明：a) 增强的鲁棒性，b) 对大范围快照的估计的弹性，以及 c) 在低信噪比下解析紧密间隔角度的能力。</p>
<p>​	此外，我们还为源数量未知的情况引入了另一种应用程序培训方法。特别是，我们在低信噪比和不同数量的源下训练网络，从而产生概率角度估计器。报告的结果表明，所提出的 CNN 能够以高概率成功地与 DoA 联合识别（未知）数量的源。此外，预测角度是对真实 DoA 的足够好的估计。未来可能的研究方向包括派生具有更高网格分辨率（更细网格）的架构来识别大量目标。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/">http://example.com/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/DoA/">DoA</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/vHV5BKD/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/02/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20task3%20%20SELD%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E5%AE%9A%E4%BD%8D%E6%A3%80%E6%B5%8B/" title="文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data"><img class="cover" src="https://i.ibb.co/b7XKwb2/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data</div></div></a></div><div class="next-post pull-right"><a href="/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/" title="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays"><img class="cover" src="https://i.ibb.co/XVMYNX3/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/" title="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays"><img class="cover" src="https://i.ibb.co/XVMYNX3/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-25</div><div class="title">文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays</div></div></a></div><div><a href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"><img class="cover" src="https://i.ibb.co/rk6B9g0/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control</div></div></a></div><div><a href="/2024/04/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20SFANC-FxNLMS%20Algorithm%20for%20Active%20Noise%20Control%20based%20on%20Deep%20Learning/" title="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning"><img class="cover" src="https://i.ibb.co/5GmwJBS/image.png	" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-23</div><div class="title">文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning</div></div></a></div><div><a href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"><img class="cover" src="https://i.ibb.co/LSLjfkw/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-05</div><div class="title">文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL</div></div></a></div><div><a href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"><img class="cover" src="https://i.ibb.co/xSTFTsh/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-06</div><div class="title">文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control</div></div></a></div><div><a href="/2023/08/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ABlind%20Localization%20of%20Early%20Room%20Reflections%20Using%20Phase%20Aligned%20Spatial%20Correlation/" title="文献阅读:Blind Localization of Early Room Reflections Using Phase Aligned Spatial Correlation"><img class="cover" src="https://i.ibb.co/L56nmjT/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-25</div><div class="title">文献阅读:Blind Localization of Early Room Reflections Using Phase Aligned Spatial Correlation</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">28</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">基本信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">信号模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E-DOA-%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">用于 DOA 估计的深度卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E5%92%8C%E6%A0%87%E7%AD%BE"><span class="toc-number">5.1.</span> <span class="toc-text">A. 数据管理和标签</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%9F%E8%AE%AE%E7%9A%84-CNN-%E6%9E%B6%E6%9E%84"><span class="toc-number">5.2.</span> <span class="toc-text">拟议的 CNN 架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">训练方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-%E5%9B%BA%E5%AE%9A%E6%BA%90%E6%95%B0%E9%87%8F"><span class="toc-number">6.1.</span> <span class="toc-text">A.固定源数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-%E6%B7%B7%E5%90%88%E6%9D%A5%E6%BA%90%E6%95%B0%E9%87%8F"><span class="toc-number">6.2.</span> <span class="toc-text">B. 混合来源数量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BF%E7%9C%9F%E7%BB%93%E6%9E%9C"><span class="toc-number">7.</span> <span class="toc-text">仿真结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95%E6%AF%94%E8%BE%83"><span class="toc-number">7.1.</span> <span class="toc-text">与其他方法比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%B2%E7%9F%A5%E6%BA%90%E6%95%B0%E9%87%8F"><span class="toc-number">7.2.</span> <span class="toc-text">已知源数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AA%E7%9F%A5%E6%BA%90%E7%9A%84%E6%95%B0%E9%87%8F"><span class="toc-number">7.3.</span> <span class="toc-text">未知源的数量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">8.</span> <span class="toc-text">结论</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"><img src="https://i.ibb.co/xSTFTsh/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control"/></a><div class="content"><a class="title" href="/2024/05/06/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Transferable%20Latent%20of%20CNN-Based%20Selective%20Fixed-Filter%20Active%20Noise%20Control/" title="文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control">文献阅读: Transferable Latent of CNN-Based Selective Fixed-Filter Active Noise Control</a><time datetime="2024-05-06T12:48:52.939Z" title="发表于 2024-05-06 20:48:52">2024-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"><img src="https://i.ibb.co/rk6B9g0/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control"/></a><div class="content"><a class="title" href="/2024/05/06/%20%20%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-%20Real-time%20implementation%20and%20explainable%20AI%20analysis%20of%20delayless%20CNN-based%20selective%20fixed-filter%20active%20noise%20control/" title="文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control">文献阅读: Real-time implementation and explainable AI analysis of delayless CNN-based selective fixed-filter active noise control</a><time datetime="2024-05-05T16:00:00.000Z" title="发表于 2024-05-06 00:00:00">2024-05-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"><img src="https://i.ibb.co/LSLjfkw/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL"/></a><div class="content"><a class="title" href="/2024/05/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADEEP%20GENERATIVE%20FIXED-FILTER%20ACTIVE%20NOISE%20CONTROL/" title="文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL">文献阅读: DEEP GENERATIVE FIXED-FILTER ACTIVE NOISE CONTROL</a><time datetime="2024-05-05T11:30:38.183Z" title="发表于 2024-05-05 19:30:38">2024-05-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/04/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20SFANC-FxNLMS%20Algorithm%20for%20Active%20Noise%20Control%20based%20on%20Deep%20Learning/" title="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning"><img src="https://i.ibb.co/5GmwJBS/image.png	" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning"/></a><div class="content"><a class="title" href="/2024/04/23/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20Hybrid%20SFANC-FxNLMS%20Algorithm%20for%20Active%20Noise%20Control%20based%20on%20Deep%20Learning/" title="文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning">文献阅读: A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning</a><time datetime="2024-04-23T13:51:09.660Z" title="发表于 2024-04-23 21:51:09">2024-04-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMaximum%20Likelihood%20DOA%20Estimation%20of%20Multiple%20Wideband%20Sources%20in%20the%20Presence%20of%20Nonuniform%20Sensor%20Noise/" title="文献阅读: Maximum Likelihood DOA Estimation of Multiple Wideband Sources in the Presence of Nonuniform Sensor Noise"><img src="https://i.ibb.co/gtsst1D/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Maximum Likelihood DOA Estimation of Multiple Wideband Sources in the Presence of Nonuniform Sensor Noise"/></a><div class="content"><a class="title" href="/2024/01/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AMaximum%20Likelihood%20DOA%20Estimation%20of%20Multiple%20Wideband%20Sources%20in%20the%20Presence%20of%20Nonuniform%20Sensor%20Noise/" title="文献阅读: Maximum Likelihood DOA Estimation of Multiple Wideband Sources in the Presence of Nonuniform Sensor Noise">文献阅读: Maximum Likelihood DOA Estimation of Multiple Wideband Sources in the Presence of Nonuniform Sensor Noise</a><time datetime="2024-01-26T16:00:00.000Z" title="发表于 2024-01-27 00:00:00">2024-01-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>