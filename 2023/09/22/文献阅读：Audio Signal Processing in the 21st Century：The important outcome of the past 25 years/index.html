<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文献阅读: Audio signal processing in the 21st Century: the important outcomes of the past 25 years | Gavin</title><meta name="author" content="Gavin"><meta name="copyright" content="Gavin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基本信息论文地址：Audio Signal Processing in the 21st Century: The important outcomes of the past 25 years | IEEE Journals &amp; Magazine | IEEE Xplore引用格式：@article{richardAudioSignalProcessing2023,    title &amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="文献阅读: Audio signal processing in the 21st Century: the important outcomes of the past 25 years">
<meta property="og:url" content="http://example.com/2023/09/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AAudio%20Signal%20Processing%20in%20the%2021st%20Century%EF%BC%9AThe%20important%20outcome%20of%20the%20past%2025%20years/index.html">
<meta property="og:site_name" content="Gavin">
<meta property="og:description" content="基本信息论文地址：Audio Signal Processing in the 21st Century: The important outcomes of the past 25 years | IEEE Journals &amp; Magazine | IEEE Xplore引用格式：@article{richardAudioSignalProcessing2023,    title &amp;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.ibb.co/MRvpKwh/image.png">
<meta property="article:published_time" content="2023-09-22T15:13:24.000Z">
<meta property="article:modified_time" content="2023-09-22T15:13:24.000Z">
<meta property="article:author" content="Gavin">
<meta property="article:tag" content="Signal Processing">
<meta property="article:tag" content="overview">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.ibb.co/MRvpKwh/image.png"><link rel="shortcut icon" href="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"><link rel="canonical" href="http://example.com/2023/09/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AAudio%20Signal%20Processing%20in%20the%2021st%20Century%EF%BC%9AThe%20important%20outcome%20of%20the%20past%2025%20years/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文献阅读: Audio signal processing in the 21st Century: the important outcomes of the past 25 years',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-22 23:13:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.ibb.co/MRvpKwh/image.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Gavin"><img class="site-icon" src="https://i.ibb.co/3BGBwps/2c8b98a62fbc3615.png"/><span class="site-name">Gavin</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">文献阅读: Audio signal processing in the 21st Century: the important outcomes of the past 25 years</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-22T15:13:24.000Z" title="发表于 2023-09-22 23:13:24">2023-09-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-22T15:13:24.000Z" title="更新于 2023-09-22 23:13:24">2023-09-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/">音频信号处理</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="文献阅读: Audio signal processing in the 21st Century: the important outcomes of the past 25 years"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>论文地址：<a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/337906812_On_the_detection_quality_of_early_room_reflection_directions_using_compressive_sensing_on_rigid_spherical_microphone_array_data"><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10188482">Audio Signal Processing in the 21st Century: The important outcomes of the past 25 years | IEEE Journals &amp; Magazine | IEEE Xplore</a></a><br>引用格式：<br>@article{richardAudioSignalProcessing2023,<br>    title &#x3D; {Audio Signal Processing in the 21st Century: The important outcomes of the past 25 years},<br>    volume &#x3D; {40},<br>    issn &#x3D; {1558-0792},<br>    doi &#x3D; {10.1109&#x2F;MSP.2023.3276171},<br>    journaltitle &#x3D; {IEEE} Signal Processing Magazine},<br>    author &#x3D; {Richard, Gaël and Smaragdis, Paris and Gannot, Sharon and Naylor, Patrick A. and Makino, Shoji and Kellermann, Walter and Sugiyama, Akihiko},<br>    date &#x3D; {2023-07},<br>    note &#x3D; {Conference Name: {IEEE} Signal Processing Magazine},<br>}</p>
<h2 id="纪念信号处理社区75周年特别刊"><a href="#纪念信号处理社区75周年特别刊" class="headerlink" title="纪念信号处理社区75周年特别刊"></a>纪念信号处理社区75周年特别刊</h2><p>​	音频信号处理在其作为研究课题的发展中已经通过了许多里程碑。许多是众所周知的，例如19世纪下半叶留声机的发展以及与数字电话相关的技术在20世纪后期蓬勃发展，并且仍然是多种形式的热门话题。有趣的是，音频技术的发展不仅受到技术能力进步的推动，还受到消费者的高期望和客户参与度的推动。从环绕声电影院到最新的入耳式设备，人们喜欢声音，并很快将新的音频技术作为必不可少和预期的功能融入他们的日常生活。</p>
<p>​	1997年之前音频和声学信号处理（AASP）研究的一些主要成果总结在IEEE信号处理学会（SPS）成立50周年之际发表的一篇具有里程碑意义的论文中[1]。当时，绝大多数工作都是由构建模型来捕获所分析音频信号的基本特征并用一组有限的参数和组件表示它的目标驱动的。该领域现在已经超越了过去探索的基本特征。例如，此后提出了各种各样的语音&#x2F;音频信号模型，特别是围绕信号分解&#x2F;分解模型和稀疏信号表示。尽管如此，IEEE技术委员会（TC）关于AASP的整个研究领域正在见证向基于机器学习，特别是深度学习的数据驱动方法的范式转变。</p>
<p>​	在许多应用中，如果有适当的数据可用于训练模型，则此类数据驱动的模型可以获得最先进的结果。与此同时，我们不断努力收集非常有价值的公共数据收集（特别是带注释的数据），这些数据收集实际上对于数据驱动的算法至关重要。同时，为了促进可重复的研究并确定最先进的方法，出现了许多挑战，例如，环境声学表征（ACE），混响语音处理（REVERB），声源定位和跟踪，声源分离（SiSEC）、回声消除 （AEC）、专用于单麦克风降噪的深度噪声抑制以及声学场景和事件的检测和分类 （DCASE），自 2016 年以来一直是年度活动的主题（SPS 数据挑战：<a target="_blank" rel="noopener" href="https://signalprocessingsociety.org/publications-resources/data-challenges">Data Challenges | IEEE Signal Processing Society</a>出版物资源&#x2F;数据挑战;混响挑战：<a target="_blank" rel="noopener" href="http://reverb2014.dereverberation.com/">The REVERB challenge - Evaluating de-reverberation and ASR techniques in reverberant environments (dereverberation.com)</a>;SiSEC挑战：<a target="_blank" rel="noopener" href="https://sisec.inria.fr/">SiSEC 2018 – Community-Based Signal Separation Evaluation Campaign (inria.fr)</a>;和 DCASE 挑战：<a target="_blank" rel="noopener" href="https://dcase.community/challenge2022/">DCASE2022 Challenge - DCASE</a>。</p>
<p>​	本文没有详尽无遗，而是提供了该领域在过去25年中的重要成果的观点，也说明了纯数据驱动模型的出现。特别是，本文涵盖了信号模型和表示方面的研究;声学环境和声学场景的建模、分析和合成;信号增强和分离;音乐信息检索;以及声学场景和事件的检测和分类 （DCASE）。</p>
<p>​	文章的整体结构如下。我们在“进展和亮点（演变和突破）”部分讨论了该领域的进展主轴和亮点，强调了该领域的演变和突破。然后，在“新兴主题”部分，我们将重点放在过去25年中主要出现的新主题上，然后提出一些结论和观点。</p>
<h2 id="进步和亮点（演变和突破）"><a href="#进步和亮点（演变和突破）" class="headerlink" title="进步和亮点（演变和突破）"></a>进步和亮点（演变和突破）</h2><p>在[1]中已经讨论过的1997年之前的成就的基础上，我们在本节中总结了近年来的主要进展和亮点。</p>
<h3 id="建模和表示"><a href="#建模和表示" class="headerlink" title="建模和表示"></a>建模和表示</h3><p>我们首先讨论音频编码和信号建模的发展，重点是多声道音频通道编码。然后，我们描述了在声学环境的建模，分析和合成中所做的一些重要工作，并特别强调了房间脉冲响应（RIR）分析和综合。</p>
<h4 id="编码和信号建模"><a href="#编码和信号建模" class="headerlink" title="编码和信号建模"></a>编码和信号建模</h4><p>音频编码是该领域一个长期存在的话题，并导致了几个国际标准。[国际标准化组织&#x2F;国际电工委员会（ISO&#x2F;IEC）的以下音频编码标准可通过 <a target="_blank" rel="noopener" href="https://www.iso.org/standards.htmlby">404 - Page not found (iso.org)</a>在搜索窗口中提供括号中的数字和年份来访问。</p>
<p>​	该领域在 1990 年代进入了黄金时代，第一个国际标准的音频编码 MPEG-1 Audio （11172-3：1993），并扩展到多达 5 个 通道的多声道信号，MPEG-2 Audio （13818-3：1995）。MPEG-2 Audio是为多声道和多语言应用开发的，例如欧洲的数字无线电广播，与MPEG-1向后兼容。</p>
<p>​	但是，在没有向后兼容性约束的情况下，MPEG-2高级音频编码（AAC）（13818-7：1997）成功地实现了更高的主观质量。它仍然是当今音频编码算法的基础，并用于日本和拉丁美洲的地面电视广播。从应用的角度来看，MPEG-4 AAC （14496-3：2009） 和 MPEG-4 高效 （AAC HE-ACC） （14496-3：2009&#x2F;Amd 7：2018） 分别以 64 kbit&#x2F;s 和 32 kbit&#x2F;s 的速度为移动应用实现了足够的音频质量，并且目前使用最广泛。</p>
<p>​	带宽扩展（BWE）带来的重大改进之一，也称为子带复制（SBR），它仅对低频子带加高频功率包络信息进行编码，从而降低了比特率，听不见质量下降。解码器将低频频谱复制到高频频段，并通过传输的包络信息调整包络，重建全频段音频（见图1）。MPEG-4 AAC和HE-AAC用于各种消费产品，例如PC，平板电脑，手机和汽车导航系统，仅举几例。</p>
<p>![image-20230922164104618](&#x2F;images&#x2F;文献阅读：Audio Signal Processing in the 21st Century：The important outcome of the past 25 years&#x2F;image-20230922164104618.png)</p>
<p>​	MPEG-1 Audio 通过 MPEG-4 HEAAC 的历史是消除频域（变换编码）、时域（预测）和空间域（多通道编码）中输入音频的冗余。MPEG音频的下一阶段，MPEG环绕声（MPS）（230031：2007），基于双耳提示编码，利用空间域中的进一步冗余[2]。多声道音频信号以多个时频块（段）中的耳间电平差 （ILD） 和耳间时间差 （ITD） 的形式分解为单声道信号和附加空间信息。单声道数据由MPEG-4 AAC编码，少量侧面信息代表ILD和ITD。MPS 以 MPEG-4 AAC 比特率的三分之一实现了与 MPEG-4 AAC 相当的质量。绝对的主观品质对源信号，适用于地理上分散的演播室和广播电台之间的内容交付。MPEG 空间音频对象编码 （SAOC） （230032：2010） 根据每个音频对象的组成消除输入音频的冗余。输入音频信号由多个音频对象组成，这些对象是独立的音频源，例如单个乐器。每个音频对象通过对象级差异 （OLD） 和对象间交叉相干 （IOC） 在多个频率图块中表示。OLD 是下混信号能量的相对能量，下混信号是音频对象的组合。IOC 是下混信号的互相关。多个物体的下混信号由MPEG-4 AAC编码，而每个物体的DED和IOC编码为侧面信息。解码器从下混信号、OLD 和 IOC 中恢复每个对象。与MPEG SAOC的直接链接也可以与同时开发的（基于编码的）知情源分离的工作线建立[3]。</p>
<p>​	在MPEG SAOC之前，语音主导的音频信号和更通用的音频信号已经用不同的算法编码。MPEG统一语音和音频编码（USAC）（14496-3：2009&#x2F;Amd 3：2012）是第一个基于多个时频块中的输入信号分析结果，在面向语音算法和面向音频算法之间自动切换的音频编码框架。MPEG Audio家族的最新成员是MPEG-H（23008-3 2019），它是通用编码，包括3D音频（高阶立体声或HoA）。</p>
<p>​	音频编码最成功的应用是便携式音频播放器，以苹果的iPod为代表。第一个原型是1994年开发的Silicon Audio，它是2001年首次投放市场的iPod的前身。音频播放器后来扩展到包括视频数据处理。iPhone于2007年发布，是世界上第一款，它与大型显示屏相结合以制造平板电脑或与微型显示屏相结合以制造智能手表。这些方便的个人终端的历史可以在[4]中找到。尽管如此，尽管它们取得了巨大的成功，但音频播放器现在正逐渐被音乐流媒体所取代。</p>
<h4 id="声学环境建模、分析和合成"><a href="#声学环境建模、分析和合成" class="headerlink" title="声学环境建模、分析和合成"></a>声学环境建模、分析和合成</h4><h5 id="声脉冲响应的建模和分析"><a href="#声脉冲响应的建模和分析" class="headerlink" title="声脉冲响应的建模和分析"></a>声脉冲响应的建模和分析</h5><p>封闭空间中的声音传播的特点是多重反射和噪声的增加，两者都与声学环境有关。当声学信号在回声环境中传播时，它会被外壳中的房间面和物体反射，从而导致混响现象。与声源和麦克风相关的声脉冲响应 （AIR） 的持续时间通常为几百毫秒，相当于以典型采样率进行离散时间滤波的几千次抽头。声学环境中声能的衰减率由混响时间T60测量，T60是混响尾部的暴露衰减功率曲线从其初始值衰减60 dB所需的时间。典型的办公室的 T60 约为 300-400 毫秒，较大的房间可以接近 1 秒，具体取决于体积、形状和材料。感知的混响还取决于直接路径（包括早期反射）和尾部功率之间的比率，表示为直接混响比（DRR）。在相同的环境中，远距离声源将表现出较低的 DRR，并被认为具有更多的混响。</p>
<p>​	混响会降低语音信号的质量，在严重的情况下，特别是在噪声中，会降低其清晰度。自动语音识别 （ASR） 系统的单词错误率 （WER） 通常受到高混响水平的严重影响，尤其是对于低 DRR。</p>
<p>​	AIR包含整个反射模式，由直接路径，早期反射（由几个可区分的到达点组成）和晚期反射尾部组成，具有指数衰减的功率分布。后半部分是混响现象的主要原因。当声学环境是房间时，其 AIR 称为 RIR。在设计声学信号处理算法时，即使在温和的混响条件下，也应考虑室内声学，否则可能会严重降低其性能。因此，对RIR的特性进行建模和准确分析至关重要。</p>
<h5 id="房间模拟器、RIR-数据集和声场发生器"><a href="#房间模拟器、RIR-数据集和声场发生器" class="headerlink" title="房间模拟器、RIR 数据集和声场发生器"></a>房间模拟器、RIR 数据集和声场发生器</h5><p>应在混响条件下评估声学信号处理算法。这可以通过使用记录的 RIR 或使用房间模拟器来实现。这种模拟器的结果可能不太准确，但使用它们可以让该领域的研究人员生成大量示例。最近，随着需要大量和多样性训练数据的机器学习算法的出现，这一点变得非常重要。该领域是从Schröder（频域建模），Polack（时域建模）以及Allen和Berkely（镜像源）在声学方面的开创性工作演变而来的[5]。基于这些模型（特别是镜像源），开发了许多RIR发生器：RIR发生器（<a target="_blank" rel="noopener" href="https://github.com/ehabets/RIR-Generator">ehabets&#x2F;RIR-Generator: Generating room impulse responses (github.com)</a>），PyRoomAcoustics（<a target="_blank" rel="noopener" href="https://pyroomacoustics.readthedocs.io/en/pypi-release/modules.html">pyroomacoustics — Pyroomacoustics 0.7.3 documentation</a>）和gpuRIR（<a target="_blank" rel="noopener" href="https://pyroomacoustics.readthedocs.io/en/pypi-release/modules.html">pyroomacoustics — Pyroomacoustics 0.7.3 documentation</a>）。使用这些生成器，可以评估音频处理算法的性能，还可以训练数据驱动的方法。最近的进展使用数据驱动的方法（通常是生成对抗网络（GAN））改进了RIR的生成。</p>
<p>​	还提供真实世界 RIR 数据库，便于对算法进行可靠评估<a target="_blank" rel="noopener" href="https://www.dreams-itn.eu/index.php/dissemination/science-blogs/24-rir-databases">Room Impulse Response Databases - www.AcouSP.org (dreams-itn.eu)</a> 或者<a target="_blank" rel="noopener" href="https://github.com/RoyJames/room-impulse-responses">RoyJames&#x2F;room-impulse-responses: A list of publicly available room impulse response datasets and scripts to download them. (github.com)</a> 以及<a target="_blank" rel="noopener" href="https://asap.ite.tul.cz/downloads/mirage/">MIRaGe: Multichannel room Impulse Response database on Grid | (tul.cz)</a> 同时，还提出了噪声场发生器，包括各向同性噪声（<a target="_blank" rel="noopener" href="https://github.com/ehabets/INF-Generator%EF%BC%89%E5%92%8C%E9%A3%8E%E5%99%AA%E5%A3%B0%EF%BC%88https://github.com/ehabets/Wind-Generator%EF%BC%89%E3%80%82">https://github.com/ehabets/INF-Generator）和风噪声（https://github.com/ehabets/Wind-Generator）。</a></p>
<h5 id="房间特征的推断"><a href="#房间特征的推断" class="headerlink" title="房间特征的推断"></a>房间特征的推断</h5><p>表征外壳声学特性的参数可以从AIR和混响声音本身推断出来。这些参数可用于开发音频处理算法以及渲染声场面。混响时间、T60 和 DRR 在前面进行了测量。相干与扩散功率比（CDR）是声场的另一个属性，它决定了混响的影响，取决于声源-麦克风的距离和混响时间。如果直接路径和早期反射占主导地位，则声音被认为更连贯、更少扩散和更少混响。ACE挑战赛（<a target="_blank" rel="noopener" href="http://www.ee.ic.ac.uk/naylor/ACEweb%EF%BC%89%E8%87%B4%E5%8A%9B%E4%BA%8E%E4%B8%BA%E4%B9%8B%E5%89%8D%E7%9A%84%E6%88%BF%E9%97%B4%E5%A3%B0%E5%AD%A6%E5%8F%82%E6%95%B0%E5%BC%80%E5%8F%91%E5%92%8C%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%BC%B0%E8%AE%A1%E7%A8%8B%E5%BA%8F%E3%80%82%E6%9C%80%E8%BF%91%E4%B8%80%E4%B8%AA%E5%B8%A6%E6%9C%89%E6%B3%A8%E9%87%8A%E5%8F%8D%E5%B0%84%E7%9A%84RIR%E6%95%B0%E6%8D%AE%E5%BA%93%EF%BC%88%E2%80%9CdEchorate%E2%80%9D%EF%BC%89%E5%8F%AF%E7%94%A8%E4%BA%8E%E8%BF%9B%E4%B8%80%E6%AD%A5%E6%8E%A8%E8%BF%9B%E8%BF%99%E6%96%B9%E9%9D%A2%E7%9A%84%E7%A0%94%E7%A9%B6%EF%BC%88[The">http://www.ee.ic.ac.uk/naylor/ACEweb）致力于为之前的房间声学参数开发和基准测试估计程序。最近一个带有注释反射的RIR数据库（“dEchorate”）可用于进一步推进这方面的研究（[The</a> dEchorate dataset | Zenodo](<a target="_blank" rel="noopener" href="https://zenodo.org/record/4626590#.Y1cMoOxByAQ)%EF%BC%89">https://zenodo.org/record/4626590#.Y1cMoOxByAQ)）</a></p>
<h5 id="产生人工混响"><a href="#产生人工混响" class="headerlink" title="产生人工混响"></a>产生人工混响</h5><p>另一个蓬勃发展的研究方向是人工混响的产生，最流行的方法是反馈延迟网络[6]。传统上（来自施罗德的开创性工作），这些算法已广泛用于音乐制作，现在在游戏音频等新领域得到应用，包括虚拟现实和增强现实。</p>
<p>​	不同的研究角度更愿意考虑几何方法，它依赖于基于物理的模型。图像方法对于后期混响建模仍然难以处理，尤其是大型房间的混响。引入辐射转移法（RTM）是为了克服这一限制，因为它可以模拟后期混响的漫反射和声能衰减[7]。虽然很复杂，但后来证明RTM可以链接到反馈延迟网络，以构建高效的基于几何形状的混响器[8]。</p>
<h3 id="声学场景分析"><a href="#声学场景分析" class="headerlink" title="声学场景分析"></a>声学场景分析</h3><p>在这里，我们探索声学场景分析领域，使用排列在结构化星座（例如，球形和圆形）中或任意分布在声学外壳中的麦克风阵列。我们讨论了声源的定位和数据独立空间滤波的基本概念。我们进一步讨论了使用圆柱形或球形谐波域的波域表示[9]。虽然这些表示源自声场渲染和麦克风阵列波束成形，但现在经常用于声源定位、回声消除、主动噪声控制 （ANC） 和盲源分离 （BSS），下文将对此进行讨论。</p>
<h4 id="声学传感器网络"><a href="#声学传感器网络" class="headerlink" title="声学传感器网络"></a>声学传感器网络</h4><p>微型和低功耗设备设计的最新技术进步使所谓的无线声学传感器网络（WAS）得以部署。WAS由多个（通常是电池供电的）麦克风节点组成，每个节点都配备了一个或多个麦克风、一个信号处理单元和一个无线通信模块。这种麦克风星座的大空间分布会产生大量的空间信息，从而增加了麦克风子集（节点）接近相关声源的可能性。许多日常生活设备现在都配备了多个麦克风和相当大的音频处理能力。这些技术进步极大地推动了研究向前发展。WAS 可用于助听设备、语音通信系统、声学监控、环境智能等。</p>
<p>​	但是，这些新的临时体系结构中出现了新的挑战。通常，对于空间扩展网络，应评估传感器对给定任务的效用，并且对于多个传感器节点的相干信号处理，信号必须同步。特别是，当由于缺乏专用的中央处理设备或过于苛刻的传输&#x2F;处理要求而无法实现数据集中化时，必须依靠分布式处理，其中节点仅相互共享压缩&#x2F;融合的麦克风信号。下面将讨论各种算法的相应修改，例如波束成形及其非分布式版本。还采取了第一步，将移动机器人视为声学传感器网络的一部分。</p>
<h4 id="定位和跟踪"><a href="#定位和跟踪" class="headerlink" title="定位和跟踪"></a>定位和跟踪</h4><p>扬声器定位算法，主要是到达时间差（TDoA）和到达方向（DoA）估计，出现于 20 世纪 70 年代，其解决方案基于一对麦克风接收到的信号之间的归一化交叉相关性，即所谓的广义交叉相关性，后来扩展到多麦克风解决方案，其中最著名的是转向响应功率相位变换[10]，它将波束转向所有候选方向。特别是为了同时定位多个声源，通用频率估计和测向算法（如 MUSIC 和 ESPRIT）也被应用于声学领域，其中最突出的是圆柱和球谐波域。虽然 TDoA 和 DoA 估算在定位工作中占主导地位，但基于声场特征（如 CDR）的高效范围估算已被证明并应用于 WASN 的位置估算 [11]。</p>
<p>​	后来，很多人尝试采用统计方法，这些方法也有助于在动态场景中追踪信号源，包括贝叶斯方法，如卡尔曼滤波器的非线性扩展、粒子滤波器和概率假设密度滤波器，以及非贝叶斯方法，如递归期望最大化（EM）。</p>
<p>​	声反射可能会降低定位和跟踪算法的性能，特别是在高度混响的环境以及多个扬声器同时活动时的环境中。文献中有两种减轻混响对定位精度影响的方法。第一种范式侧重于提取声音从声源到麦克风的直接传播路径，同时尽量减少长 AIR 的影响。在第二种模式下，从麦克风信号中提取更多的一般特征。这些特征描述了声音传播的特点。然后，学习从这些高维特征到声源位置的映射。基于Manifold 学习的方法就采用了这种范式（参见 2019 欧洲信号处理大会教程：<a target="_blank" rel="noopener" href="https://sharongannot/">https://sharongannot</a>. group&#x2F;wp-content&#x2F;uploads&#x2F;2021&#x2F;06&#x2F;Speaker-Localization-on-Manifolds.pdf）。这是数据驱动方法趋势的一部分，特别是基于深度神经网络（DNN）的算法，可从特征向量推断声源位置[12]。最近的一项调查[13]探讨了许多这类方法。。</p>
<p>​	在同一模式下，同步定位和映射（SLAM）可用于声学领域（声学 SLAM），使机器人等配备麦克风的设备能够在环境中移动，探索、适应并与感兴趣的声源互动 [14]。</p>
<h4 id="空间滤波"><a href="#空间滤波" class="headerlink" title="空间滤波"></a>空间滤波</h4><p>基本上，所有多通道算法都或隐或显地利用传感器排列的空间多样性进行空间选择性信号处理。关于其他空间滤波方法，如与数据相关的波束成形和多通道信号源分离及信号提取，请参阅后面的章节，在这里，我们只考虑与数据无关的线性空间滤波，这在 [1] 中被描述为一个活跃的研究领域。此后，该领域取得的显著进展包括利用球谐波域[9]、[15]以及差分传声器阵列[16]、[17]，因为它们具有高指向性。这些技术还包括引入多项式波束成形技术，以实现高效灵活的波束转向；使用强大的优化算法对波束成形器进行非迭代设计，以满足鲁棒性约束（如白噪声增益）；以及将与物体相关的传递函数（如与头部相关的传递函数 (HRTF)）纳入波束成形器设计。虽然这些与数据无关的技术是为麦克风阵列信号处理而设计的，但它们也可用于扬声器阵列的声音重现。对于后者，下文将讨论更多针对重现的技术。</p>
<h3 id="声音场景合成"><a href="#声音场景合成" class="headerlink" title="声音场景合成"></a>声音场景合成</h3><h4 id="以听众为中心的双耳渲染"><a href="#以听众为中心的双耳渲染" class="headerlink" title="以听众为中心的双耳渲染"></a>以听众为中心的双耳渲染</h4><p>双耳渲染通常是指使用耳机重现空间声音的过程。一种流行的方法是使用 HRTF 滤波器。这种滤波器包含了能让听者定位声源的所有线索（尤其是频谱线索以及时间和强度上的耳际差异）[18]。然后，通过与空间中给定位置相对应的 HRTF 对输入的单声道信号进行滤波，为每只耳朵获取双耳信号。混响环境的渲染更为复杂，因为它需要为早期反射的每个方向叠加不同的 HRTF。然而，这种方法面临着重大挑战：难以获得大型 HRTF 数据库、难以获得通用和非个性化 HRTF，以及必须限制高质量渲染的计算复杂度。这些挑战推动了多个互补方向的广泛研究：1) 获取更通用的 HRTF，2) 获取使通用 HRTF 适应个人的方法（例如，通过平均 HRTF 集、使用人体测量法和借助物理模型），以及 3) 通过主观测试等方法从大型数据库中选择合适的 HRTF 集[19]。</p>
<h4 id="声场渲染"><a href="#声场渲染" class="headerlink" title="声场渲染"></a>声场渲染</h4><p>除了基于有限元和有限差分的通用数值方法外，声场信号处理开始利用波域表示法，特别是使用圆柱或球面谐波域[9]，目前已被用于解决声场渲染中的许多关键难题。</p>
<p>​	有一类重要的声音渲染技术依赖于围绕聆听区域的分布式扬声器的特定设置。根据立体声原理开发的特定格式适用于各种配置：6 个声道，包括一个额外的低频声道（5.1）；8 个声道（7.1）；12 个声道（10.2）；以及 24 个声道（22.2）。这些格式与定向声场编码有关，对扬声器的位置有严格的限制。此外，在实际应用中，空间错觉只在房间中心周围的一小块区域内（称为 “甜点”）是正确的。在这个甜蜜点之外，声音会被认为来自最近的扬声器。基于声场再现的方法，如最初由 Gerson 于 1973 年提出的环境声学（ambisonics）和 Berkhout 于 20 世纪 80 年代提出的波场合成（wave field synthesis），以及更普遍的空间频率域[20]，通过考虑扬声器的实际位置并为每个所需方向创建虚拟扬声器，解决了其中一些限制因素。在实践中，这些方法可以依赖基于对象的编码，并具有更广泛的优势。自问世以来，这些方法受到了广泛关注，并通过参数和非参数方法对声场再现进行了许多扩展，有可能将用于录音的小尺寸麦克风阵列扩展到任意扬声器布局[21]。一旦声场渲染也考虑到声学环境，房间均衡技术就变得十分必要，[22] 对此进行了研究。</p>
<h4 id="声信号增强"><a href="#声信号增强" class="headerlink" title="声信号增强"></a>声信号增强</h4><p>在本节中，我们将探讨用于声学信号增强的单麦克风和多麦克风方法，解决多种干扰源即回声和反馈混响、噪声和竞争信号。图 2 是前面讨论过的声学信号处理结构和声场合成的一般视图的问题。</p>
<h4 id="回声消除"><a href="#回声消除" class="headerlink" title="回声消除"></a>回声消除</h4><p>回声消除技术出现于 20 世纪 60 年代，但在过去 50 年中取得了长足的进步。在 SPS 50 周年庆典上[1]，人们探讨了回声消除领域的许多进展，包括递归最小二乘、仿射投影、子带和频域自适应滤波器以及复音检测器。AEC 成为免提电信系统，特别是现代视频会议系统的使能技术。</p>
<p>​	随后，考虑到再现系统的非线性因素，几项重要挑战得到了解决[23]、[24]，后者还利用 DNN 提高了性能。将（残余）回声消除、去混响和降噪结合起来的全局方法（通常通过应用后过滤阶段）也是一个广泛研究的课题。传统的频谱后过滤可以用 DNN 等现代结构来替代，以进一步提高性能。在存在叠加噪声的多麦克风环境中，设计 AEC 和波束成形阶段，使其交叉干扰最小化非常重要。步长控制从双音检测[25]发展到基于卡尔曼滤波器，最近又发展到基于深度学习的步长优化卡尔曼滤波器。Sondhi 的开创性著作中讨论的立体声 AEC 扩展到了多通道情况 [26] 和波域中的多输入、多输出 AEC。</p>
<p>​	关于声学回声和噪声控制领域的全面研究、其成就和仍然面临的挑战，可参阅 [26] 和 [27]。声学回声和噪声控制国际研讨会 (<a target="_blank" rel="noopener" href="https://www.iwaenc.org/">https://www.iwaenc.org</a>) 始于 1989 年，每两年举行一次，最初是专门讨论声学回声和噪声控制的，但其范围迅速扩展到其他音频信号处理领域，因此更名为声学信号增强国际研讨会。</p>
<h4 id="声反馈和-ANC"><a href="#声反馈和-ANC" class="headerlink" title="声反馈和 ANC"></a>声反馈和 ANC</h4><p>当扬声器回放麦克风信号时，就会产生声反馈（如公共广播系统和助听器）。这就形成了一个闭合回路，在系统变得不稳定并产生嚎叫效应之前，回路中的放大量受到限制 [28]。助听器佩戴者都知道这个问题，他们认为这是助听器的主要缺点之一，尤其是那些因中度和重度听力障碍而需要高增益的人。第一步，助听器的良好 “封闭 “验配通常可以稳定地提高有用增益。除此之外，20 世纪 90 年代还引入了自适应处理技术来消除反馈成分，近年来，通过使用更好的反馈路径模型和更好的反馈消除算法控制方法，这种方法一直在不断进步。在某些情况下，可用增益提高了 10 分贝之多，为听障人士带来了相应的好处。</p>
<p>​	ANC 系统以麦克风为基础，可捕捉音量外的声音并呈现 “反声音”，从而创建一个安静区域。商业产品（如降噪耳机、飞机和汽车应用）推动了这一领域的研究。除了在给定区域抑制噪音外，多区域渲染也成为一个理论和实践上都备受关注的课题[29]：在每个区域，多个同时活动的声源中只有一个能被听到，即形成一个 “明亮 “区域，而其他所有声源都被抑制，即形成一个 “黑暗 “区域。这项技术可应用于娱乐、商业和健康领域。例如，同一病房中多台电视机发出的声音可分别分区到每个病人的病床上。此外，电影的音量和渲染策略也可以根据听音室的不同座位进行不同的分区，形成 “亮区 “和 “暗区”。不同语言的对白也可在特定区域播放。</p>
<p>​	需要注意的是，一旦某个区域的不良声音的参考信息不需要通过麦克风获取，而是可以通过可观测的声源以及建模和测量的声音传播路径特征（如脉冲响应）估算出来，那么暗区和亮区的创建就简化为空间过滤任务了。</p>
<p>![image-20230922182728444](&#x2F;images&#x2F;文献阅读：Audio Signal Processing in the 21st Century：The important outcome of the past 25 years&#x2F;image-20230922182728444.png)</p>
<h4 id="消除混响"><a href="#消除混响" class="headerlink" title="消除混响"></a>消除混响</h4><p>与消除混响的目标相关，消除混响的课题也受到越来越多的关注，因为消除音频信号中的混响，特别是语音处理任务中的混响，有着明确的需求。与 AEC 不同，去混响是一个盲估计问题，因为没有消声信号的参考信号。20 世纪 90 年代末，虽然只有少数几种去混响算法，但去混响已成为一个蓬勃发展的研究领域，并达到了一定的成熟度，这体现在一本总结了十年密集活动的专著[30]和后来的全社会 REVERB 挑战赛上。基于单麦克风或者多麦克风去混响算法已经被提出并且评估了。RIR 尾部衰减的统计建模已被用于推导单麦克风消混响的频谱方法 [31]。</p>
<p>​	在多通道情况下，消除混响可视为盲均衡问题。因此，要么需要估算 RIR 系数，要么需要估算脉冲响应矩阵的逆矩阵。多通道均衡系统的估计程序包括子空间方法，即从接收麦克风信号的空间相关性矩阵的空子空间中提取 RIR，以及用于（部分）均衡多通道 RIR 和混响效应的最小平方方法。在卡尔曼滤波的同时，还可以采用（递归）EM 算法来联合估计消声信号和（时变）RIR。</p>
<p>​	加权预测误差（WPE）方法[32] 基于多通道线性预测（MCLP）实现了对时变彩色音源（如语音）的盲消除混响。为使 MCLP 能够处理此类音源，WPE 为其引入了两个必要的扩展：非稳态高斯音源模型和延迟预测，以防止 MCLP 白化固有的音源相关性。WPE 建立了一种新的有效 MCLP 算法，称为方差归一化延迟线性预测。此外，还提出了该方法的若干扩展方案，包括联合 BSS 和去混响以及 DNNs。</p>
<p>​	近年来，一些基于 DNN 的数据驱动方法被成功提出 [33]。我们相信，这一研究方向将会继续下去，探索包括现实世界场景中的噪声和时变性质在内的各个方面，很可能会将基于模型和数据驱动的范例结合起来。</p>
<h4 id="噪音抑制"><a href="#噪音抑制" class="headerlink" title="噪音抑制"></a>噪音抑制</h4><p>20 世纪 70 年代末，随着 Boll 和 Berouti 等人发表开创性的单通道频谱减法，降噪算法的发展势头迅猛。几年后，随着 Ephraim 和 Malah 发表关于估计频谱振幅和对数频谱振幅 (LSA) 的开创性论文，统计最优方法成为主流。除了在高斯假设下对语音频谱成分进行统计最优估计外，这些论文还引入了与信号存在不确定性下的估计相关的新概念，以及用于先验信噪比（SNR）估计的决策导向方法。后来还介绍了对其他概率分布（如超高斯分布）的扩展。关于 21 世纪前十年技术发展状况的全面概述，可参阅 [34] 和 [35]。</p>
<p>​	多年来，人们一直认为相位估计并不重要，只需估计语音的振幅频谱并用噪声相位进行增强即可，但最近的研究结果表明，估计相位也是有益的 [36]。</p>
<p>​	Lim 和 Oppenheim 采用了在传统语音压缩算法中广泛使用的语音信号全极建模方法，开发出一种迭代方案，在估计语音自回归系数和使用维纳滤波法增强语音信号之间交替进行。后来，在 EM 框架下使用了相同的语音模型，用卡尔曼滤波器代替了维纳滤波器。</p>
<p>​	文献[37]提出了一种早期的数据驱动型语音增强模型。在这项工作中，没有使用特定的语音 LSA 模型，而是在训练阶段利用整个 TIMIT 数据库推断出一个高斯混合模型。近年来，基于 DNN 的算法在单麦克风语音增强（包括降噪）领域占据了主导地位。其中许多算法都将降噪问题重塑为掩码估计。理想二进制掩码 (IBM) 可以确定每个时频分区是由语音还是噪声主导。另一种流行的掩码是理想比率掩码 (IRM)，它是 IBM 的柔和版本。文献[38]对许多降噪算法进行了调查，并对其他掩码（例如对相位也很敏感的复合 IRM）进行了探讨和比较。尽管已经取得了令人瞩目的成果，但仍然存在许多挑战。许多算法都需要大量的语音和噪声数据进行训练，而训练后的模型通常非常庞大。人们对开发 “薄型 “模型的兴趣与日俱增，这种模型可以部署在手机等边缘设备中，甚至可以部署在作为 WASN 节点的简单设备中。此外，在大多数电信应用中，低延迟是必须的，这使得语篇级算法无法胜任。还有许多具有挑战性的声学环境需要进一步改进算法。例如，繁忙的咖啡馆和酒吧通常都有咿咿呀呀的噪音。另一个例子是工厂和矿井，其特点是噪音水平极高。第三个例子是瞬态噪声，例如键盘输入和风噪。</p>
<h4 id="空间滤波（波束成形）"><a href="#空间滤波（波束成形）" class="headerlink" title="空间滤波（波束成形）"></a>空间滤波（波束成形）</h4><p>多通道接口提供的增强和分离能力通常高于单通道接口，尽管基于 DNN 的单麦克风解决方案现在也能提供具有竞争力的性能。我们已经探讨了与数据无关的波束成形器。本节将专门讨论与数据相关的波束成形器，即适应接收麦克风信号的波束成形器。早期的多麦克风语音增强和扬声器分离解决方案采用了具有自由声场传播模型的波束成形技术[1]。文献[39]总结了在波束成形器设计和先进的扬声器定位算法中纳入统计最优解决方案的早期尝试。</p>
<p>​	如前所述，声学箱体内的声场通常具有高阶多径传播的特点。如果传声器数量太少，无法形成窄波束，仅使用声场的直接路径可能无法提供足够的音质。因此，在波束成形器设计中考虑整个空气声道成为一种普遍做法。Jan 和 Flanagan 于 1996 年首次提出了针对声音的多重反射设计匹配滤波器的概念，但没有讨论空气声估计程序。</p>
<p>​	在 [40] 中，使用子空间跟踪程序估算了扬声器和麦克风阵列之间的声学传递函数（ATF），并将其用于设计最小方差无失真响应（MVDR）波束成形器。后来又引入了相对传递函数（RTF），并在 MVDR 设计中用来替代 ATF。RTF 包含声源和一对传声器之间声波传播的相关信息。关于麦克风阵列的文献中使用了多种最佳设计标准，即 MVDR、多通道维纳滤波器（MWF）及其变体语音失真加权 MWF [41]、最大 SNR 和线性约束最小方差（LCMV）。后者解决的是扬声器提取问题，与本文下一节讨论的（半）盲扬声器分离密切相关。在此，我们仅简要地指出，麦克风阵列处理和 BSS 范例现在密切相关，并经常相互借鉴。关于空间处理算法的进一步阐述，可参见文献[42]和[43]，包括空间处理标准和算法以及与盲说话者分离的关系。</p>
<p>​	通用多麦克风语音增强算法的目的是有选择地增强所需的语音源，抑制干扰源和环境背景噪声，而双耳算法的目的还在于保留声音场景的听觉印象。要做到这一点，就必须保留所需的语音源、干扰源和背景噪声的所谓双耳线索，这样才能利用听觉系统的双耳听觉优势，避免因声学和视觉信息不匹配而产生混淆。文献[43，第 18 章]介绍了一系列实现这一目标的多通道滤波器。</p>
<p>​	前面讨论的所有标准都是为集中处理而设计的。在 WASN 中，当集中处理变得过于昂贵时，就应采用最优或次优分布式算法。最佳分布式算法的结果应与集中式算法完全相同，而次优算法可能会导致性能下降。后一种算法的优势在于减少了通信带宽，有时甚至降低了本地计算负荷。有关 WASN 处理的典型挑战、几种重要应用和几种高效的节点融合方案，可参阅文献 [44]。前面许多标准的分布式版本都可以在文献中找到。在 WASN 处理中，采样率同步可能是保证系统正常运行的关键。文献中提供了多种再同步方案。</p>
<p>​	近年来，人们提出了大量基于 DNN 的空间处理算法。从目前的文献中可以发现三大趋势。第一种趋势是利用 DNN 估算统计最优波束成形器的构件。在第二种方法中，例如在 [45] 中，DNN 直接估计波束成形器的多通道权重。后者的优势在于能够超越传统的二阶统计，使用更具感知意义的成本函数（或在 ASR 应用中使用 WER 作为损失函数）实现波束成形器。不过，这种方法的鲁棒性可能不如 DNN 控制的波束成形器。第三种方法是直接将 DNN 应用于多通道数据，而不保留波束成形器结构。</p>
<h4 id="视听信号增强器"><a href="#视听信号增强器" class="headerlink" title="视听信号增强器"></a>视听信号增强器</h4><p>视觉模式显然可以支持增强任务。例如，将注意力集中在说话者的面部，特别是嘴唇上，就能从背景噪声和竞争说话者中提取出所需的说话者[46]。</p>
<h3 id="信号分离"><a href="#信号分离" class="headerlink" title="信号分离"></a>信号分离</h3><p>20 世纪 90 年代中期，源分离和盲源分离（BSS）日益受到关注，并逐渐从确定和超确定情况转向更具挑战性的欠确定情况，即可能存在比观测混合物更多的源[47]）。</p>
<h4 id="确定案件"><a href="#确定案件" class="headerlink" title="确定案件"></a>确定案件</h4><p>BSS 起源于独立分量分析（ICA）的应用。从 1999 年开始，每隔 1.5 年就会举行一系列 ICA 会议，对促进该领域的发展起到了重要作用。由于到达不同传感器的源信号的 TDoAs 和混响，音频信号在房间中会发生卷积混合。由于时域中的卷积混合物可以转换为频域中的瞬时混合物，因此频域 ICA 方法通过使用短时傅里叶变换（STFT）将时域信号转换为时频域信号。ICA 理论本身包含两个模糊点：输出顺序（排列）和输出振幅（缩放）。在频域 ICA 中，这两个问题都很严重。要解决排列问题，信号源的空间信息和频谱信息是关键信息。研究进一步表明，基于 ICA 的 BSS 会对干扰源形成空指向性模式，并抑制干扰源 [48]。</p>
<p>​	一个有趣的卷积混合物多通道盲信号处理框架，即卷积混合物的 Triple-N ICA [49]，定义了一个信息论成本函数，并能利用三个基本信号特性，即非白度、非高斯性和非平稳性。非负矩阵因式分解（NMF）[50］利用共同频率模式作为频率基来分离信号源。独立低阶矩阵分析 [51] 利用 ICA 的空间信息和 NMF 的频谱信息分离音源。与音频处理的大多数领域一样，深度学习方法现已得到广泛应用，其中一些是经典算法的改进变体。例如，多通道变异自动编码器（VAE）[52] 结合了 ICA 的空间信息和 DNN 的频谱信息。音源分离方法和算法的研究见 [43] 和 [53]。</p>
<h4 id="单声道分离"><a href="#单声道分离" class="headerlink" title="单声道分离"></a>单声道分离</h4><p>虽然多声道分离提供了一种反转混合的方法，但输入混合物仅以单声道呈现的情况（即单声道分离）提出了新的挑战。在这一领域出现的技术利用生成模型或各种掩蔽方法来恢复预期音源。这个问题也让训练有素的分离算法成为焦点，而不是盲法。</p>
<p>​	基于 NMF 的模型[50]是早期成功的方法。这些模型使用声音示例进行预训练，学习特定目标的频谱字典，并能从输入混合物中分离和重建目标。这种方法的变体包括多通道版本、卷积模型、基于各种频谱时态表征训练的模型、马尔可夫模型、概率公式等[54]，[55]。</p>
<p>​	虽然生成模型在当时表现出色，但另一种方法来自于一种首次用于多声道分离的技术。W-disjoint orthogonality [56]利用大多数声音时频表示的稀疏性，直接在频谱图上应用二进制掩码并分离出所需的声音。这一想法最初是针对立体声录音提出的，后来成为基于 NN 的方法的基石，并产生了一种解决分离问题的判别方法，其中每个时频点都被归类为有用或无用。深度聚类[57]是利用这一思想的一种流行的 NN 模型，它将混合物投射到一个空间中，在这个空间中，时间-频率分区可以被聚类，并相应地标记为属于独立声源。其他 NN 模型则省去了聚类步骤，从而失去了一些通用性，而是直接尝试在输入混合物的情况下预测掩码 [38]。后一种方法在最近的音源分离研究中占据了主导地位，提供了许多听起来令人印象深刻的方法，其应用范围从目前大多数语音通信常用的小型高效设备上语音增强器，到更大型的高质量离线模型，例如用于修复披头士乐队历史录音的获奖模型。这些模型探索了许多新的神经架构（U 型网、变压器等），并进行了大量扩展，如使用软掩码、学习潜空间而不是使用 STFT 的模型[58]、解决输出源顺序模糊性的模型（包络不变训练、用户引导目标的条件模型、直接优化感知指标的模型等）。图 3 举例说明了几种单声道分离方法。</p>
<img src="/images/文献阅读：Audio Signal Processing in the 21st Century：The important outcome of the past 25 years/image-20230922231904024.png" alt="image-20230922231904024"  />

<p>​	这些模型中的一个特例对音乐处理产生了重大影响。面向音乐的易用音源分离模型（<a target="_blank" rel="noopener" href="https://research.deezer.com/">https://research.deezer.com</a> &#x2F;projects&#x2F;spleeter）的发布催生了大量免费和商业软件，使用户可以将音乐录音分解成各组成乐器音轨，并进行自由混音和处理。除了是一种非常有用的工具外，它还增强了我们与录制音乐的互动方式，并为媒体互动开辟了新的途径，这些途径仍在探索之中。</p>
<p>​	虽然判别式模型性能优越，使用相对方便，但与生成式方法相比，它们的缺点是容易过度专业化，不能轻易扩展和重新部署用于其他用途。在如何制作通用分离器、利用有限的训练数据进行学习、将训练好的模型扩展到分布外工作等方面，仍存在一些未决问题。尽管这些演示听起来令人印象深刻，但在这一领域仍有许多工作要做。</p>
<h2 id="新出现的主题"><a href="#新出现的主题" class="headerlink" title="新出现的主题"></a>新出现的主题</h2><p>前文讨论的演变和突破的另一个视角是新课题的出现，这些课题在 20 世纪 90 年代几乎不存在，但如今已成为最热门的领域之一。</p>
<h3 id="客观评估"><a href="#客观评估" class="headerlink" title="客观评估"></a>客观评估</h3><p>在过去的 25 年中，语音和音频质量的客观评估已成为一个非常重要的课题。如果说语音&#x2F;音频质量评价和可懂度评估的最终手段是人类感知测试，那么众所周知，组织这种测试的成本高昂且繁琐。这就促使业界开发与感知更相关的客观音质指标。例如，在语音编码界的领导下，开发了几种语音质量度量方法（并进行了标准化），包括语音质量感知评估、感知客观听音质量评估和虚拟语音质量客观听音器。客观感知测量的概述见 [59]。助听器也广泛采用语言清晰度测量方法，如短时客观清晰度（STOI）和双耳扩展方法：修正的双耳 STOI。这些指标是评估人机接口设备中语音增强算法影响的事实标准。同样，还提出了几种衡量标准来评估音频质量（如音频质量感知评估和基于感知模型的质量）和音源分离算法的性能（尺度不变信号失真比、信号失真比和信号干扰比）[60]。此外，还提出了其他有趣的客观测量方法，特别是针对听力受损的听众（概述见 [61]）。</p>
<p>​	最近，我们还看到了一些训练有素的模型，它们可以输出感知分数[62]。这些模型可以在音频输入的基础上进行训练，直接预测用户的反应，为听众测试和其他计算缓慢的评估方法提供了快速的替代方法。当与可微分模型一起使用时，这些评估方法还可直接纳入算法优化，为训练与感知相关的系统提供新的可能性。</p>
<p>​	最后，当上述任何近似方法都被认为不够充分时，音频算法设计者可以求助于现代众包工具，这些工具可以接触到成千上万的听众，并以前所未有的样本量进行实验。这种能力彻底改变了当今音频产品的评估方式，并提供了比以往更强大的统计结果。</p>
<h4 id="MIR"><a href="#MIR" class="headerlink" title="MIR"></a>MIR</h4><p>音乐与信息研究所被定义为一个涵盖所有涉及音乐理解和建模的研究课题，并使用信息处理方法的领域（参见音乐与信息研究所路线图，网址：<a target="_blank" rel="noopener" href="http://www.mires.cc/wiki/index1a1d">http://www.mires.cc/wiki/index1a1d</a>. html?title&#x3D;Roadmap&amp;oldid&#x3D;2137）。从本质上讲，这是一个涉及机器学习、信号处理和&#x2F;或音乐学的跨学科领域。经过处理的音乐的性质也可以是多种多样的，包括原始音频信号、乐谱或录音的符号表示（例如，乐器数字界面格式）、图像（例如，乐谱的扫描版本），甚至是三维轨迹运动（例如，表演者的手势）。如果说 MIR 领域最初关注的是符号音乐处理，那么一些早期研究则为后来许多针对原始音频信号的工作铺平了道路，例如语音&#x2F;音乐辨别、节拍跟踪 [63] 以及音乐分析和识别 [64] 等等。早期的方法通常从语音识别方法中汲取灵感，大多使用旋律-频率倒频带系数（MFCC）作为特征，并使用高斯混合模型（GMM）、隐马尔可夫模型（HMM）、支持向量机（SVM）等统计模型。同样，对于未确定的源分离，在使用专用的低秩和稀疏分解（如基于 NMF 和匹配追求及其变体）方面也取得了重大进展。除了一些早期利用 NN 的论文外（例如，参见用于多音调估计的论文 [65]），深度学习的出现是最近的事（见图 4）。如今的主要趋势是在几乎所有应用中都考虑深度学习，例如在复调音乐源分离、音乐转录（估计旋律、和声、节奏、歌词等）、音乐风格转换和音乐合成等方面都取得了显著成就[66]。与语音识别一样，该领域也对端到端的深度学习方法产生了浓厚的兴趣，甚至用数据驱动的表征学习范式取代了传统的特征提取步骤。</p>
<p>​	音乐信号的多样性和复杂性也促使人们为表征学习和无监督学习开发新的定制方法，以避免特别繁琐的音乐信号标注阶段。最近推出的一种自监督音高估计方法尤为有趣[67]。除了 MIR 的主要历史领域，音乐合成也正在成为一个更强大的领域，尤其是围绕新的生成模型，取得了令人印象深刻的成果。近年来，我们目睹了在 DNN 和经典生成模型的交叉点上出现了所谓的深度生成模型。其中最流行的模型包括不同形式的自动编码器（包括 VAE、自回归模型和 GAN）。与此同时，特别是在音乐生成方面，还有一种趋势是重新使用经典的音频信号模型，例如语音生成的源滤波器模型和谐波+噪声模型。事实上，这些模型在混合神经架构中有着巨大的潜力，在可微分信号处理模块的形式下集成了音频模型[68]。混合架构确实特别有吸引力，而且已经显示出巨大的前景。例如，可微分音源生成模型的使用为数据高效的完全无监督音乐音源分离范例开辟了道路[69]。</p>
<p>![image-20230922233419003](&#x2F;images&#x2F;文献阅读：Audio Signal Processing in the 21st Century：The important outcome of the past 25 years&#x2F;image-20230922233419003.png)</p>
<h4 id="DCASE"><a href="#DCASE" class="headerlink" title="DCASE"></a>DCASE</h4><p>然而，最近增长最快的是 DCASE 领域[70]。这种日益增长的兴趣主要体现在随着 DCASE 社区的壮大及其 DCASE 研讨会的成功举办，DCASE 系列活动于 2016 年启动（参会人数从 2016 年的 68 人增至 2019 年的 201 人，平均 50%来自业界），其配套的国际挑战赛（提交的系统数量持续增长，从 2016 年的 84 个增至 2020 年的 470 个）也随之启动。(不过要注意的是，第一届 DCASE 挑战赛是在 2013 年举办的，但从 2016 年起才成为一项年度活动）。这种兴趣的稳步增长在提交给 ICASSP 的作品数量上清晰可见：2022 年，DCASE 是迄今为止提交作品数量最多的领域，音频作品占所有提交作品的 23.5%。尽管谢弗（Schaeffer）早在 20 世纪 60 年代就在其关于音乐对象的论文中报告了关于声音对象感知的重要工作，但人们通常将计算听觉场景分析和布雷格曼（Bregman）在 20 世纪 90 年代初开展的声学场景分析工作视为 DCASE 领域最具代表性的初步工作。</p>
<p>​	如图 5 所示，该领域也经历了从语音识别启发方法到完全数据驱动的深度学习方法的类似演变（尽管速度要快得多），其中弱监督方法尤为突出[71]。</p>
<p>![image-20230922234336871](&#x2F;images&#x2F;文献阅读：Audio Signal Processing in the 21st Century：The important outcome of the past 25 years&#x2F;image-20230922234336871.png)</p>
<p>​	除了 Sawhney 和 Maes 于 1997 年利用 NNs 所做的工作之外，2015 年之前的大多数研究都依赖于更为传统的聚类和机器学习范例，例如基于 SVM、GMM 和 HMM 的研究。此外，与 21 世纪初的音源分离和 MIR 领域类似，许多研究都采用了一些方法来获得紧凑、信息丰富的音频信号表示。稀疏分解方法、基于图像的特征和 NMF 尤为流行。随后，自 2014 年以来，深度学习获得了强劲的发展势头，并迅速成为主流架构。在 2016 年的 DCASE 挑战赛中，所有提交的声学场景分类系统中，除了四个系统之外，都涉及到了 NN，尽管它们还没有定义最先进的技术。两年后，在 2018 年的挑战赛中，表现最好的 30 个系统都是基于 DNN 的，这证实了 NN 在此类任务中无可争议的优势。尽管 DCASE 通常指的是单一领域，但实际上它考虑了多种应用，而这些应用都有各自的特点和限制。在声学场景识别这一较为成熟的应用中，提出了许多低复杂度的方法，在这方面，使用网络压缩、剪枝和知识提炼（例如，利用师生框架）是最成功的发展之一。对于声学事件检测和定位任务而言，可以轻松访问庞大的弱注释数据库。这显然伴随着一系列弱监督和少拍学习方法的出现，例如围绕原型网络和平均教师架构的方法，这些方法对于少拍学习、弱监督学习和领域适应特别有效。最后，值得一提的是数据增强技术的广泛应用，事实证明，在许多领域，数据增强技术都能非常有效地减少模型的过拟合。流行的数据增强技术包括 SpecAugment（带有特征扭曲和时频掩蔽）、音调偏移、时间拉伸、多通道录音中的混合和通道混淆、随机噪声添加等。</p>
<h3 id="功能强大的消费电子设备和快速的互联网连接"><a href="#功能强大的消费电子设备和快速的互联网连接" class="headerlink" title="功能强大的消费电子设备和快速的互联网连接"></a>功能强大的消费电子设备和快速的互联网连接</h3><p>最后，近年来，具有音频处理能力的功能强大的消费电子设备的部署速度非常快，而且通常不止一个麦克风。这些设备包括笔记本电脑、平板电脑、手机、智能手机和智能手表、智能扬声器、听力设备和可听设备、智能扬声器（亚马逊 Echo、苹果 HomePod 和谷歌 Home）以及虚拟现实和增强现实眼镜。也有专用的多麦克风硬件，如球形麦克风阵列（见 <a target="_blank" rel="noopener" href="https://mhacoustics.com/">https://mhacoustics.com</a> 上的 Eigenmike）。</p>
<p>​	与此同时，快速互联网连接的快速部署，特别是蜂窝网络数据的快速部署，极大地改变了我们的通信方式。从有线电话网络通信，到后来的蜂窝网络通信，我们现在广泛使用网络电话（VoIP）作为廉价可靠的替代方式。此外，远程会议工具（如 Google Meet、Skype 和 Zoom）已变得非常流行，最近在 COVID-19 大流行期间就证明了这一点，使每个人都能在家工作，并与同事和同事进行远程通信。网络电话技术促进了对 IP 音频编码、数据包丢失隐藏和回声消除的研究。同样，互联网的广泛使用也通过新的应用彻底改变了音乐消费，如音频和音乐检索及音乐识别[如流行的 Shazam 服务（<a href="https://www.shazam.com）]，以及自动推荐和生成播放列表的流媒体服务。">https://www.shazam.com）]，以及自动推荐和生成播放列表的流媒体服务。</a></p>
<h3 id="结论和展望"><a href="#结论和展望" class="headerlink" title="结论和展望"></a>结论和展望</h3><p>人们对 AASP 领域的兴趣与日俱增，开展了广泛的具体和跨学科研究与开发。伴随着这一增长的是 AASP TC，其 “使命是支持、促进和引领音频和声学信号处理所有领域的科学和技术发展”。多年来，特别是最近，该领域已转向几乎所有语音和音频应用的数据驱动方法。在某些情况下，所开发的方法是纯粹的端到端方法，所有的 “知识 “都是从数据中提取的。我们认为，这是一个非常强劲的趋势，未来还会进一步发展，但可能会从不同的角度发展。事实上，纯粹的端到端深度神经方法非常复杂，通常会过度参数化，而且在很多情况下，仍然相当难以解释。因此，人们有兴趣转向更节俭的数据驱动、可解释和可控制系统。一种可能的途径是将数据驱动范式的优势与高效信号模型相结合，建立新的基于模型（和混合）的深度神经架构。例如，在 MIR 中，可以将可微分的声音生成模型与深度学习架构结合起来，设计出可解释、更节俭、更高效的方法。这可能是未来开发新算法和新技术的途径之一，这些算法和技术将符合可持续生态发展和高道德标准，我们相信这将成为人们普遍关注的重要问题。</p>
<p>​	未来音频处理领域另一个值得关注的研究方向是联合（或协作）学习 [72]。事实上，大量数据现在都存储在设备上。因此，现在可以直接在设备上（通常称为边缘）训练更多的模型。这让我们能更好地考虑隐私问题（记录的数据不是集中存储的），但也给音频应用带来了许多挑战，尤其是在通信限制下的全局优化、异构数据学习（从不同和异构记录设备中记录的音频数据）以及部分和缺失数据学习方面。因此，集合了使用多个分布式设备进行机器学习和统计信号处理技术的 “联合学习 “似乎是未来音频处理应用的一个特别有前途的框架。拥有更强大处理单元和更快通信能力的更强大边缘设备必将支持这一趋势。</p>
<p>​	我们还预计，多模态处理将变得更加突出，在不久的将来，我们将看到更多利用视觉支持扬声器定位和分离的算法。除视听处理外，其他模式也将得到更广泛的应用，例如利用脑电信号进行脑信息语音分离[73]。</p>
<h3 id="作者"><a href="#作者" class="headerlink" title="作者"></a>作者</h3><p>Gaël Richard (<a href="mailto:&#103;&#97;&#x65;&#x6c;&#46;&#114;&#x69;&#x63;&#104;&#97;&#114;&#100;&#64;&#116;&#101;&#x6c;&#101;&#x63;&#111;&#109;&#45;&#112;&#97;&#x72;&#x69;&#x73;&#x2e;&#102;&#x72;">&#103;&#97;&#x65;&#x6c;&#46;&#114;&#x69;&#x63;&#104;&#97;&#114;&#100;&#64;&#116;&#101;&#x6c;&#101;&#x63;&#111;&#109;&#45;&#112;&#97;&#x72;&#x69;&#x73;&#x2e;&#102;&#x72;</a>) 于 1990 年在法国巴黎电信获得国家工程学位，并于 1999 年在巴黎电信获得博士学位。 1994 年获得巴黎萨克雷大学博士学位。他现在是巴黎电信学院音频信号处理专业教授，巴黎理工学院，91120 Palaiseau，法国，以及 Hi! 项目的科学联合主任。巴黎人工智能和数据分析跨学科中心。他是 IEEE 信号处理协会音频和声学信号处理技术委员会的前任主席。 2020 年，他因其在科学和技术方面的研究贡献而获得了 IMT 国家科学院大奖。 2022 年，他因音频混合深度学习项目获得了欧洲研究委员会高级资助。他的研究兴趣包括机器学习、音频和音乐信号处理。他是 IEEE 会士。</p>
<p>Paris Smaragdis (<a href="mailto:&#115;&#x6d;&#97;&#x72;&#97;&#103;&#x64;&#x69;&#115;&#x40;&#x69;&#101;&#101;&#101;&#46;&#111;&#x72;&#103;">&#115;&#x6d;&#97;&#x72;&#97;&#103;&#x64;&#x69;&#115;&#x40;&#x69;&#101;&#101;&#101;&#46;&#111;&#x72;&#103;</a>) 获得博士学位。 2001年获得麻省理工学院博士学位。现任伊利诺伊大学厄巴纳-香槟分校（Champaign, IL 61801, USA）计算机科学教授。他是 IEEE 杰出讲师 (20162017)，此前曾担任 IEEE 数据科学计划、IEEE 音频和声学信号处理技术委员会以及 IEEE 信号处理机器学习技术委员会主席。他还担任 IEEE 信号处理协会理事会成员。他目前是《IEEE Transactions on Audio, Speech, and Language》的主编。他的研究兴趣包括机器学习和信号处理。他是 IEEE 会士。</p>
<p>Haron Gannot (<a href="mailto:&#x73;&#104;&#x61;&#x72;&#x6f;&#x6e;&#x2e;&#x67;&#x61;&#110;&#x6e;&#x6f;&#116;&#x40;&#x62;&#x69;&#x75;&#46;&#x61;&#x63;&#x2e;&#105;&#108;">&#x73;&#104;&#x61;&#x72;&#x6f;&#x6e;&#x2e;&#x67;&#x61;&#110;&#x6e;&#x6f;&#116;&#x40;&#x62;&#x69;&#x75;&#46;&#x61;&#x63;&#x2e;&#105;&#108;</a>) 获得博士学位。 2000 年获得以色列特拉维夫大学电气工程博士学位。目前，他是以色列拉马特甘 5290002 巴伊兰大学工程学院的正教授。他目前担任 IEEE 音频、语音和语言处理汇刊的高级区域主席； IEEE信号处理杂志高级编委； IEEE 信号处理学会数据科学计划主席。他还于 2017 年至 2018 年担任 IEEE 音频和声学信号处理技术委员会主席。他是 2010 年声学信号增强国际研讨会和 2013 年 IEEE 声学信号增强国际研讨会的总联合主席。信号处理在音频和声学中的应用。他是 2022 年欧洲信号处理协会技术成就奖获得者。他的研究兴趣包括单麦克风和多麦克风阵列的统计信号处理和机器学习方法，应用于语音增强、降噪和说话者分离，以及二值化、去混响、说话者定位和跟踪。他是 IEEE 会士。</p>
<p>Patrick A. Naylor (<a href="mailto:&#x70;&#46;&#110;&#x61;&#x79;&#108;&#111;&#x72;&#64;&#x69;&#x6d;&#x70;&#101;&#114;&#x69;&#x61;&#108;&#46;&#x61;&#x63;&#x2e;&#x75;&#x6b;">&#x70;&#46;&#110;&#x61;&#x79;&#108;&#111;&#x72;&#64;&#x69;&#x6d;&#x70;&#101;&#114;&#x69;&#x61;&#108;&#46;&#x61;&#x63;&#x2e;&#x75;&#x6b;</a>) 获得博士学位。 1990 年在伦敦帝国理工学院获得博士学位。他现在是伦敦帝国学院 (SW7 2AZ London, U.K.) 的语音和声学信号处理教授。他曾在 IEEE 信号处理协会理事会任职，担任 IEEE 音频和声学委员会主席信号处理技术委员会，担任 IEEE Signal Processing Letters 的副主编，以及 IEEE Transactions on Audio, Speech, and Language Processing 的高级领域编辑。他是欧洲信号处理协会的前任主席。他的研究兴趣包括麦克风阵列信号处理、说话者二值化和定位，以及应用于双耳助听器的多通道语音增强。他是 IEEE 会士。</p>
<p>Shoji Makino (<a href="mailto:&#x73;&#46;&#x6d;&#97;&#x6b;&#x69;&#x6e;&#x6f;&#x40;&#105;&#x65;&#101;&#x65;&#x2e;&#111;&#114;&#x67;">&#x73;&#46;&#x6d;&#97;&#x6b;&#x69;&#x6e;&#x6f;&#x40;&#105;&#x65;&#101;&#x65;&#x2e;&#111;&#114;&#x67;</a>) 获得博士学位。 1993 年获得东北大学博士学位。现任日本早稻田大学（北九州 808-0135）教授。他曾在 IEEE 信号处理协会 (SPS) 理事会、SPS 技术方向委员会、SPS 奖项委员会和 SPS 院士评估委员会任职。他曾获得 30 个奖项，包括 2022 年 SPS Leo L. Beranek 优异服务奖、2014 年 SPS 最佳论文奖、2007 年 IEEE 信号处理机器学习竞赛奖、2006 年 ICA 无监督学习先锋奖。他的研究兴趣包括自适应滤波技术、声学信号处理以及语音和音频应用的机器学习。他是 SPS 杰出讲师和 IEEE 院士。</p>
<p>Walter Kellermann (<a href="mailto:&#119;&#97;&#108;&#116;&#x65;&#x72;&#x2e;&#x6b;&#101;&#x6c;&#108;&#x65;&#114;&#x6d;&#97;&#110;&#x6e;&#x40;&#102;&#x61;&#x75;&#x2e;&#x64;&#101;">&#119;&#97;&#108;&#116;&#x65;&#x72;&#x2e;&#x6b;&#101;&#x6c;&#108;&#x65;&#114;&#x6d;&#97;&#110;&#x6e;&#x40;&#102;&#x61;&#x75;&#x2e;&#x64;&#101;</a>) 获得工程博士学位。 1988 年在达姆施塔特工业大学获得博士学位。目前，他是埃尔兰根-纽伦堡大学（邮编：91058 Erlangen）的教授。他是 IEEE 信号处理协会的杰出讲师和技术指导副总裁。他是十项最佳论文奖的共同获得者，也是欧洲信号处理协会 (EURASIP) 集团技术成就奖的获得者。他的主要研究兴趣集中在基于物理模型和数据驱动的多通道方法，用于声学信号处理和语音增强。他是 EURASIP 院士和 IEEE 终身院士。</p>
<p>Akihiko Sugiyama (<a href="mailto:&#x61;&#46;&#115;&#x75;&#x67;&#x69;&#121;&#x61;&#x6d;&#x61;&#x40;&#105;&#x65;&#101;&#x65;&#46;&#111;&#x72;&#103;">&#x61;&#46;&#115;&#x75;&#x67;&#x69;&#121;&#x61;&#x6d;&#x61;&#x40;&#105;&#x65;&#101;&#x65;&#46;&#111;&#x72;&#103;</a>) 获得工程博士学位。 1998 年获得东京都立大学博士学位。目前就职于雅虎日本公司（东京 1028282）。曾任 IEEE 音频和声学信号处理技术委员会主席、IEEE Transactions on Signal Processing 副主编、IEEE Fellow 委员会成员。他曾担任 ICASSP 2012 的技术项目主席、前 IEEE 信号处理协会 (SPS) 杰出行业发言人、前 SPS 和 IEEE 消费者技术协会杰出讲师。他的研究兴趣包括音频编码和干扰&#x2F;噪声控制。他是 IEEE 会士。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li>M. Kahrs et al., “The past, present and future of audio signal processing,” <em>IEEE Signal Process. Mag.</em>, vol. 14, no. 5, pp. 30–57, Sep. 1997. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.1997.1179708">DOI: 10.1109&#x2F;MSP.1997.1179708</a>.</li>
<li>F. Baumgarte and C. Faller, “Binaural cue coding-part I: Psychoacoustic fundamentals and design principles,” <em>IEEE Trans. Speech Audio Process.</em>, vol. 11, no. 6, pp. 509–519, Nov. 2003. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TSA.2003.818109">DOI: 10.1109&#x2F;TSA.2003.818109</a>.</li>
<li>A. Ozerov, A. Liutkus, R. Badeau, and G. Richard, “Coding-based informed source separation: Nonnegative tensor factorization approach,” <em>IEEE Trans. Audio, Speech, Language Process.</em>, vol. 21, no. 8, pp. 1699–1712, Aug. 2013. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2013.2260153">DOI: 10.1109&#x2F;TASL.2013.2260153</a>.</li>
<li>A. Sugiyama and M. Iwadare, “The origin of digital information devices: The silicon audio and its family,” <em>APSIPA Trans. Signal Inf. Process.</em>, vol. 7, no. 1, Jan. 2018. <a target="_blank" rel="noopener" href="https://doi.org/10.1017/ATSIP.2017.16">DOI: 10.1017&#x2F;ATSIP.2017.16</a>.</li>
<li>J. Allen and D. Berkley, “Image method for efficiently simulating small-room acoustics,” <em>J. Acoust. Soc. Amer.</em>, vol. 65, no. 4, pp. 943–950, Apr. 1979. <a target="_blank" rel="noopener" href="https://doi.org/10.1121/1.382599">DOI: 10.1121&#x2F;1.382599</a>.</li>
<li>V. Välimäki, J. D. Parker, L. Savioja, J. O. Smith, and J. S. Abel, “More than 50 years of artificial reverberation,” <em>J. Audio Eng. Soc.</em>, Jan. 2016.</li>
<li>E. Nosal, M. Hodgson, and I. Ashdown, “Improved algorithms and methods for room sound-field prediction by acoustical radiosity in arbitrary polyhedral rooms,” <em>J. Acoust. Soc. Amer.</em>, vol. 116, no. 2, pp. 970–980, Sep. 2004. <a target="_blank" rel="noopener" href="https://doi.org/10.1121/1.1772400">DOI: 10.1121&#x2F;1.1772400</a>.</li>
<li>H. Bai, G. Richard, and L. Daudet, “Late reverberation synthesis: From radiance transfer to feedback delay networks,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 23, no. 12, pp. 2260–2271, Dec. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2015.2478116S">DOI: 10.1109&#x2F;TASLP.2015.2478116S</a>.</li>
<li>B. Rafaely, <em>Fundamentals of Spherical Array Processing</em>. Berlin, Germany: Springer-Verlag, 2015.</li>
<li>J. H. DiBiase, H. F. Silverman, and M. S. Brandstein, “Robust localization in reverberant rooms,” in <em>Microphone Arrays</em>, M. Brandstein and D. Ward, Eds. Berlin, Germany: Springer, 2001, pp. 157–180.</li>
<li>A. Brendel and W. Kellermann, “Distributed source localization in acoustic sensor networks using the coherent-to-diffuse power ratio,” <em>IEEE J. Sel. Topics Signal Process.</em>, vol. 13, no. 1, pp. 61–75, Mar. 2019. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/JSTSP.2019.2900911">DOI: 10.1109&#x2F;JSTSP.2019.2900911</a>.</li>
<li>S. Chakrabarty and E. A. Habets, “Multi-speaker DOA estimation using deep convolutional networks trained with noise signals,” <em>IEEE J. Sel. Topics Signal Process.</em>, vol. 13, no. 1, pp. 8–21, Mar. 2019. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/JSTSP.2019.2901664">DOI: 10.1109&#x2F;JSTSP.2019.2901664</a>.</li>
<li>P.-A. Grumiaux, S. Kitic´, L. Girin, and A. Guérin, “A survey of sound source localization with deep learning methods,” <em>J. Acoust. Soc. Amer.</em>, vol. 152, no. 1, pp. 107–151, Jul. 2022. <a target="_blank" rel="noopener" href="https://doi.org/10.1121/10.0011809">DOI: 10.1121&#x2F;10.0011809</a>.</li>
<li>C. Evers and P. A. Naylor, “Acoustic SLAM,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 26, no. 9, pp. 1484–1498, Sep. 2018. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2018.2828321">DOI: 10.1109&#x2F;TASLP.2018.2828321</a>.</li>
<li>D. P. Jarrett, E. A. Habets, and P. A. Naylor, <em>Theory and Applications of Spherical Microphone Array Processing</em>. Cham, Switzerland: Springer, 2017.</li>
<li>G. W. Elko, “Differential microphone arrays,” in <em>Audio Signal Processing for Next-Generation Multimedia Communication Systems</em>, Y. Huang and J. Benesty, Eds. Boston, MA, USA: Springer, 2004, pp. 11–65.</li>
<li>J. Benesty and C. Jingdong, <em>Study and Design of Differential Microphone Arrays</em>. Berlin, Germany: Springer-Verlag, 2012.</li>
<li>D. Begault and L. Trejo, “3-D sound for virtual reality and multimedia,” Nat. Aeronaut. Space Admin., Washington, DC, USA, NASA&#x2F;TM-2000-209606, 2000.</li>
<li>D. Poirier-Quinot and B. F. Katz, “On the improvement of accommodation to non-individual HRTFs via VR active learning and inclusion of a 3d room response,” <em>Acta Acoust.</em>, vol. 5, no. 25, pp. 1–17, Jun. 2021. <a target="_blank" rel="noopener" href="https://doi.org/10.1051/aacus/2021019">DOI: 10.1051&#x2F;aacus&#x2F;2021019</a>.</li>
<li>J. Ahrens and S. Spors, “Sound field reproduction using planar and linear arrays of loudspeakers,” <em>IEEE Trans. Audio, Speech, Language Process.</em>, vol. 18, no. 8, pp. 2038–2050, Nov. 2010. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2010.2041106">DOI: 10.1109&#x2F;TASL.2010.2041106</a>.</li>
<li>A. Politis, J. Vilkamo, and V. Pulkki, “Sector-based parametric sound field reproduction in the spherical harmonic domain,” <em>IEEE J. Sel. Topics Signal Process.</em>, vol. 9, no. 5, pp. 852–866, Aug. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/JSTSP.2015.2415762">DOI: 10.1109&#x2F;JSTSP.2015.2415762</a>.</li>
<li>S. Cecchi, A. Carini, and S. Spors, “Room response equalization - A review,” <em>Appl. Sci.</em>, vol. 8, no. 1, 2018. <a target="_blank" rel="noopener" href="https://doi.org/10.3390/app8010016">DOI: 10.3390&#x2F;app8010016</a>.</li>
<li>A. Stenger and W. Kellermann, “Adaptation of a memoryless preprocessor for nonlinear acoustic echo cancelling,” <em>Signal Process.</em>, vol. 80, no. 9, pp. 1747–1760, Sep. 2000. <a target="_blank" rel="noopener" href="https://doi.org/10.1016/S0165-1684(00)00085-2">DOI: 10.1016&#x2F;S0165-1684(00)00085-2</a>.</li>
<li>M. M. Halimeh, C. Huemmer, and W. Kellermann, “A neural network-based nonlinear acoustic echo canceller,” <em>IEEE Signal Process. Lett.</em>, vol. 26, no. 12, pp. 1827–1831, Dec. 2019. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/LSP.2019.2951311">DOI: 10.1109&#x2F;LSP.2019.2951311</a>.</li>
<li>T. Gänsler and J. Benesty, “The fast normalized cross-correlation double-talk detector,” <em>Signal Process.</em>, vol. 86, no. 6, pp. 1124–1139, Jun. 2006. <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.sigpro.2005.07.035">DOI: 10.1016&#x2F;j.sigpro.2005.07.035</a>.</li>
<li>J. Benesty, M. M. Sondhi, and Y. Huang, <em>Springer Handbook of Speech Processing</em>. Berlin, Germany: Springer-Verlag, 2008.</li>
<li>E. Hänsler and G. Schmidt, <em>Acoustic Echo and Noise Control: A Practical Approach</em>. New York, NY, USA: Wiley, 2005.</li>
<li>T. Van Waterschoot and M. Moonen, “Fifty years of acoustic feedback control: State of the art and future challenges,” <em>Proc. IEEE</em>, vol. 99, no. 2, pp. 288–327, Feb. 2011. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/JPROC.2010.2090998">DOI: 10.1109&#x2F;JPROC.2010.2090998</a>.</li>
<li>Y. J. Wu and T. D. Abhayapala, “Spatial multizone soundfield reproduction: Theory and design,” <em>IEEE Trans. Audio, Speech, Language Process.</em>, vol. 19, no. 6, pp. 1711–1720, Aug. 2011. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2010.2097249">DOI: 10.1109&#x2F;TASL.2010.2097249</a>.</li>
<li>P. A. Naylor and N. D. Gaubitch, <em>Speech Dereverberation</em>. London, U.K.: Springer-Verlag, 2010.</li>
<li>E. Habets, S. Gannot, and I. Cohen, “Late reverberant spectral variance estimation based on a statistical model,” <em>IEEE Signal Process. Lett.</em>, vol. 16, no. 9, pp. 770–773, Sep. 2009. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/LSP.2009.2024791">DOI: 10.1109&#x2F;LSP.2009.2024791</a>.</li>
<li>T. Nakatani, T. Yoshioka, K. Kinoshita, M. Miyoshi, and B.-H. Juang, “Speech dereverberation based on variance-normalized delayed linear prediction,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 18, no. 7, pp. 1717–1731, Sep. 2010. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2010.2052251">DOI: 10.1109&#x2F;TASL.2010.2052251</a>.</li>
<li>K. Han, Y. Wang, D. Wang, W. S. Woods, I. Merks, and T. Zhang, “Learning spectral mapping for speech dereverberation and denoising,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 23, no. 6, pp. 982–992, Jun. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2015.2416653">DOI: 10.1109&#x2F;TASLP.2015.2416653</a>.</li>
<li>R. C. Hendriks, T. Gerkmann, and J. Jensen, “DFT-domain based single-microphone noise reduction for speech enhancement: A survey of the state of the art,” in <em>Synthesis Lectures Speech Audio Processing</em>, vol. 9. San Rafael, CA, USA: Morgan &amp; Claypool, 2013, pp. 1–80.</li>
<li>P. C. Loizou, <em>Speech Enhancement: Theory and Practice</em>. Boca Raton, FL, USA: CRC Press, 2007.</li>
<li>T. Gerkmann, M. Krawczyk-Becker, and J. Le Roux, “Phase processing for single-channel speech enhancement: History and recent advances,” <em>IEEE Signal Process. Mag.</em>, vol. 32, no. 2, pp. 55–66, Mar. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.2014.2369251">DOI: 10.1109&#x2F;MSP.2014.2369251</a>.</li>
<li>D. Burshtein and S. Gannot, “Speech enhancement using a mixture-maximum model,” <em>IEEE Trans. Speech Audio Process.</em>, vol. 10, no. 6, pp. 341–351, Oct. 2002. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TSA.2002.803420">DOI: 10.1109&#x2F;TSA.2002.803420</a>.</li>
<li>D. Wang and J. Chen, “Supervised speech separation based on deep learning: An overview,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 26, no. 10, pp. 1702–1726, Oct. 2018. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2018.2842159">DOI: 10.1109&#x2F;TASLP.2018.2842159</a>.</li>
<li>M. Brandstein and D. Ward, <em>Microphone Arrays: Signal Processing Techniques and Applications</em>. Berlin, Germany: Springer-Verlag, 2001.</li>
<li>S. Affes and Y. Grenier, “A signal subspace tracking algorithm for microphone array processing of speech,” <em>IEEE Trans. Speech Audio Process.</em>, vol. 5, no. 5, pp. 425–437, Sep. 1997. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/89.622565">DOI: 10.1109&#x2F;89.622565</a>.</li>
<li>S. Doclo, A. Spriet, J. Wouters, and M. Moonen, “Speech distortion weighted multichannel wiener filtering techniques for noise reduction,” in <em>Speech Enhancement</em>. Berlin, Germany: Springer-Verlag, 2005, pp. 199–228.</li>
<li>S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multimicrophone speech enhancement and source separation,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 25, no. 4, pp. 692–730, Apr. 2017. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2016.2647702">DOI: 10.1109&#x2F;TASLP.2016.2647702</a>.</li>
<li>E. Vincent, T. Virtanen, and S. Gannot, Eds. <em>Audio Source Separation and Speech Enhancement</em>. Hoboken, NJ, USA: Wiley, 2018.</li>
<li>A. Bertrand, “Applications and trends in wireless acoustic sensor networks: A signal processing perspective,” in <em>Proc. IEEE Symp. Commun. Veh. Technol. Benelux (SCVT)</em>, 2011, pp. 1–6. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/SCVT.2011.6101302">DOI: 10.1109&#x2F;SCVT.2011.6101302</a>.</li>
<li>X. Xiao, “Deep beamforming networks for multi-channel speech recognition,” in <em>Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</em>, 2016, pp. 5745–5749. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICASSP.2016.7472778">DOI: 10.1109&#x2F;ICASSP.2016.7472778</a>.</li>
<li>A. Ephrat et al., “Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation,” 2018. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.03619">arXiv:1804.03619</a>.</li>
<li>P. Comon and C. Jutten, <em>Handbook of Blind Source Separation: Independent Component Analysis and Applications</em>. New York, NY, USA: Academic, 2010.</li>
<li>S. Araki, R. Mukai, S. Makino, T. Nishikawa, and H. Saruwatari, “The fundamental limitation of frequency domain blind source separation for convolutive mixtures of speech,” <em>IEEE Trans. Speech Audio Process.</em>, vol. 11, no. 2, pp. 109–116, Apr. 2003. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TSA.2003.809193">DOI: 10.1109&#x2F;TSA.2003.809193</a>.</li>
<li>H. Buchner, R. Aichner, and W. Kellermann, “A generalization of blind source separation algorithms for convolutive mixtures based on second-order statistics,” <em>IEEE Trans. Speech Audio Process.</em>, vol. 13, no. 1, pp. 120–134, Feb. 2005. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TSA.2004.838775">DOI: 10.1109&#x2F;TSA.2004.838775</a>.</li>
<li>P. Smaragdis, “Convolutive speech bases and their application to supervised speech separation,” <em>IEEE Trans. Audio, Speech, Language Process.</em>, vol. 15, no. 1, pp. 1–12, Jan. 2007. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASL.2006.876726">DOI: 10.1109&#x2F;TASL.2006.876726</a>.</li>
<li>D. Kitamura, N. Ono, H. Sawada, H. Kameoka, and H. Saruwatari, “Determined blind source separation unifying independent vector analysis and nonnegative matrix factorization,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 24, no. 9, pp. 1626–1641, Sep. 2016. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2016.2577880">DOI: 10.1109&#x2F;TASLP.2016.2577880</a>.</li>
<li>H. Kameoka, L. Li, S. Inoue, and S. Makino, “Supervised determined source separation with multichannel variational autoencoder,” <em>Neural Comput.</em>, vol. 31, no. 9, pp. 1891–1914, Sep. 2019. <a target="_blank" rel="noopener" href="https://doi.org/10.1162/neco_a_01217">DOI: 10.1162&#x2F;neco_a_01217</a>.</li>
<li>S. Makino, <em>Audio Source Separation</em>. Cham, Switzerland: Springer, 2018.</li>
<li>T. Virtanen, J. F. Gemmeke, B. Raj, and P. Smaragdis, “Compositional models for audio processing: Uncovering the structure of sound mixtures,” <em>IEEE Signal Process. Mag.</em>, vol. 32, no. 2, pp. 125–144, Mar. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.2013.2288990">DOI: 10.1109&#x2F;MSP.2013.2288990</a>.</li>
<li>P. Smaragdis, C. Févotte, G. J. Mysore, N. Mohammadiha, and M. Hoffman, “Static and dynamic source separation using nonnegative factorizations: A unified view,” <em>IEEE Signal Process. Mag.</em>, vol. 31, no. 3, pp. 66–75, May 2014. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.2013.2297715">DOI: 10.1109&#x2F;MSP.2013.2297715</a>.</li>
<li>S. Rickard and O. Yilmaz, “On the approximate W-disjoint orthogonality of speech,” in <em>Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</em>, 2002, vol. 1, pp. I–529–I–532. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICASSP.2002.5743771">DOI: 10.1109&#x2F;ICASSP.2002.5743771</a>.</li>
<li>J. R. Hershey, Z. Chen, J. L. Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in <em>Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</em>, 2016, pp. 31–35. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICASSP.2016.7471631">DOI: 10.1109&#x2F;ICASSP.2016.7471631</a>.</li>
<li>Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 27, no. 8, pp. 1256–1266, Aug. 2019. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2019.2915167">DOI: 10.1109&#x2F;TASLP.2019.2915167</a>.</li>
<li>M. Torcoli, T. Kastner, and J. Herre, “Objective measures of perceptual audio quality reviewed: An evaluation of their application domain dependence,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 29, pp. 1530–1541, Mar. 2021. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2021.3069302">DOI: 10.1109&#x2F;TASLP.2021.3069302</a>.</li>
<li>E. Vincent, R. Gribonval, and C. Fevotte, “Performance measurement in blind audio source separation,” <em>IEEE Trans. Audio, Speech, Language Process.</em>, vol. 14, no. 4, pp. 1462–1469, Jul. 2006. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TSA.2005.858005">DOI: 10.1109&#x2F;TSA.2005.858005</a>.</li>
<li>T. H. Falk et al., “Objective quality and intelligibility prediction for users of assistive listening devices: Advantages and limitations of existing tools,” <em>IEEE Signal Process. Mag.</em>, vol. 32, no. 2, pp. 114–124, 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/MSP.2014.2358871">DOI: 10.1109&#x2F;MSP.2014.2358871</a>.</li>
<li>A. R. Avila, H. Gamper, C. Reddy, R. Cutler, I. Tashev, and J. Gehrke, “Nonintrusive speech quality assessment using neural networks,” in <em>Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP)</em>, 2019, pp. 631–635. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/ICASSP.2019.8683175">DOI: 10.1109&#x2F;ICASSP.2019.8683175</a>.</li>
<li>E. D. Scheirer, “Tempo and beat analysis of acoustic musical signals,” <em>J. Acoust. Soc. Amer.</em>, vol. 103, no. 1, pp. 588–601, Jan. 1998. <a target="_blank" rel="noopener" href="https://doi.org/10.1121/1.421129">DOI: 10.1121&#x2F;1.421129</a>.</li>
<li>J. Foote, “An overview of audio information retrieval,” <em>Multimedia Syst.</em>, vol. 7, no. 1, pp. 2–10, Jan. 1999. <a target="_blank" rel="noopener" href="https://doi.org/10.1007/s005300050106">DOI: 10.1007&#x2F;s005300050106</a>.</li>
<li>M. Marolt, “A connectionist approach to automatic transcription of polyphonic piano music,” <em>IEEE Trans. Multimedia</em>, vol. 6, no. 3, pp. 439–449, Jun. 2004. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TMM.2004.827507">DOI: 10.1109&#x2F;TMM.2004.827507</a>.</li>
<li>G. Peeters and G. Richard, “Deep learning for audio and music,” in <em>MultiFaceted Deep Learning: Models and Data, J. Benois-Pineau and A. Zemmari, Eds</em>. Cham, Switzerland: Springer, 2021, pp. 231–266.</li>
<li>B. Gfeller, C. Frank, D. Roblek, M. Sharifi, M. Tagliasacchi, and M. Velimirovic´, “SPICE: Self-supervised pitch estimation,” <em>IEEE&#x2F;ACM Trans. Audio, Speech, Language Process.</em>, vol. 28, pp. 1118–1128, Mar. 2020. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TASLP.2020.2982285">DOI: 10.1109&#x2F;TASLP.2020.2982285</a>.</li>
<li>J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “DDSP: Differentiable digital signal processing,” in <em>Proc. Int. Conf. Learn. Representations (ICLR)</em>, 2020.</li>
<li>K. Schulze-Forster, C. S. J. Doire, G. Richard, and R. Badeau, “Unsupervised audio source separation using differentiable parametric source models,” 2022. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.09592">arXiv:2201.09592</a>.</li>
<li>D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and M. D. Plumbley, “Detection and classification of acoustic scenes and events,” <em>IEEE Trans. Multimedia</em>, vol. 17, no. 10, pp. 1733–1746, Oct. 2015. <a target="_blank" rel="noopener" href="https://doi.org/10.1109/TMM.2015.2428998">DOI: 10.1109&#x2F;TMM.2015.2428998</a>.</li>
<li>T. Virtanen, D. Ellis, and M. Plumbley, Eds. <em>Computational Analysis of Sound Scenes and Events</em>. Cham, Switzerland: Springer, 2018.</li>
<li>J. Konecný, H. B. McMahan, D. Ramage, and P. Richtárik, “Federated optimization: Distributed machine learning for on-device intelligence,” 2016. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02527">arXiv:1610.02527</a>.</li>
<li>E. Ceolini et al., “Brain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception,” <em>Neuroimage</em>, vol. 223, Dec. 2020, Art. no. 117282. <a target="_blank" rel="noopener" href="https://doi.org/10.1016/j.neuroimage.2020.117282">DOI: 10.1016&#x2F;j.neuroimage.2020.117282</a>.</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Gavin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/09/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AAudio%20Signal%20Processing%20in%20the%2021st%20Century%EF%BC%9AThe%20important%20outcome%20of%20the%20past%2025%20years/">http://example.com/2023/09/22/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AAudio%20Signal%20Processing%20in%20the%2021st%20Century%EF%BC%9AThe%20important%20outcome%20of%20the%20past%2025%20years/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Gavin</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Signal-Processing/">Signal Processing</a><a class="post-meta__tags" href="/tags/overview/">overview</a></div><div class="post_share"><div class="social-share" data-image="https://i.ibb.co/MRvpKwh/image.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/09/29/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFundamentals%20of%20Spherical%20Array%20Processing/" title="文献阅读:Fundamentals of Spherical Array Processing"><img class="cover" src="https://i.ibb.co/pRR1S1b/Eigenmike.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">文献阅读:Fundamentals of Spherical Array Processing</div></div></a></div><div class="next-post pull-right"><a href="/2023/09/15/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%EF%BC%9ACramer-Rao%20Lower%20Bound%20(CRLB)/" title="知识总结: 什么是Cramer Rao Lower Bound"><img class="cover" src="https://i.ibb.co/DKg0bzT/image.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">知识总结: 什么是Cramer Rao Lower Bound</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/09/10/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AA%20DEREVERBERATION%20ALGORITHM%20FOR%20SPHERICAL%20MICROPHONE%20ARRAYS%20USING%20COMPRESSED%20SENSING%20TECHNIQUES/" title="文献阅读: A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES"><img class="cover" src="https://i.ibb.co/HCJqn32/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-10</div><div class="title">文献阅读: A DEREVERBERATION ALGORITHM FOR SPHERICAL MICROPHONE ARRAYS USING COMPRESSED SENSING TECHNIQUES</div></div></a></div><div><a href="/2023/09/08/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AEstimation%20of%20Reflections%20from%20Impulse%20Responses/" title="文献阅读: Acoustic Reflection Localization fromRoom Impulse Responses"><img class="cover" src="https://i.ibb.co/4mDq6vK/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-08</div><div class="title">文献阅读: Acoustic Reflection Localization fromRoom Impulse Responses</div></div></a></div><div><a href="/2023/09/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADirection%20of%20Arrival%20Estimation%20of%20Reflections%20from%20Room%20Impulse%20Responses%20Using%20a%20Spherical%20Microphone%20Array/" title="文献阅读: Direction of Arrival Estimation of Reflections from Room Impulse Responses Using a Spherical Microphone Array"><img class="cover" src="https://i.ibb.co/m4s8sZr/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-03</div><div class="title">文献阅读: Direction of Arrival Estimation of Reflections from Room Impulse Responses Using a Spherical Microphone Array</div></div></a></div><div><a href="/2023/09/05/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALocalization%20of%20distinct%20reflections%20in%20rooms%20using%20spherical%20microphone%20array%20eigenbeam%20processing/" title="文献阅读: Localization of distinct reflections in rooms using spherical microphone array eigenbeam processing"><img class="cover" src="https://i.ibb.co/r3g5qL8/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-05</div><div class="title">文献阅读: Localization of distinct reflections in rooms using spherical microphone array eigenbeam processing</div></div></a></div><div><a href="/2023/08/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ALocalization%20of%20Multiple%20Speakers%20under%20High%20Reverberation%20using%20a%20Spherical%20Microphone%20Array%20and%20the%20Direct-Path%20Dominance%20Test/" title="文献阅读:Localization of Multiple Speakers under High Reverberation using a Spherical Microphone Array and the Direct-Path Dominance Test"><img class="cover" src="https://i.ibb.co/GQZT1sx/image-20230822154738866.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-25</div><div class="title">文献阅读:Localization of Multiple Speakers under High Reverberation using a Spherical Microphone Array and the Direct-Path Dominance Test</div></div></a></div><div><a href="/2023/09/03/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AOn%20the%20detection%20quality%20of%20early%20room%20reflection%20directions%20using%20compressive%20sensing%20on%20rigid%20spherical%20microphone%20array%20data%20205500/" title="文献阅读: On the detection quality of early room reflection directions using compressive sensing on rigid spherical microphone array data"><img class="cover" src="https://i.ibb.co/tPd3xBr/image.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-03</div><div class="title">文献阅读: On the detection quality of early room reflection directions using compressive sensing on rigid spherical microphone array data</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.ibb.co/ch2RrDp/20230822001522.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Gavin</div><div class="author-info__description">我的过去常常追赶着我</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">25</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/alexandergwm"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/alexandergwm" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:wenmiaogao@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">May you stay young forever, do the thing in your own zone.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">1.</span> <span class="toc-text">基本信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%AA%E5%BF%B5%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E7%A4%BE%E5%8C%BA75%E5%91%A8%E5%B9%B4%E7%89%B9%E5%88%AB%E5%88%8A"><span class="toc-number">2.</span> <span class="toc-text">纪念信号处理社区75周年特别刊</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E6%AD%A5%E5%92%8C%E4%BA%AE%E7%82%B9%EF%BC%88%E6%BC%94%E5%8F%98%E5%92%8C%E7%AA%81%E7%A0%B4%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">进步和亮点（演变和突破）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1%E5%92%8C%E8%A1%A8%E7%A4%BA"><span class="toc-number">3.1.</span> <span class="toc-text">建模和表示</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%92%8C%E4%BF%A1%E5%8F%B7%E5%BB%BA%E6%A8%A1"><span class="toc-number">3.1.1.</span> <span class="toc-text">编码和信号建模</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A3%B0%E5%AD%A6%E7%8E%AF%E5%A2%83%E5%BB%BA%E6%A8%A1%E3%80%81%E5%88%86%E6%9E%90%E5%92%8C%E5%90%88%E6%88%90"><span class="toc-number">3.1.2.</span> <span class="toc-text">声学环境建模、分析和合成</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A3%B0%E8%84%89%E5%86%B2%E5%93%8D%E5%BA%94%E7%9A%84%E5%BB%BA%E6%A8%A1%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">声脉冲响应的建模和分析</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%88%BF%E9%97%B4%E6%A8%A1%E6%8B%9F%E5%99%A8%E3%80%81RIR-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E5%A3%B0%E5%9C%BA%E5%8F%91%E7%94%9F%E5%99%A8"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">房间模拟器、RIR 数据集和声场发生器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%88%BF%E9%97%B4%E7%89%B9%E5%BE%81%E7%9A%84%E6%8E%A8%E6%96%AD"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">房间特征的推断</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%A7%E7%94%9F%E4%BA%BA%E5%B7%A5%E6%B7%B7%E5%93%8D"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">产生人工混响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E5%AD%A6%E5%9C%BA%E6%99%AF%E5%88%86%E6%9E%90"><span class="toc-number">3.2.</span> <span class="toc-text">声学场景分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A3%B0%E5%AD%A6%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.1.</span> <span class="toc-text">声学传感器网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%BD%8D%E5%92%8C%E8%B7%9F%E8%B8%AA"><span class="toc-number">3.2.2.</span> <span class="toc-text">定位和跟踪</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2"><span class="toc-number">3.2.3.</span> <span class="toc-text">空间滤波</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E9%9F%B3%E5%9C%BA%E6%99%AF%E5%90%88%E6%88%90"><span class="toc-number">3.3.</span> <span class="toc-text">声音场景合成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A5%E5%90%AC%E4%BC%97%E4%B8%BA%E4%B8%AD%E5%BF%83%E7%9A%84%E5%8F%8C%E8%80%B3%E6%B8%B2%E6%9F%93"><span class="toc-number">3.3.1.</span> <span class="toc-text">以听众为中心的双耳渲染</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A3%B0%E5%9C%BA%E6%B8%B2%E6%9F%93"><span class="toc-number">3.3.2.</span> <span class="toc-text">声场渲染</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A3%B0%E4%BF%A1%E5%8F%B7%E5%A2%9E%E5%BC%BA"><span class="toc-number">3.3.3.</span> <span class="toc-text">声信号增强</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9E%E5%A3%B0%E6%B6%88%E9%99%A4"><span class="toc-number">3.3.4.</span> <span class="toc-text">回声消除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A3%B0%E5%8F%8D%E9%A6%88%E5%92%8C-ANC"><span class="toc-number">3.3.5.</span> <span class="toc-text">声反馈和 ANC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E9%99%A4%E6%B7%B7%E5%93%8D"><span class="toc-number">3.3.6.</span> <span class="toc-text">消除混响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%99%AA%E9%9F%B3%E6%8A%91%E5%88%B6"><span class="toc-number">3.3.7.</span> <span class="toc-text">噪音抑制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2%EF%BC%88%E6%B3%A2%E6%9D%9F%E6%88%90%E5%BD%A2%EF%BC%89"><span class="toc-number">3.3.8.</span> <span class="toc-text">空间滤波（波束成形）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%86%E5%90%AC%E4%BF%A1%E5%8F%B7%E5%A2%9E%E5%BC%BA%E5%99%A8"><span class="toc-number">3.3.9.</span> <span class="toc-text">视听信号增强器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%A1%E5%8F%B7%E5%88%86%E7%A6%BB"><span class="toc-number">3.4.</span> <span class="toc-text">信号分离</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A1%AE%E5%AE%9A%E6%A1%88%E4%BB%B6"><span class="toc-number">3.4.1.</span> <span class="toc-text">确定案件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%A3%B0%E9%81%93%E5%88%86%E7%A6%BB"><span class="toc-number">3.4.2.</span> <span class="toc-text">单声道分离</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E5%87%BA%E7%8E%B0%E7%9A%84%E4%B8%BB%E9%A2%98"><span class="toc-number">4.</span> <span class="toc-text">新出现的主题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%A2%E8%A7%82%E8%AF%84%E4%BC%B0"><span class="toc-number">4.1.</span> <span class="toc-text">客观评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MIR"><span class="toc-number">4.1.1.</span> <span class="toc-text">MIR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DCASE"><span class="toc-number">4.1.2.</span> <span class="toc-text">DCASE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E5%BC%BA%E5%A4%A7%E7%9A%84%E6%B6%88%E8%B4%B9%E7%94%B5%E5%AD%90%E8%AE%BE%E5%A4%87%E5%92%8C%E5%BF%AB%E9%80%9F%E7%9A%84%E4%BA%92%E8%81%94%E7%BD%91%E8%BF%9E%E6%8E%A5"><span class="toc-number">4.2.</span> <span class="toc-text">功能强大的消费电子设备和快速的互联网连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E5%92%8C%E5%B1%95%E6%9C%9B"><span class="toc-number">4.3.</span> <span class="toc-text">结论和展望</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E8%80%85"><span class="toc-number">4.4.</span> <span class="toc-text">作者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reference"><span class="toc-number">4.5.</span> <span class="toc-text">Reference</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/13/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%EF%BC%9A%E5%9F%BA%E4%BA%8E%E7%90%83%E9%BA%A6%E5%85%8B%E9%A3%8E%E9%98%B5%E5%88%97%E7%9A%84%E5%A3%B0%E6%BA%90DoA%E4%BC%B0%E8%AE%A1/" title="知识总结: 基于球麦克风阵列的声源DoA估计"><img src="https://i.ibb.co/1dkyDjy/acoustic-camera-gfaitech-Sphere120-beamforming-microphone-array-3d-measurement-interior-right.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="知识总结: 基于球麦克风阵列的声源DoA估计"/></a><div class="content"><a class="title" href="/2024/01/13/%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93%EF%BC%9A%E5%9F%BA%E4%BA%8E%E7%90%83%E9%BA%A6%E5%85%8B%E9%A3%8E%E9%98%B5%E5%88%97%E7%9A%84%E5%A3%B0%E6%BA%90DoA%E4%BC%B0%E8%AE%A1/" title="知识总结: 基于球麦克风阵列的声源DoA估计">知识总结: 基于球麦克风阵列的声源DoA估计</a><time datetime="2024-01-13T00:51:54.000Z" title="发表于 2024-01-13 08:51:54">2024-01-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/02/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20task3%20%20SELD%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E5%AE%9A%E4%BD%8D%E6%A3%80%E6%B5%8B/" title="文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data"><img src="https://i.ibb.co/b7XKwb2/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data"/></a><div class="content"><a class="title" href="/2023/11/02/%E6%AF%94%E8%B5%9B%E7%9B%B8%E5%85%B3%EF%BC%9ADCASE%202019%20task3%20%20SELD%E5%A3%B0%E9%9F%B3%E4%BA%8B%E4%BB%B6%E5%AE%9A%E4%BD%8D%E6%A3%80%E6%B5%8B/" title="文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data">文献阅读: DCASE 2019 task3  SELD声音事件定位检测 data</a><time datetime="2023-11-02T07:02:20.000Z" title="发表于 2023-11-02 15:02:20">2023-11-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/" title="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR"><img src="https://i.ibb.co/vHV5BKD/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR"/></a><div class="content"><a class="title" href="/2023/10/27/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9A%20Deep%20Networks%20for%20Direction-of-Arrival%20Estimation%20in%20Low%20SNR/" title="文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR">文献阅读: Deep Networks for Direction-of-Arrival Estimation in Low SNR</a><time datetime="2023-10-26T23:33:28.000Z" title="发表于 2023-10-27 07:33:28">2023-10-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/" title="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays"><img src="https://i.ibb.co/XVMYNX3/image.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays"/></a><div class="content"><a class="title" href="/2023/10/25/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9ADeep%20learning%20assisted%20sound%20source%20localization%20using%20two%20orthogonal%20first-order%20differential%20microphone%20arrays/" title="文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays">文献阅读: Deep learning assisted sound source localization using two orthogonal first-order differential microphone arrays</a><time datetime="2023-10-25T11:05:44.000Z" title="发表于 2023-10-25 19:05:44">2023-10-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/29/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFundamentals%20of%20Spherical%20Array%20Processing/" title="文献阅读:Fundamentals of Spherical Array Processing"><img src="https://i.ibb.co/pRR1S1b/Eigenmike.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="文献阅读:Fundamentals of Spherical Array Processing"/></a><div class="content"><a class="title" href="/2023/09/29/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%EF%BC%9AFundamentals%20of%20Spherical%20Array%20Processing/" title="文献阅读:Fundamentals of Spherical Array Processing">文献阅读:Fundamentals of Spherical Array Processing</a><time datetime="2023-09-29T15:32:04.000Z" title="发表于 2023-09-29 23:32:04">2023-09-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Gavin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Have a nice day!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>